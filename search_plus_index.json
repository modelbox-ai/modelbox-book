{"./":{"url":"./","title":"ModelBox介绍","keywords":"","body":"ModelBox介绍 什么是ModelBox 一个典型场景AI算法的商用落地除了模型训练外，还需要进行视频图片解码、HTTP服务、预处理、后处理、多模型复杂业务串联、运维、打包等工程开发，往往需要耗费比模型训练多得多的时间，同时算法的性能和可靠性通常随开发人员的工程能力水平高低而参差不齐，严重影响AI算法的上线效率。 ModelBox是一套专门为AI开发者提供的易于使用，高效，高扩展的AI推理开发框架，它可以帮助AI开发者快速完成从模型文件到AI推理应用的开发和上线工作，降低AI算法落地门槛，同时带来AI应用的高稳定性和极致性能。 ModelBox特点 易于开发，AI推理业务可视化编排开发，功能模块化，丰富组件库；C++、Python多语言支持。 易于集成，集成云上对接的组件，云上对接更容易。 高性能，高可靠，pipeline并发运行，数据计算智能调度，资源管理调度精细化，业务运行更高效。 软硬件异构， CPU、GPU、NPU多异构硬件支持，TensorRT、TensorFlow、LibTorch、MindSpore等多推理引擎兼容, 资源利用高效和开发便捷，。 全场景，视频，语音，文本，NLP全场景，专为服务化定制，云上集成更容易，一次开发端边云部署。 易于维护，服务运行状态可视化，应用，组件性能实时监控，优化更容易。 ModelBox解决的问题 目前AI应用开发时，完成模型训练后，需要将多个模型和应用逻辑串联在一起组成AI应用，并上线发布成为商用服务或应用。在整个过程中，需要面临复杂的应用编程工作： 工作事项 内容说明 需要开发AI应用的周边功能 比如AI应用编译工程，应用初始化，配置管理接口，日志管理口，应用故障监控等功能。 需要开发AI常见的前后处理 音视频加解码，图像转换处理，推理前处理，YOLO后处理等开发。 需要开发和云服务对接的周边功能 比如HTTP服务开发，云存储，大数据服务，视频采集服务对接开发。 需要开发出高性能的推理应用 需要基于多线程，内存池化，显存池化，多GPU加速卡，模型batch批处理，调用硬件卡的API等手段开发应用。 需要开发验证Docker镜像 需要开发Docker镜像，集成必要的FFmpeg，OpenCV软件，CUDA，MindSpore，TensorFlow等软件，并做集成测试验证。 多种AI业务，需要共享代码，降低维护工作 需要复用不同组件的代码，包括AI前后处理代码，AI应用管理代码，底层内存，线程管理代码等。 模型开发者，验证模型功能比较复杂 模型开发者完成模型训练后，需要编写Python代码验证，之后，再转成生产代码；在高性能，高可靠场景改造工作量大。 ModelBox的目标是解决AI开发者在开发AI应用时的编程复杂度，降低AI应用的开发难度，将复杂的数据处理，并发互斥，多设备协同，组件复用，数据通信，交由ModelBox处理。开发者主要聚焦AI业务逻辑本身，而不是软件细节。 在提高AI推理开发的效率同时，保证软件的性能，可靠性，安全性等属性。 ModelBox可以给开发者提供如下能力： 完整的工程框架基础能力，包括配置管理、日志管理、内存显存管理、异常机制等。 提供AI应用开发常用的多硬件功能单元，如编解码、图像处理、模型后处理等。 提供高性能的调度引擎，帮助开发者解决AI应用商用性能问题。 提供Docker开发和运行环境，减少用户环境构建时间。 提供端边云统一框架接口，屏蔽硬件差异，业务一次开发，多硬件支持。 提供与第三方平台对接的插件机制，同时提供华为云DIS、OBS、ModelArts等服务的对接插件。 ModelBox主要功能 功能 说明 用户群体 软件开发者，研究人员，学生，平台集成商 应用场景 视频，图片，文字，语音等 部署场景 云服务器， 边侧设备，嵌入式设备 支持OS Ubuntu, OpenEuler 支持系统架构 X86, Arm 支持加速卡 CPU, Nvidia GPU, Ascend 310, Ascend 710, RK3568 支持推理引擎 TensorRT、TensorFlow、LibTorch、MindSpore、Ascend ACL 可视化编排 支持可视化UI的业务流程编排 支持开发语言 C++ , Python, Java 图编排能力 支持展开，合并，条件，循环分支等 调测能力 ModelBox Tool工具，性能跟踪Profiling工具 一次开发，多处运行 Python功能单元，C++功能单元 预置功能单元 包含了大部分高性能的基础功能单元，包括HTTP，视频，图像，云相关的功能单元 ModelBox应用 ModelBox提供了一些预置的解决方案，可供直接调用的API，详见ModelBox应用章节。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"environment/environment.html":{"url":"environment/environment.html","title":"环境准备","keywords":"","body":"环境准备 ModelBox的环境准备有两种方式：1. 可以通过下载源码进行手动编译安装； 2. 也可以直接下载ModelBox提供的容器镜像启动开发环境； 源码编译安装需要先安装依赖软件，耗时较长，所以在支持Docker的系统上建议优先使用下载容器镜像的方式使用ModelBox。 源码编译安装 使用容器镜像 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"environment/compile.html":{"url":"environment/compile.html","title":"源码编译安装","keywords":"","body":"源码编译安装 ModelBox框架采用C++语言编写，工程编译软件是CMake，本章节主要讲解ModelBox源代码的手动编译安装过程。 基于ModelBox开发AI应用，推荐使用容器镜像方式开发，避免从源代码构建ModelBox耗时。源码编译安装方式主要用于不支持Docker的系统，如部分端侧、嵌入式设备等。 一键式脚本 如果只是运行mnist验证，可以使用如下一键式脚本准备编译环境。脚本：get-start.sh curl https://modelbox-ai.com/modelbox-book/environment/get-start.sh | sh /dev/stdin -m -j2 参数说明： -m： 从国内镜像下载依赖软件包。 -j[n]： 并发编译数量。默认是2个。 编译依赖准备 在编译ModelBox之前，需要安装依赖软件，ModelBox依赖软件要求如下: 类别 依赖 依赖说明 最低版本 推荐版本 是否必须 相关组件 编译器 gcc gcc编译器 4.8 7.x 是 所有 编译器 g++ g++编译器 4.8 7.x 是 所有 编译器 CMake CMake工具 2.9 3.5 是 所有 OS Linux Linux操作系统 ubuntu16.04, centOS 7.2 ubuntu 18.04 是 所有 运行时 nodejs 前端编译 10.x V12.x 否 前端Editor 运行时 python Python编译 3.x 3.8 否 Python支持 开发库 cuda cuda支持 10.1 10.2 否 cuda支持 开发库 cann Ascend支持 5.0.4 5.0.4 否 Ascend支持 开发库 ffmpeg 视频解码编码支持 否 视频相关功能 开发库 tensorrt TensorRT模型推理 否 TensorRT相关的模型推理功能 开发库 tensorflow TensorFlow推理支持 否 TensorFlow相关的模型推理功能 开发库 libtorch LibTorch推理支持 否 LibTorch相关的模型推理功能 开发库 mindspore MindSpore推理支持 否 MindSpore相关的模型推理功能 开发库 httplib HTTP服务支持 是 modelbox-server相关的功能组件 开发库 cpprest HTTP服务支持 否 HTTP相关功能单元 上述依赖可按需求选择，其中 是否必须 为 是 的依赖，必须要安装到编译环境中才能正常编译代码。如果使用基于镜像的开发环境，可以省去依赖库的安装, 具体见容器镜像章节。 如果不使用容器镜像，也可按上述依赖列表自行基于当前操作系统进行安装。ubuntu操作系统基础软件安装命令如下： apt-get update apt-get -y install cmake git wget build-essential npm curl \\ python3 python3-pip python-is-python3 \\ libssl-dev libcpprest-dev libopencv-dev libgraphviz-dev python3-dev \\ libavfilter-dev libavdevice-dev libavcodec-dev pip install requests opencv-python 如上述依赖安装比较慢，可以使用国内的镜像进行安装，具体镜像如下： pip镜像下载： 配置参考： https://mirrors.tuna.tsinghua.edu.cn/help/pypi/ 临时使用参考： pip install -i https://pypi.tuna.tsinghua.edu.cn/simple requests opencv-python apt镜像下载： 配置参考： https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/ https://mirrors.tuna.tsinghua.edu.cn/help/debian/ npm镜像： 配置参考： npm config set registry https://registry.npmmirror.com 如果需要使用模型推理，还需要安装对应推理引擎、CUDA或CANN等相关依赖。 编译ModelBox 准备 编译ModelBox之前，需要准备好开发环境。或在镜像中进行编译，或按上述依赖列表，安装相应的依赖组件。 下载ModelBox代码 github： git clone https://github.com/modelbox-ai/modelbox.git gitee： git clone https://gitee.com/modelbox/modelbox.git 编译ModelBox mkdir build cd build cmake .. -DUSE_CN_MIRROR=yes：在编译过程中，还需要下载第三方依赖，请保持网络能正常连接第三方服务器，如在国内无法下载，可以增加此cmake参数，从国内镜像下载依赖。 -DLOCAL_PACKAGE_PATH：若本地已经有依赖的第三方软件包，则可以使用此参数指定本地依赖包路径，若使用ModelBox编译镜像时，编译镜像的/opt/thirdparty/source已经有相关依赖包，可直接指定本地路径使用，若需要从公共源码仓下载，则无需指定此参数，但需要确保网络通畅。 -WITH_WEBUI=off：可视化WEBUI会增加一些额外依赖下载，如果不需要可增加此参数关闭。 -DWITH_ALL_DEMO=off：是否编译全量的样例，如果不需要可以关闭。 如需编译release版本，可以执行如下cmake命令 cmake -DCMAKE_BUILD_TYPE=Release .. 如需进行断点调试，则应编译debug版本，可以执行如下cmake命令 cmake -DCMAKE_BUILD_TYPE=Debug .. 编译安装包 make package -j16 编译完成后，将在release目录下生成对应的安装包。 安装ModelBox ModelBox编译完成后，将生成配套OS安装的安装包，如deb、rpm包和tar.gz包，路径为编译目录的release子目录。可根据使用需求进行安装，下表是软件包的用途对照表。 安装包功能对照表 类型 名称 说明 运行库 modelbox-x.x.x-Linux-libmodelbox.[deb|rpm] ModelBox核心运行库。 运行库 modelbox-x.x.x-Linux-graph-graphviz.[deb|rpm] 图解析组件。 服务组件 modelbox-x.x.x-Linux-server.[deb|rpm] ModelBox Server服务组件。 运行库 modelbox-x.x.x-Linux-ascend-device-flowunit.[deb|rpm] Ascend设备SDK以及配套基础功能单元组件。 运行库 modelbox-x.x.x-Linux-cpu-device-flowunit.[deb|rpm] CUDA设备SDK以及配套基础功能单元组件。 运行库 modelbox-x.x.x-Linux-cuda-device-flowunit.[deb|rpm] CPU设备SDK以及配套基础功能单元组件。 开发库 modelbox-x.x.x-Linux-libmodelbox-devel.[deb|rpm] ModelBox开发库。 开发库 modelbox-x.x.x-Linux-server-devel.[deb|rpm] ModelBox Server服务插件开发库。 开发库 modelbox-x.x.x-Linux-ascend-device-flowunit-devel.[deb|rpm] Ascend设备开发库。 开发库 modelbox-x.x.x-Linux-cpu-device-flowunit-devel.[deb|rpm] CPU开发包。 开发库 modelbox-x.x.x-Linux-cuda-device-flowunit-devel.[deb|rpm] CUDA设备开发库。 手册 modelbox-x.x.x-Linux-document.deb.[deb|rpm] 开发手册，包含API说明。 WebUI modelbox-x.x.x-Linux-modelbox-webui.[deb|rpm] 编排界面。 运行库 modelbox-x.x.x-py3-none-any.whl python wheel包。 全量包 modelbox-x.x.x-Linux.tar.gz 全量安装包，包括上述所有组件。 安装包说明 ModelBox运行库，CPU运行库，graphviz图解析组件必须安装。 Ascend设备组件，CUDA设备组件，可根据硬件配置情况合理安装。 modelbox-server服务组件，推荐安装。 modelbox-x.x.x-py3-none-any.whl在需要Python运行时安装。 modelbox-x.x.x-Linux.tar.gz可解压直接使用。推荐使用安装包，方便管理。 安装命令说明 debian安装包 sudo dpkg -i *.deb rpm安装包 sudo rpm -i *.rpm Python wheel包 pip install *.whl tar.gz包的使用。（可选，如果已经安装了deb|rpm包，则可不用安装tar.gz包） tar xf modelbox-x.x.x-Linux.tar.gz 将解压后的目录，复制到/目录，此步骤可选。 cd modelbox cp * / -avf 若未选择上述步骤，未复制modelbox文件到/目录，则需要将/lib/systemd/system/modelbox.service中文件修改为对应的解压目录 ExecStart=[path/to]/modelbox -p /var/run/modelbox.pid -c [path/to]/modelbox.conf 将上述路径[[path/to]修改为对应解压后的路径。然后执行如下命令 cp modelbox.service /lib/systemd/system/modelbox.service systemctl daemon-reload 启动服务 如安装了modelbox-x.x.x-Linux-server，可以使用下述命令启动服务 systemctl enable modelbox systemctl start modelbox 关于ModelBox Server服务的配置，请查阅运行服务章节 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"environment/container-usage.html":{"url":"environment/container-usage.html","title":"使用容器镜像","keywords":"","body":"使用容器镜像 AI推理业务依赖的外部组件比较多，手工安装部署工作量比较大，ModelBox提供了多种推理引擎、支持不同硬件加速卡的容器镜像方便开发者使用。本章节介绍了容器镜像使用的步骤。 容器镜像选择 ModelBox目前提供了支持CUDA，Ascend加速卡硬件和TensoFlow，LibTorch，TensorRT，MindSpore，ACL等推理引擎的多种镜像，开发者可以按照需要下载使用。具体可见容器列表 目前ModelBox的镜像类型分为两种：开发镜像和运行镜像。 开发镜像：用于AI应用的开发和调试，镜像安装了ModelBox开发库、头文件、Web UI、GDB等开发阶段所需的组件。 运行镜像：用于AI应用的部署运行。镜像只安装了AI应用运行所需的运行环境。 运行镜像比开发镜像精简稳定，通常可以在开发镜像开发完毕应用程序后打包，再安装在运行镜像，用于商用部署。 容器镜像下载 使用以下命令拉取相关的镜像。比如cuda11.2，TensorFlow的unbuntu开发镜像，则下载最新版本镜像命令如下： docker pull modelbox/modelbox-develop-tensorflow_2.6.0-cuda_11.2-ubuntu-x86_64:latest 也可以选择需要的版本号下载。 一键式启动脚本 为方便使用，可以使用如下一键式脚本，快速启动容器。可将如下脚本按需修改后，粘贴到ssh终端中执行： #!/bin/bash # ssh map port, [modify] SSH_MAP_PORT=50022 # editor map port [modify] EDITOR_MAP_PORT=1104 # http server port [modify] HTTP_SERVER_PORT=8080 # container name [modify] CONTAINER_NAME=\"modelbox_instance_`date +%s` \" # image name IMAGE_NAME=\"modelbox/modelbox-develop-tensorflow_2.6.0-cuda_11.2-ubuntu-x86_64\" HTTP_DOCKER_PORT_COMMAND=\"-p $HTTP_SERVER_PORT:$HTTP_SERVER_PORT\" docker run -itd --gpus all -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,video \\ --tmpfs /tmp --tmpfs /run -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ --name $CONTAINER_NAME -v /home:/home \\ -p $SSH_MAP_PORT:22 -p $EDITOR_MAP_PORT:1104 $HTTP_DOCKER_PORT_COMMAND \\ $IMAGE_NAME 如果Docker版本低于19.03，则需要替换docker run命令为 docker run -itd --runtime=nvidia -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,video \\ --tmpfs /tmp --tmpfs /run -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ --name $CONTAINER_NAME -v /home:/home -p $SSH_MAP_PORT:22 \\ -p $EDITOR_MAP_PORT:1104 $HTTP_DOCKER_PORT_COMMAND \\ $IMAGE_NAME 上述脚本可支持GPU设备的挂载，如果需要使用Ascend加速硬件，则需要替换docker run命令为 # ascend npu card id [modify] ASCEND_NPU_ID=0 docker run -itd --device=/dev/davinci$ASCEND_NPU_ID --device=/dev/davinci_manager \\ --device=/dev/hisi_hdc --device=/dev/devmm_svm \\ --tmpfs /tmp --tmpfs /run -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ --name $CONTAINER_NAME -v /home:/home -p $SSH_MAP_PORT:22 \\ -p $EDITOR_MAP_PORT:1104 $HTTP_DOCKER_PORT_COMMAND \\ $IMAGE_NAME 参数说明： SSH_MAP_PORT: 为容器ssh映射端口号。 EDITOR_MAP_PORT: 为可视化开发界面链接端口号。 HTTP_SERVER_PORT: 为HTTP功能单元端口号。 IMAGE_NAME: 要启动的镜像名称。 ASCEND_NPU_ID: 需要挂载的Ascend加速卡设备号，可以通过npu-smi info 查询，一般取值为0~7。 注意事项： 可使用vim start_docker.sh创建文件后，i进入编辑模式后，粘贴上述代码，编辑修改后，wx保存。 docker启动脚本中，请注意启动的镜像版本是否与自己所需的镜像版本一致。 如果需要在容器中进行gdb调试，需要在启动容器添加--privileged参数。 如果在没有GPU的机器上执行上述命令，可以删除--gpus相关的参数。但此时只能使用CPU相关的功能单元。 如果启动镜像之后，端口未被占用却仍旧无法访问，需要检查防火墙设置。 使用容器 容器启动完成后，需要配置容器中账号信息，并方便后续使用ssh工具连接进入容器，具体的步骤： 从Host中进入容器，设置root密码 docker exec -it [container id] bash passwd 使用ssh工具连接到容器内部进行后续的AI应用开发 ssh [docker ip] -p [ssh map port] [docker ip]: 容器所在得Host IP地址。 [ssh map port]：步骤1中SSH_MAP_PORT映射的端口号。 Docker启动脚本详解 此处以modelbox-develop-tensorflow_2.6.0-cuda_11.2-ubuntu-x86_64:latest镜像举例 docker run -itd --gpus all -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,video \\ --tmpfs /tmp --tmpfs /run -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ modelbox/modelbox-develop-tensorflow_2.6.0-cuda_11.2-ubuntu-x86_64:latest 参数: -itd 选项 选项简写 说明 -detach -d 在后台运行容器,并且打印容器id。 -interactive -i 即使没有连接，也要保持标准输入保持打开状态，一般与 -t 连用。 –tty -t 容器重新分配一个伪输入终端，一般与 -i 连用。 参数: -gpus 请通过 docker -v 检查 Docker 版本。对于 19.03 之前的版本，需要使用 nvidia-docker2 和 --runtime=nvidia 参数；对于 19.03 及之后的版本，则使用 nvidia-container-toolkit 软件包和 --gpus all 参数。 参数：--device 挂载设备和驱动，例如挂载0号Ascend设备和驱动：--device=/dev/davinci0 --device=/dev/davinci_manager --device=/dev/hisi_hdc --device=/dev/devmm_svm。 参数: -e 设置环境变量 参数: -tmpfs 挂载目录到容器中，而且容器内的修改不会同步到宿主机，也不希望存储在容器内， 调用这个参数，将该挂载存储在主机的内存中，当容器停止后， tmpfs挂载被移除，即使提交容器，也不会保存tmpfs挂载 参数: -v 挂载宿主机的指定目录 ( 或文件 ) 到容器内的指定目录 ( 或文件 ) ro表示read-only 注意事项： 容器目录必须为绝对路径 容器销毁后，挂载的文件以及 在容器修改过的内容仍然保留在宿主机中 参数: --privileged=true 当开发者需要使用gdb调试功能时，需要使用特权模式启动Docker 参数: --cap-add=SYS_PTRACE 增加容器镜像系统的权限 ptrace()系统调用函数提供了一个进程（the “tracer”）监察和控制 另一个进程（the “tracee”）的方法。 并且可以检查和改变“tracee”进程的内存和寄存器里的数据。 它可以用来实现断点调试和系统调用跟踪。（用于gdb） 参数: --security-opt seccomp=unconfined Seccomp是Secure computing mode的缩写。 设为unconfined可以允许容器执行全部的系统的调用。 有遇到无法启动的问题， 请检查是否安装nvidia-container-toolkit 和对应的cuda版本 支持容器列表 ModelBox镜像仓库地址如下：https://hub.docker.com/u/modelbox 当前支持的开发镜像列表如下： 镜像名称 操作系统 系统架构 加速平台版本 推理引擎版本 下载地址 modelbox-develop-tensorflow_2.6.0-cuda_11.2-ubuntu-x86_64 ubuntu x86_64 cuda11.2 TensorFlow 2.6.0 下载链接 modelbox-develop-tensorflow_2.6.0-cuda_11.2-openeuler-x86_64 openeuler x86_64 cuda11.2 TensorFlow 2.6.0 下载链接 modelbox-develop-libtorch_1.9.1-cuda_10.2-ubuntu-x86_64 ubuntu x86_64 cuda10.2 LibTorch 1.9.1 下载链接 modelbox-develop-libtorch_1.9.1-cuda_10.2-openeuler-x86_64 openeuler x86_64 cuda10.2 LibTorch 1.9.1 下载链接 modelbox-develop-tensorrt_7.1.3-cuda_10.2-ubuntu-x86_64 ubuntu x86_64 cuda10.2 TensorRT 7.1.3 下载链接 modelbox-develop-tensorrt_7.1.3-cuda_10.2-openeuler-x86_64 openeuler x86_64 cuda10.2 TensorRT 7.1.3 下载链接 modelbox-develop-mindspore_1.6.1-cann_5.0.4-ubuntu-x86_64 ubuntu x86_64 cuda11.2 MindSpore 1.6.1、ACL 下载链接 modelbox-develop-mindspore_1.6.1-cann_5.0.4-openeuler-x86_64 openeuler x86_64 cuda11.2 MindSpore 1.6.1、ACL 下载链接 modelbox-develop-mindspore_1.6.1-cann_5.0.4-ubuntu-aarch64 ubuntu aarch64 cann 5.0.4 MindSpore 1.6.1、ACL 下载链接 modelbox-develop-mindspore_1.6.1-cann_5.0.4-openeuler-aarch64 openeuler aarch64 cann 5.0.4 MindSpore 1.6.1、ACL 下载链接 各组合对应运行镜像列表如下： 镜像名称 操作系统 系统架构 加速平台版本 推理引擎版本 下载地址 modelbox-runtime-tensorflow_2.6.0-cuda_11.2-ubuntu-x86_64 ubuntu x86_64 cuda11.2 TensorFlow 2.6.0 下载链接 modelbox-runtime-tensorflow_2.6.0-cuda_11.2-openeuler-x86_64 openeuler x86_64 cuda11.2 TensorFlow 2.6.0 下载链接 modelbox-runtime-libtorch_1.9.1-cuda_10.2-ubuntu-x86_64 ubuntu x86_64 cuda10.2 LibTorch 1.9.1 下载链接 modelbox-runtime-libtorch_1.9.1-cuda_10.2-openeuler-x86_64 openeuler x86_64 cuda10.2 LibTorch 1.9.1 下载链接 modelbox-runtime-tensorrt_7.1.3-cuda_10.2-ubuntu-x86_64 ubuntu x86_64 cuda10.2 TensorRT 7.1.3 下载链接 modelbox-runtime-tensorrt_7.1.3-cuda_10.2-openeuler-x86_64 openeuler x86_64 cuda10.2 TensorRT 7.1.3 下载链接 modelbox-runtime-mindspore_1.6.1-cann_5.0.4-ubuntu-x86_64 ubuntu x86_64 cuda11.2 MindSpore 1.6.1、ACL 下载链接 modelbox-runtime-mindspore_1.6.1-cann_5.0.4-openeuler-x86_64 openeuler x86_64 cuda11.2 MindSpore 1.6.1、ACL 下载链接 modelbox-runtime-mindspore_1.6.1-cann_5.0.4-ubuntu-aarch64 ubuntu aarch64 cann 5.0.4 MindSpore 1.6.1、ACL 下载链接 modelbox-runtime-mindspore_1.6.1-cann_5.0.4-openeuler-aarch64 openeuler aarch64 cann 5.0.4 MindSpore 1.6.1、ACL 下载链接 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"first-app/first-app.html":{"url":"first-app/first-app.html","title":"第一个应用","keywords":"","body":"第一个应用 第一个ModelBox应用以Mnist为例，详细描述了从模型到ModelBox应用的开发过程，详见mnist。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"first-app/mnist.html":{"url":"first-app/mnist.html","title":"Mnist","keywords":"","body":"Mnist MNIST案例是使用MNIST数据集，训练的一个手写数字识别TensorFlow模型，搭建的一个简易的HTTP请求服务。 功能 将MNIST数据，通过base64格式发送到ModelBox推理，并获取结果。 流程上：启动HTTP Server监听端口接收HTTP请求，然后从请求体中的image_base64解析出图片，接着用训练出的MNIST模型进行推理预测，最后将识别出的数字返回给用户。 Request 请求样例： { \"image_base64\": \"xxxxx\" } Response 响应样例： { \"predict_reuslt\": \"x\" } 模型准备 AI应用开发前需要准备好匹配当前ModelBox版本支持的推理框架和版本的模型文件，这里默认已经准备好了TensorFlow 2.6.0版本的minist pb模型文件。模型训练可参考TensorFlow教程。 AI应用开发 首先准备开发环境，备好了之后进入应用开发环节，主要分为流程图编排、功能单元编写、运行与调试、打包部署4个开发步骤。 环境准备 环境准备可以使用现成ModelBox镜像，也可以从源代码构建ModelBox。本章节使用现成ModelBox镜像开发，如果没有相关的镜像，可以参考树莓派开发板Mnist。 使用镜像开发，省去了准备复杂编译环境的大量工作，推荐ModelBox开发者直接使用镜像开发，ModelBox镜像相关的指导，可以先参考容器使用章节。 安装启动Docker后，执行下列命令下载Docker镜像。由于训练的是TensorFlow的模型（支持CPU和GPU推理），所以这里拉取对应TensorFlow版本镜像。 docker pull modelbox/modelbox-develop-tensorflow_2.6.0-cuda_11.2-ubuntu-x86_64:latest 配置并启动容器 可采用一键式脚本快速进入容器。参考一键式启动脚本相关内容。 项目创建与运行 进入容器并且切换至ModelBox开发者模式 modelbox-tool develop -s 注意事项： 如果需要通过可视化UI进行图的编排，可参考可视化编排服务章节访问http://[host]:[EDITOR_MAP_PORT]/editor/地址； 如果访问被拒绝，可参考运行编排服务中的访问控制列表相关内容。 连接ModelBox编排服务 服务启动后，可直接连接编排服务，服务启动信息可通过如下命令查询： modelbox-tool develop -q 浏览器访问上述地址的1104端口，注意事项： 如有权限问题，修改conf/modelbox.conf配置文件中的acl.allow数组，增加允许访问的IP范围。 推荐使用vscode的远程连接的终端操作，vscode可以自动建立端口转发。远程开发 创建项目工程 点击任务编排 点击项目->新建项目， 新建项目： 输入创建项目的名称:mnist 路径: /home/[user] 项目模板为: mnist 创建出的文件夹说明如下所示： ├─CMake：CMake目录，存放一些自定义CMake函数 ├─package：CPack打包配置目录，目前支持tgz、rpm、deb三种格式 ├─src：源代码目录，开发的功能单元、流程图、服务插件（可选）都存放在这个目录下 │ ├─flowunit：功能单元目录 │ │ ├─mnist_preprocess：预处理功能单元 │ │ ├─mnist_infer：tensorflow推理功能单元 │ │ ├─mnist_response：HTTP响应构造功能单元 │ ├─graph：流程图目录 │ └─service-plugin：服务插件目录 ├─test：单元测试目录，使用的是Gtest框架 └─thirdparty：第三方库目录 │ └─CMake：预制的下载第三方库cmake文件 |---CMakeLists.txt CMake编译文件 运行流程图 在任务编排页面中打开流程图，点击绿色运行按钮可运行流程图； 测试 脚本测试 可以使用已经准备好测试脚本[project_root]/src/graph/test_mnist.py，直接运行python3 test_mnist.py会下载测试图片并进行HTTP测试验证。 UI界面测试 在任务管理页面中点击调试可进行api调试，选择Mnist模板，将测试图片进行base64编码后放入请求body中，再点击send按钮可进行测试； 流程图说明 流程图编排是根据实际情况将现有业务逻辑拆分为N个功能单元，再将功能单元串联成一个完整的业务的过程。功能单元分为ModelBox预置功能单元和自定义功能单元，当预置功能单元满足不了业务场景时，需要开发者进行功能单元开发。有两种方式可编排流程图，第一种是使用UI进行可视化UI编排，第二种是直接编写图文件。具体可参考流程图开发章节。这里采用第二种方式。 如上图所示，根据业务流程可以将业务划分为5个功能单元，分别为接收HTTP请求，MNIST预处理，MNIST模型推理，MNIST响应构造，发送HTTP响应。对应图编排文件描述如下 [graph] format = \"graphviz\" graphconf = '''digraph mnist_sample { node [shape=Mrecord] httpserver_sync_receive[type=flowunit, flowunit=httpserver_sync_receive, device=cpu, time_out_ms=5000, endpoint=\"http://0.0.0.0:8190\", max_requests=100] mnist_preprocess[type=flowunit, flowunit=mnist_preprocess, device=cpu] mnist_infer[type=flowunit, flowunit=mnist_infer, device=cpu, deviceid=0, batch_size=1] mnist_response[type=flowunit, flowunit=mnist_response, device=cpu] httpserver_sync_reply[type=flowunit, flowunit=httpserver_sync_reply, device=cpu] httpserver_sync_receive:out_request_info -> mnist_preprocess:in_data mnist_preprocess:out_data -> mnist_infer:Input mnist_infer:Output -> mnist_response:in_data mnist_response:out_data -> httpserver_sync_reply:in_reply_info } 除了构建图之外，还需要增加必要配置，如功能单元扫描路径，日志级别等，具体可参考样例文件[project_root]/src/graph/mnist.toml。 功能单元说明 ModelBox提供基础预置功能单元，除此之外还需补充流程图中缺失的功能单元，具体开发可参考功能单元开发章节。 这里接收HTTP请求、发送HTTP响应两个功能单元ModelBox已提供，我们只需实现MNIST的预处理，推理，响应构造三个功能单元即可。 下面将详细说明这三个功能单元： MNIST预处理功能单元 预处理需要做：解析出图片，对图片进行reshape，构建功能单元输出Buffer。 # 获取输入输出端口BufferList in_data = data_context.input(\"In_1\") out_data = data_context.output(\"Out_1\") for buffer in in_data: # 从Buffer中获取请求body request_body = json.loads(buffer.as_object().strip(chr(0))) if request_body.get(\"image_base64\"): img_base64 = request_body[\"image_base64\"] img_file = base64.b64decode(img_base64) # 将图片进行resize img = cv2.imdecode(np.fromstring(img_file, np.uint8), cv2.IMREAD_GRAYSCALE) img = cv2.resize(img, (28, 28)) infer_data = np.array([255 - img], dtype=np.float32) infer_data = np.reshape(infer_data, (784,)) / 255. # 构建输出Buffer add_buffer = modelbox.Buffer(self.get_bind_device(), infer_data) out_data.push_back(add_buffer) else: # 获取字段错误处理 error_msg = \"wrong key of request_body\" modelbox.error(error_msg) add_buffer = modelbox.Buffer(self.get_bind_device(), \"\") add_buffer.set_error(\"MnistPreprocess.BadRequest\", error_msg) out_data.push_back(add_buffer) 详细代码可参考[project_root]/src/flowunit/mnist_preprocess。 MNIST推理功能单元 ModelBox已经适配了TensorFlow推理引擎，支持CPU和GPU推理，只需推理功能单元只需准备好模型和对应的配置文件即可。 配置文件如下： [base] name = \"mnist_infer\" # 功能单元名称 device = \"cpu\" # 功能单元设备类型 version = \"1.0.0\" # 功能单元版本号 description = \"Recognition handwritten digits recognition.\" # 功能单元版本号 entry = \"./mnist_model.pb\" # 模型路径 type = \"inference\" # 功能单元类型 virtual_type = \"tensorflow\" # 推理引擎 [input] [input.input1] name = \"Input\" # 模型输入端口名称 type = \"float\" # 模型输入端口设备类型 [output] [output.output1] name = \"Output\" # 模型输出端口名称 type = \"float\" # 模型输出端口数据类型 详细代码可参考[project_root]/src/flowunit/mnist_infer。 MNIST响应功能单元 得到推理的结果之后，需要构造响应： # 获取输入输出端口BufferList in_data = data_context.input(\"in_data\") out_data = data_context.output(\"out_data\") for buffer in in_data: # 从Buffer中获取推理结果 result_str = '' if buffer.has_error(): error_msg = buffer.get_error_msg() result = { \"error_msg\": str(error_msg) } else: max_index = np.argmax(buffer.as_object()) result = { \"predict_result\": str(max_index) } # 构建输出Buffer result_str = (json.dumps(result) + chr(0)).encode('utf-8').strip() add_buffer = modelbox.Buffer(self.get_bind_device(), result_str) # 将输出Buffer放入对应的输出端口BufferList中 out_data.push_back(add_buffer) 详细代码可参考[project_root]/src/flowunit/mnist_response。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/use-modelbox.html":{"url":"use-modelbox/use-modelbox.html","title":"AI应用开发","keywords":"","body":"AI应用开发 本章节介绍基于ModelBox的AI应用开发流程。ModelBox框架采用图编排的方式开发业务，在开发前，需要对以下基本概念有些了解： 流程图：ModelBox中用流程图(Graph)来表达应用逻辑。采用有向图的方式，将应用的执行逻辑表达为顶点和边，其中顶点表示了应用的某个数据处理逻辑单元，边则表示了逻辑单元之间的数据传递关系。在ModelBox中，针对流程图的开发，可以使用文本方式直接编辑，也可以使用可视化的编辑器进行编辑。对于流程图的概念详细介绍可参考流程图章节。 功能单元：ModelBox将流程图中的顶点称为功能单元(FlowUnit)。功能单元是应用的基本组成部分，也是ModelBox的执行单元。在ModelBox中，内置了大量的基础功能单元，开发者可以复用这些功能单元直接集成到AI应用流程图中，这也是基于流程图开发的一大好处。除内置功能单元外，ModelBox支持功能单元的自定义开发，支持的功能单元形式多样，如C/C++动态库、Python脚本、模型+模型配置文件等。对于功能单元的概念详细介绍可参考功能单元章节。 AI应用开发的主要工作是流程图的设计开发和自定义功能单元的开发。在开发AI应用之前，首先需要根据使用场景选择合适的应用模式，ModelBox提供了两种应用开发模式，适用于不同场景： 标准模式 SDK模式 推荐使用标准模式，应用模式的介绍和选择可参考开发模式概述章节： ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/develop-mode.html":{"url":"use-modelbox/develop-mode.html","title":"开发模式概述","keywords":"","body":"开发模式概述 ModelBox开发套件 ModelBox的开发视图如下，图中蓝色的部分是开发者可以自定义扩展的部分。 ModelBox开发套件包含如下部分： 流程图 Flow 控制ModelBox执行的过程，采用Graphviz DOT语言进行流程图设计，开发。 ModelBox Server 为简化服务化应用开发者的工作量，ModelBox集成了服务功能，ModelBox Server有完善的服务集成功能，REST-API功能，任务管理功能，并集成了ModelBox library，对应用开发者提供了插件扩展的接口，应用开发者只需编写业务需要的服务插件，即可驱动ModelBox和业务控制服务对接。 自定义应用 与ModelBox Server对应，如果业务需要更多的定制能力，业务可以直接使用ModelBox SDK提供的接口来驱动ModelBox，支持的语言有C++，Python, Java等。 ModelBox SDK ModelBox应用提供的开发API，包含了C++, Python, Java等多种语言的支持。开发者可以使用自己熟悉的语言进行集成或扩展。 ModelBox Tool ModelBox 开发调试工具，可用于检查图的正确性，调试单个图，查询可用的功能单元Flowunit，模型加密等功能。 ModelBox Library ModelBox核心库，包含了ModelBox运行需要的功能，包含内存管理，调度管理，图管理等功能。此核心组件不可定制，开发者直接通过API使用ModelBox核心库功能。 ModelBox Flowunit 功能单元，ModelBox的关键组成，处理数据的关键组件，也是开发者主要开发的组件。 ModelBox Device 设备支持组件，用于支持特定的硬件，如GPU，Ascend芯片等，ModelBox已经内置了主流的GPU，Ascend芯片开发支持。开发者只需要开发相应的功能即可。 ModelBox应用场景 新算法快速上线 开发者新开发算法模型已经训练完成，希望能快速开发出完整的AI应用，并部署上线，同时性能、可靠性等都能满足商用要求。ModelBox框架提供了从开发环境到打包部署全流程的开发能力，提升开发效率，同时保障AI应用质量。 已有业务性能提升 开发者现有AI应用性能遇到瓶颈，希望通过异步、多线程、模型Batch推理等优化方式提升性能，但自己开发门槛高且工作量大，希望能通过较少的改造快速提升整体性能。ModelBox框架提供了灵活的业务切换方式，可以切换部分业务提升整体性能。 多模型复杂业务追求极致性能 开发者在开发多模型复杂业务如实时视频流场景时，除了实现业务功能外，往往需要花费大量精力用于性能优化，开发者希望能快速获得极致性能。ModelBox框架多线程全异步的智能调度为开发者提供了极致性能保障。 多软硬件适配 开发者的软硬件存在多样化，希望AI应用能同时运行在GPU、Ascend或者只有CPU的等多种硬件设备上，X86、ARM多系统架构，Ubuntu、OpenEuler等多操作系统，LibTorch、TensorFlow、MindSpore等多种模型引擎， 而只需要维护一套业务代码、工程。ModelBox框架提供了多设备、多系统、多模型引擎的适配能力，开发者一次开发多处运行。 ModelBox应用模式 标准模式 如下图所示，标准模式应用的组成部分。 这种模式下AI应用的主入口由ModelBox管理，启动时，ModelBox会读取ModelBox配置，加载配置中选择的插件列表，初始化并启动所有的插件。插件在ModelBox部分起着重要的作用，被设计来完成应用部分的管理工作。ModelBox内置了两个插件：ModelBox插件，用于加载流程图来启动应用部分，具体请见ModelBox插件; Editor插件，用于开发时可视化管理应用部分的工作，如执行样例应用、可视化编排流程图等，具体请见Editor插件。 应用的全部逻辑承载编排在流程图配置文件中，开发者首先通过流程图配置文件描述整个应用的数据处理过程，然后实现流程图中缺少的功能单元，完成整个应用。 SDK模式 如下图所示，SDK模式应用的组成部分。 这种模式下，开发者控制AI应用的主入口。启动后，由流程图调用部分通过ModelBox SDK提供的API管理流程图的初始化、启动及数据交互。 流程图调用部分中，开发者可以通过API进行流程逻辑编排、加载已有的流程图配置文件、或者通过solution使用预置的逻辑等方式构建流程图，流程图构建由Flow对象完成，可以通过Flow对象提供的数据输入输出接口完成对算法的调用，Flow可以存在多个，即可以加载多个流程图逻辑。 应用模式选择 进行开发之前，需要先选择如何使用ModelBox帮助应用开发，以下是两种模式开发所需的开发工作、特性能力、使用场景推荐的介绍： 开发工作： 标准模式时，主要关心如何在应用部分编排全部的应用逻辑，需要将应用拆分成流程图形式，ModelBox预置功能单元可以直接在编排中使用，缺失的功能，则要完成自定义功能单元开发工作。默认情况下，使用已有的ModelBox插件管理应用部分的启动，如有特殊需要，还可以开发插件对应用部分进行自定义的管理。 SDK模式时，需要先确定哪些算法部分要作为流程图，一个应用可以包含多个流程图，每个流程图完成确定的算法逻辑。和标准模式一样完成流程图后，还需要在业务部分调用流程图的启动、停止和外部数据交互。 特性能力： ModelBox通过流程图的方式可以获得以下收益：使用ModelBox提供的预置功能单元能力；多个功能单元可以并发调度，提供并发度和批处理能力；支持多硬件。因此考虑逻辑编排成图的时候，可以考虑收益是否达到预期。 标准模式可以使用ModelBox提供的插件能力，开发功能单元可以使用此模式的可视化UI编排进行流程图开发。使用此模式也可以只通过流程图配置和自定义功能单元完成一个AI应用，并且可以收获到样例工程创建和一键打包的功能。 SDK模式下，主入口由用户控制，项目由开发者创建，算法部分的复杂度，算法部分的数量均由开发者决定，自由度较高。同时应用的打包需要由开发者考虑。 使用场景： 标准模式场景：整体业务逻辑清晰，比较容易通过流程图方式表达，开发者只需编写自定义功能单元和编排流程图即完成应用开发；需开发插件简单管理应用部分。 SDK模式场景：应用逻辑不能全部进入流程图中，例如存在多个算法部分配合使用，控制逻辑较为复杂的场景；已有业务迁移场景，改造原有业务工作量过大时，可以通过SDK方式快速实现对模型推理部分的加速。 通常情况下，推荐使用标准模式，可以获得更好的开发体验和性能。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/modelbox-app-mode.html":{"url":"use-modelbox/standard-mode/modelbox-app-mode.html","title":"标准模式","keywords":"","body":"标准模式 本章节将开始介绍如何使用ModelBox标准模式进行应用的开发流程。通常情况下，AI应用开发流程可分为一下几个步骤： 开发前准备 除了准备好ModelBox的开发环境外，AI应用开发前还需要准备好匹配当前ModelBox版本支持的推理框架和版本的模型文件，用于模型推理。如果是采用TensorRT框架，还需要注意模型转换时的显卡类型需要与运行时的显卡类型匹配。 项目创建 通过ModelBox提供的WebUI或者命令行的方式创建AI应用项目，可以创建空项目，也可以基于各类项目模板进行创建。 流程图开发 梳理AI应用业务逻辑，将整个流程拆分为一系列功能单元，确定各个功能单元的主体功能、前后功能单元数据交互，开发者可以使用预置功能单元，也可自定义功能单元。最终将所有功能单元通过流程图的方式进行编排。 功能单元开发 开发者需要自己实现自定义功能单元，包含功能单元创建、属性配置、处理逻辑实现、编译测试等。 流程图运行 所有功能单元实现完毕后，就可以运行流程图进行业务流程的调试，同时还可以使用Profiling工具对性能数据进行采集分析和优化。 服务插件开发 如果流程图的加载运行需要与外部第三方系统对接时，需要进行自定义服务插件的开发。不涉及可跳过此步骤。 打包部署 流程图和功能单元开发调试完毕后，需要将AI应用编译打包，部署到生产环境启动运行 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/create-project.html":{"url":"use-modelbox/standard-mode/create-project.html","title":"项目创建","keywords":"","body":"项目创建 启动开发模式 创建项目前需要先启动ModelBox开发模式，ModelBox开发模式提供了可视化UI编排等多种开发能力。 开发者可以通过命令行工具modelbox-tool来启动ModelBox开发模式： modelbox-tool develop -s # 会打印如下ModelBox开发调试信息： Debug ModelBox Info: Home: $HOME/modelbox-service Config: $HOME/modelbox-service/conf/modelbox.conf Log: $HOME/modelbox-service/log Service command: $HOME/modelbox-service/modelbox {start|stop|restart|status} Manager command: $HOME/modelbox-service/modelbox-manager {start|stop|restart|status} Tool Command: modelbox-tool server -conf /root/modelbox-service/conf/modelbox.conf UI URL: http://0.0.0.0:1104/editor/ Service Status: modelbox service is running. pid xxxx Manager Status: modelbox-manager service is running. pid xxxx 打印信息说明如下： Home：ModelBox开发调试管理工具目录 Config：ModelBox服务配置文件 Log：ModelBox调试日志目录 Service command：ModelBox Server命令工具，可启动、停止、重启、查询ModelBox服务状态 Manager command：modelbox-manager用来监控管理ModelBox服务 Tool Command：ModelBox插件服务查询命令 UI URL：可视化编排服务服务URL Service Status：ModelBox服务状态和pid Manager Status：modelbox-manager 服务状态和pid 后面也可通过modelbox-tool develop -q查询ModelBox开发调试信息。 创建项目 有两种方式创建项目工程：UI界面创建项目，命令行创建项目。 UI界面创建项目 通过可视化UI编排界面创建项目步骤如下： 进入UI界面 -> 点击“编排”进入任务编排页面 -> 点击“项目”，“新建项目” -> 填写“项目名称”、“项目路径”、“项目模板” 可在对应目录下创建ModelBox项目工程。 命令行创建项目 ModelBox提供了modelbox-tool工具一键式创建工程： modelbox-tool template -project -name [name] -template [template_name] -path [project_path] 参数说明 -name：项目名称； -template：选择创建项目的模板类型，可选参数：empty、mnist、mnist-mindspore、hello-world、car_detection、emotion_detection、resize； -path：项目工程路径； 更多详细参数使用可通过modelbox-tool template --help查询。 项目工程目录 创建好的工程目录如下： ├─CMake：CMake目录，存放一些自定义CMake函数 ├─package：CPack打包配置目录，目前支持tgz、rpm、deb三种格式 ├─src：源代码目录，开发的功能单元、流程图、服务插件（可选）都存放在这个目录下 │ ├─flowunit：功能单元目录 │ ├─graph：流程图目录 │ └─service-plugin：服务插件目录 ├─test：单元测试目录，使用的是Gtest框架 └─thirdparty：第三方库目录 │ └─CMake：预制的下载第三方库cmake文件 |---CMakeLists.txt CMake编译文件 工程目录创建好后，开发者可在src目录下开发功能单元、流程图、服务插件（可选），基于此工程目录通过cmake ..、make package命令一键打包出tgz和rpm/deb包。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/flow/flow.html":{"url":"use-modelbox/standard-mode/flow/flow.html","title":"流程图开发","keywords":"","body":"流程图开发 流程图开发步骤 流程图开发是指开发者根据实际的业务需求，对业务处理流程进行设计拆分，最后按照流程图的开发规范创建流程图，形成ModelBox能识别的配置文件的过程。流程图基本概念可参考基础概念中的流程图章节。流程图的开发是首要进行的，流程图开发完毕后，才能明确业务处理流程，同时明确开发者还需要开发哪些功能单元。 如上图，以开发一个视频车辆检测的AI应用为例，流程图开发步骤如下： 根据业务流程可以将业务流程拆分为如下几个功能单元：视频输入、视频解封装、视频解码、图片缩放、图片颜色转换、归一化、车辆推理、原图画框、视频编码。确定哪些可以直接预置功能单元，哪些需要自定义开发。即确定流程图的点。 确定自定义功能单元的输入输出、类型，如原图画框步骤需要两个输入：车辆检测坐标、原始图片，输出为画框后的图片。即确定流程图的边。 确定自定义功能单元的类型和属性，是否是条件分支、展开、合并等。具体概念可参考基础概念中功能单元章节。 创建自定义功能单元，根据ModelBox流程图规范进行构图。可以通过ModelBox的可视化UI编排界面构图，也可以手工编写配置文件。 流程图设计原则 根据业务需求对业务流程进行拆分设计时，节点粒度越细并发度和复用度越高，但会带来节点间交互复杂度变高。流程图的设计一般可遵循以下几个原则： 模型推理一般作为单独的功能单元； ModelBox内置了大量高性能预处理功能单元，可直接作为流程图节点; ModelBox也内置了部分后处理功能单元，如YOLO后处理，可直接作为流程图节点； 如有业务逻辑在中有分支判断、拆分合并则需要作为单独的功能单元； 条件/展开功能单元之前的功能单元不能直接连接到条件/展开的内部； 其他业务逻辑，如果不是处理瓶颈，不需要拆分。 典型场景业务可以参考ModelBox提供的应用样例，具体可参考第一个应用中每个案例的的流程图开发章节。 流程图构建规范 ModelBox采用TOML配置格式作为流程图配置文件。一个流程图使用一份TOML格式的配置表示，配置文件内容如下： [driver] dir=[\"dir1\",\"dir2\"] [graph] format = \"graphviz\" graphconf = '''digraph demo { # 定义节点属性 httpserver_sync_receive[type=flowunit, flowunit=httpserver_sync_receive, device=cpu, time_out_ms=5000, endpoint=\"http://0.0.0.0:7770\"] hello_world[type=flowunit, flowunit=hello_world, device=cpu] httpserver_sync_reply[type=flowunit, flowunit=httpserver_sync_reply, device=cpu] # 定义边，即节点关系 httpserver_sync_receive:out_request_info -> hello_world:in_data hello_world:out_data -> httpserver_sync_reply:in_reply_info } ''' 配置文件说明： [driver]：用于配置功能单元的扫描路径。 dir: 指定功能单元等驱动加载路径，可以指定多个路径，通过[] 和 ，分隔。 [graph]：用于定义流程图的描述。 format指定流程图的格式，目前仅支持graphviz。 graphconf为流程图的描述内容。 ModelBox流程图的描述采用Graphviz DOT语法表达，关于DOT语法，可以查看Graphviz DOT的指导。具体格式可包含三部分： 第一部分是图 格式 digraph [graph_name] 说明 digraph开头，[graph_name]可以是字符串。 第二部分是点Node的定义 格式 node_name[type=flowunit, flowunit=flowunit_name, key=value] 说明 node_name为点的名称，key为node的配置属性，每个节点不同，value为key的配置值。 type参数指定点node的类型，可以是input, output, flowunit，当未指定type参数时，node缺省为flowunit。不同取值含义如下： flowunit: 表示此点为功能单元功能模块，配合flowunit=xx指定功能单元的执行实体。 node[type=flowunit, flowunit=httpserver] 上述配置表示，点的名称为node，类型为flowunit，其执行实体为httpserver。 input：表示此点的类型为输入端口，为整个图的配置，表示图的数据输入端口，主要用于外部数据输入到流程图。 graphinput[type=input] 上述配置表示，图输入点的名称为graphinput，在使用SDK形式调用ModelBox时可以使用此名称发送数据给图。 output: 表示此点的类型为输出端口，为整个图的配置，表示图的数据输出端口，主要用于数据输出到流程图外。 graphoutput[type=output] 上述配置表示，图输出点的名称为graphoutput，在使用SDK形式调用ModelBox时可以使用此名称接收图处理后的数据。 第三部分是点的关系定义 格式 flowunit_name1:outport -> flowunit_name2:inport 说明 flowunit_name1为点的名称，outport为输出端口名称，inport为输入端口名称。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/flowunit/flowunit.html":{"url":"use-modelbox/standard-mode/flowunit/flowunit.html","title":"功能单元开发","keywords":"","body":"功能单元开发 在完成了流程图编排之后，还需通过功能单元来实现AI应用的实际功能。有关功能单元的详细介绍可以查看基本概念章节，以及相关的功能单元、数据流章节内容。ModelBox提供了多种功能单元的开发接口，本章节内容主要介绍功能单元的开发过程。 功能单元开发类型 ModelBox功能单元的开发可以使用多种语言和方式，根据实现方式差异，功能单元可以分为四种类型： 方式 适合场景 复杂度 链接 C++功能单元 适合有高性能要求的功能开发，需要编译成so，开发复杂度稍高。 ⭐️⭐️⭐️ 指导 Python功能单元 适合对性能要求不高的业务逻辑开发，适用于应用需要快速上线场景。 ⭐️⭐️ 指导 推理功能单元 适合模型推理类功能的开发，无需写代码，配置好模型即可运行，方便快捷。 ⭐️ 指导 配置功能单元 适合ModelBox提供的特定功能的配置功能单元开发，如YOLO后处理，无需写代码，编写配置即可运行，方便快捷。 ⭐️ 指导 开发者可以选择使用合适的方式进行开发，也可以多种方式混合。 功能单元开发流程 功能单元的开发总体分为以下几个步骤： 创建功能单元 通过WebUI或者命令行的方式创建出对应设备的功能单元模板。 配置功能单元属性 根据业务需求对功能单元的数据处理类型、输入、输出等关键属性进行配置。 实现功能单元逻辑 根据本功能单元需要完成的功能，实现Open，Process，Close等接口。 编译运行 编译生成动态库以及安装包，安装到系统中，并在流程图配置运行。 每种类型功能单元的开发流程存在部分差异，其中推理功能单元和配置功能单元无需实现功能单元逻辑。详细开发步骤见如下章节： C++功能单元 Python功能单元 推理功能单元 配置类功能单元 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/flowunit/c++.html":{"url":"use-modelbox/standard-mode/flowunit/c++.html","title":"C++功能单元","keywords":"","body":"C++功能单元开发 本章节主要介绍C++功能单元的开发流程。在开发之前，可以从功能单元概念章节了解功能单元的执行过程。 功能单元创建 ModelBox提供了多种方式进行C++功能单元的创建: 通过UI创建 可参考可视化编排服务的任务编排页面章节操作步骤。 通过命令行创建 ModelBox提供了模板创建工具，可以通过ModelBox Tool工具产生C++功能单元的模板，具体命令为 modelbox-tool template -flowunit -project-path [project_path] -name [flowunit_name] -lang c++ -input name=in1,device=cuda -output name=out1 该命令将会在$project_path/src/flowunit目录下创建名为flowunit_name的C++功能单元。 创建完成的C++功能单元目录结构如下： [flowunit-name] |---CMakeList.txt # 编译文件，C++功能单元采用CMake进行编译 |---[flowunit-name].cc # 接口实现文件 |---[flowunit-name].h # 头文件 |---[flowunit-name].toml # 配置文件，用于WebUI显示 功能单元属性配置 在开发功能单元时，应该先明确开发功能单元处理数据的类型、业务的场景等，根据需求来配置功能单元类型和属性。具体功能单元类型，请查看功能单元基础概念功能单元类型章节。在确认功能单元类型后，需要对功能单元进行参数的配置。 属性配置包含两部分：Driver插件属性和FlowUnit功能单元属性。Driver和功能单元的关系如下：Driver是ModelBox中各类插件的集合，功能单元属于Driver中的一种类型。在C++语言中，一个Driver对应一个so，一个Driver可以支持注册多个功能单元，即将多个功能单元编译进一个so文件。而再Python中，Driver和功能单元一一对应。如无特殊需求，通常使用一个Driver对应一个功能单元。 典型的属性配置代码如下： #include \"modelbox/flowunit_api_helper.h\" // 设置FlowUnit功能单元属性 // `ExampleFlowUnit`为对应的插件功能单元派生对象，从FlowUnit派生出来的类 // `MODELBOX_FLOWUNIT`: 一个Driver内部可以注册多个功能单元，`MODELBOX_FLOWUNIT`可以设置多个不同的FlowUnit。 MODELBOX_FLOWUNIT(ExampleFlowUnit, desc) { desc.SetFlowUnitName(FLOWUNIT_NAME); // 功能单元名称 desc.SetFlowUnitGroupType(\"Undefined\"); // 功能单元组类型 desc.SetFlowType(modelbox::STREAM); // 功能单元工作模式 desc.AddFlowUnitInput({\"in\", FLOWUNIT_TYPE}); // 功能单元输入端口，名称叫 \"in\", 同时输入数据如果不在FLOWUNIT_TYPE设备上，则框架会自动将其搬移至该设备上 desc.AddFlowUnitOutput({\"out\"}); // 功能单元输出端口，名称叫 \"out\" desc.SetDescription(FLOWUNIT_DESC); // 功能单元描述 } // 设置Driver插件相关属性 MODELBOX_DRIVER_FLOWUNIT(desc) { desc.Desc.SetName(FLOWUNIT_NAME); // Driver名称 desc.Desc.SetClass(modelbox::DRIVER_CLASS_FLOWUNIT); // Driver功能类型，功能单元取值固定 desc.Desc.SetType(modelbox::DEVICE_TYPE); // Driver和功能单元设备类型，取值：cpu、cuda、ascend desc.Desc.SetVersion(FLOWUNIT_VERSION); // Driver版本号 desc.Desc.SetDescription(FLOWUNIT_DESC); // Driver描述 desc.Init([]() { // 如果有需要Driver相关的初始化功能，可在此实现，插件启用时调用. return modelbox::STATUS_OK; }); desc.Exit([]() { // 如果有需要Driver相关的去初始化功能，可在此实现，插件关闭时调用. return modelbox::STATUS_OK; }); return; } 通常情况下，CPU类型业务功能单元只需确认输入输出端口名即可。如果需要设置其他属性，可参考如下功能单元参数说明： 配置项 配置接口 必填 参数类型 功能描述 功能单元名称 SetName 是 String 功能单元名称 功能单元描述 SetDescription 否 String 功能单元描述 功能单元分组类别 SetFlowUnitGroupType 否 GroupType 功能单元分组类别，用于UI分组显示 功能单元数据处理类型 SetFlowType 是 FlowType 功能单元数据处理类型 ，取值为：NORMAL 和 STREAM，差异详见功能单元类型章节。 默认建议配置为 STREAM 条件类型 SetConditionType 否 ConditionType 是否为条件功能单元，取值为： NONE、IF_ELSE。差异详见功能单元类型章节。 输出类型 SetOutputType 否 FlowOutputType 设置是否为扩张或者合并功能单元，取值为: ORIGIN、EXPAND、COLLAPSE。差异详见功能单元类型章节。 功能单元输入端口 AddFlowUnitInput 是 FlowUnitInput 设置输入端口名和数据存放设备类型， 数据存放设备类型不设置时，默认与功能单元设备类型一致。 当需要操作数据时，如果前面功能单元输出数据与本功能单元输入端口配置的设备类型不一致时，ModelBox框架会自动搬移至目标设备。 功能单元输出端口 AddFlowUnitOutput 是 FlowUnitOutput 设置输出端口，输出数据存放设备类型固定与功能单元设备类型一致。 功能单元配置参数 AddFlowUnitOption 是 FlowUnitOption 设置功能单元配置参数，包括参数名，类型，描述等信息。目前用于UI显示。 输入内存是否连续 SetInputContiguous 否 bool 是否要求一次输入的一组Buffer内存地址是否连续，默认为false。 异常是否可见 SetExceptionVisible 否 bool 本功能单元是否需要捕获前面流程的异常，默认为false 功能单元逻辑实现 功能单元接口说明 ModelBox框架提供的功能单元开发C++接口如下，开发者按需实现： 接口 功能说明 是否必须 使用说明 FlowUnit::Open 功能单元初始化 否 实现功能单元的初始化，资源申请，配置参数获取等 FlowUnit::Close 功能单元关闭 否 实现资源的释放 FlowUnit::Process 功能单元数据处理 是 实现核心的数据处理逻辑 FlowUnit::CudaProcess cuda类型功能单元数据处理 否 实现cuda类型功能单元核心数据处理逻辑，以替代Process，当功能单元类型为cuda时生效 FlowUnit::AscendProcess ascend类型功能单元数据处理 否 实现ascend类型功能单元核心数据处理逻辑，以替代Process，当功能单元类型为ascend时生效 FlowUnit::DataPre 功能单元Stream流开始 否 实现Stream流开始时的处理逻辑，功能单元数据处理类型是STREAM时生效 FlowUnit::DataPost 功能单元Stream流结束 否 实现Stream流结束时的处理逻辑，功能单元数据处理类型是STREAM时生效 功能单元初始化/关闭接口 对应需实现的接口为FlowUnit::Open、FlowUnit::Close，实现样例如下： modelbox::Status ExampleFlowUnit::Open( const std::shared_ptr &opts) { // 获取流程图中功能单元配置参数值，进行功能单元的初始化 auto pixel_format = opts->GetString(\"pixel_format\", \"bgr\"); ... return modelbox::STATUS_OK; } modelbox::Status ExampleFlowUnit::Close() { // 释放功能单元的公共资源 ... return modelbox::STATUS_OK; } Open函数将在图初始化的时候调用，const std::shared_ptr &opts为流程图Toml配置中功能单元的配置参数，可调用相关的接口获取配置，返回modelbox::STATUS_OK，表示初始化成功，否则初始化失败。 数据处理接口 对应接口为FlowUnit::Process, 其为功能单元最核心函数，输入数据的处理、输出数据的构造都在此函数中实现。接口处理流程实现大致如下： 通过配置的输入输出端口名，从DataContext中获取输入BufferList、输出BufferList对象。 循环处理每一个输入Buffer数据，默认STREAM类型一次只处理一个数据，不必循环。 业务处理，获取每一个输入Buffer的Meta信息和Data信息，根据需求对输入数据进行处理。 构造输出Buffer，对每一个输出Buffer数据设置Meta信息和Data信息。 返回成功后，ModelBox框架将数据发送到后续的功能单元。 cpu功能单元的实现样例如下： modelbox::Status ExampleFlowUnit::Process(std::shared_ptr ctx) { // 获取输入输出BufferList对象，\"input\", \"output\"为对应功能单元Port名称，可以有多个。 // 此处的\"input\"和\"output\"必须与toml的端口名称一致 auto input_bufs = ctx->Input(\"input\"); auto output_bufs = ctx->Output(\"output\"); // 循环处理每个输入数据，并产生相关的输出结果。默认情况一次传递一个buffer进行处理，可以通过 // input_bufs->Front() 获取。 如果需要batch并发处理则需要修改功能单元数量处理类型为\"NORMAL\" for (auto &input : *input_bufs) { // 通过key 获取输入Buffer的Meta信息 auto input_meta = input->Get(\"key\", \"default_value\"); // 获取输入Buffer的Data数据指针，该数据只读只读，且数据已确保在输入设备类型上 auto input_data = input->ConstData(); ... // 根据当前功能单元设备类型构造buffer auto output_buffer = std::make_shared(GetBindDevice()); // 业务逻辑处理 ... //结果转换成输出Buffer，下面以string转成buffer为例 std::string test_str = \"test string xxx\"; // 申请内存，单位是字节数 output_buffer->Build(test_str.size()); // 获取输出Buffer Data指针 auto output_data = static_cast(output_buffer->MutableData()); // 拷贝string到buffer中。假设输出为cpu设备，则这里使用cpu内存拷贝 if(memcpy_s(output_data, output_buffer->GetBytes(), test_str.data(), test_str.size()) != 0 ) { MBLOG_ERROR Set(\"key\", \"value\"); // 将输出Buffer放入到输出Bufferlist中 output_bufs->PushBack(output_buffer); } return modelbox::STATUS_OK; } FlowUnit::Process的返回值说明如下： STATUS_OK: 返回成功，将输出Buffer发送到后续功能单元处理。 STATUS_CONTINUE: 返回成功，暂缓发送输出Buffer的数据。 STATUS_SHUTDOWN: 停止数据处理，终止整个流程图。 其他: 停止数据处理，当前数据处理报错。 目前ModelBox支持开发cuda 和 ascend类型的功能单元，与cpu类型不同，cuda和ascend上进行编程存在CUDA Stream、ACL Stream的概念，所以接口上有些差异，新增了FlowUnit::CudaProcess、FlowUnit::AscendProcess替代FlowUnit::Process ， 具体参考下列编程接口： modelbox::Status ExampleFlowUnit::CudaProcess(std::shared_ptr data_ctx, cudaStream_t stream) { // 实现核心业务逻辑。 接口携带cuda stream ，可直接用于调用cuda异步接口。 // 如果调用ascend同步接口，则需要先调用cudaStreamSynchronize(stream)同步数据。 ... } modelbox::Status ExampleFlowUnit::AscendProcess(std::shared_ptr data_ctx, aclrtStream stream) { // 实现核心业务逻辑。 接口携带acl stream ，可直接用于调用acl异步接口。 // 如果调用cuda同步接口，则需要先调用aclrtSynchronizeStream(stream)同步数据。 ... } 更多关于加速设备上的功能单元开发详细说明可参考多设备开发章节和Ascend类型、Nvida CUDA类型接口说明。 Stream流数据开始/结束接口 Stream数据流的概念介绍可参考数据流章节。对应需实现的接口为FlowUnit::DataPre、FlowUnit::DataPost，此接口针对Stream类型的功能单元生效。使用的典型场景如处理一个视频流时，在视频流开始时会调用FlowUnit::DataPre，视频流结束时会调用FlowUnit::DataPost。功能单元可以在DataPre阶段初始化解码器，在DataPost阶段关闭解码器，解码器的相关句柄可以设置到DataContext上下文中，在Process阶段使用。 接口处理流程大致如下： Stream流数据开始时，在DataPre中获取数据流元数据信息，并初始化相关的上下文，使用SetPrivate存储在DataContext中。 处理Stream流数据时，在Process中，使用GetPrivate获取到上下文对象，并从输入端口中获取输入，处理后，结果设置到输出端口。 Stream流数据结束时，在DataPost中释放相关的上下文信息。 使用场景及约束如下： FlowUnit::DataPre、FlowUnit::DataPost 阶段无法操作Buffer数据，仅用于 FlowUnit::Process中需要用到的一些资源的初始化，如解码器等。 FlowUnit::DataPre、FlowUnit::DataPost 不能有长耗时操作，比如文件下载、上传等，会影响并发性能。 以视频解码为例的样例如下： modelbox::Status ExampleFlowUnit::DataPre(std::shared_ptr data_ctx) { // 初始化Stream流数据处理上下文对象。 auto decoder = CreateDecoder(stream_meta); // 保存流数据处理上下文对象。 data_ctx->SetPrivate(\"Decoder\", decoder); ... return modelbox::STATUS_OK; } modelbox::Status ExampleFlowUnit::Process(std::shared_ptr ctx) { auto inputs = ctx->Input(\"input\"); auto outputs = ctx->Output(\"output\"); // 获取流数据处理上下文对象。 auto decoder = data_ctx->GetPrivate(\"Decoder\"); // 处理输入数据。 decoder->Decode(inputs, outputs); ... return modelbox::STATUS_OK; } modelbox::Status ExampleFlowUnit::DataPost(std::shared_ptr data_ctx) { // 关闭解码器。 auto decoder = data_ctx->GetPrivate(\"Decoder\"); decoder->DestroyDecoder(); ... return modelbox::STATUS_OK; } Buffer操作 在实现核心数据逻辑时，需要对Buffer进行操作：获取输入Buffer数据，处理结果转换为输出Buffer往后传递, Buffer拷贝等。Buffer数据包含了Meta数据描述信息和Data数据主体，Buffer的详细介绍看参考基础概念的Buffer章节。ModelBox提供了常用的Buffer接口用于完成复杂的业务逻辑。 获取输入Buffer信息 开发者可以根据功能单元属性配置中的输入端口名称获取输入数据队列BufferList，再获取单个Buffer对象即可获取Buffer的各种属性信息：数据指针、数据大小、Meta字段等等。 此外BufferList也提供了快速获取数据指针的接口，样例如下： modelbox::Status ExampleFlowUnit::Process(std::shared_ptr data_ctx) { // 根据输入端口名称获取输入Buffer队列，输入端口名为\"input\" auto input_bufs = data_ctx->Input(\"input\"); for (auto i = 0; i Size(); ++i) { // 方式一：先获取Buffer对象，再获取Buffer属性：数据指针、数据大小、Meta字段 auto buffer = input_bufs->At(i); const void* buffer_data1 = buffer->ConstData(); auto buffer_size = buffer->GetBytes(); int32_t height; auto exists = buffer->Get(\"height\", height); if (!exists) { MBLOG_ERROR ConstBufferData(i); ... } } 输入Buffer透传给输出端口 此场景是将输入Buffer直接作为输出Buffer向后传递，此时Buffer的数据、Meta等全部属性都将保留。此场景一般用于不需要实际访问数据的功能单元，如视频流跳帧。 modelbox::Status ExampleFlowUnit::Process(std::shared_ptr data_ctx) { // 所有输入透传给输出端口，输入端口名为\"input\", 输出端口名为\"output\" auto input_bufs = data_ctx->Input(\"input\"); auto output_bufs = data_ctx->Output(\"output\"); for (auto &buf: input_bufs) { ... output_bufs->PushBack(buf); } return STATUS_OK; } 创建输出Buffer 数据处理完成后，需要创建输出Buffer并把结果数据填充进输出Buffer，设置Buffer Meta。ModelBox提供多种方式创建Buffer： BufferList::Build : 一次创建多个指定大小的空Buffer, Buffer类型与当前功能单元硬件类型一致。Buffer Data内容需要单独填充。 BufferList::BuildFromHost : 一次创建多个指定大小的Buffer，Buffer类型为cpu类型，Buffer数据在创建时写入，一次调用完成创建和赋值。 BufferList::EmplaceBack ： 调用时隐式创建Buffer，Buffer类型与当前功能单元硬件类型一致。Buffer数据在调用时写入。一次调用完成创建和赋值，较BufferList::Build相比简单。 BufferList::EmplaceBackFromHost ： 调用时隐式创建Buffer，Buffer类型为cpu类型。Buffer数据在调用时写入。 Buffer构造函数 ： 直接调用Buffer的构造函数。 modelbox::Status ExampleFlowUnit::Process(std::shared_ptr data_ctx) { auto output_bufs = data_ctx->Output(\"output\"); // 方式一 Build：创建当前功能单元硬件类型相同的多个空Buffer，再填充数据 vector data_size_list; data_size_list.emplace_back(size1); data_size_list.emplace_back(size2); output_bufs->Build(data_size_list); for (auto i = 0; i Size(); ++i ) { auto output_buffer = output_bufs->At(0); auto output_data = output_buffer->MutableData(); // 给输出Buffer填充数据 memcpy_s(output_data, output_buf->GetBytes(), data, data_size); // 设置Buffer Meta output_buffer->Set(\"width\", width); ... } ... //方式二 BuildFromHost：创建cpu类型的多个Buffer并同时填充数据 vector data_size_list{1,1,1}; vector data_list{122,123,124} output_bufs->BuildFromHost(shape, data.data(), 12); ... // 方式三 EmplaceBack/EmplaceBackFromHost：通过开发者自行创建的设备数据直接创建Buffer void* device_ready_data1 ; std::shared_ptr device_ready_data2 ; void* host_ready_data1 ; ... //用户数据在设备上，且未通过智能指针管理 output_bufs->EmplaceBack(device_ready_data1, data_size, [](void*){}) //用户数据在设备上，通过智能指针管理 output_bufs->EmplaceBack(device_ready_data2, data_size); //用户数据在cpu内存上 output_bufs->EmplaceBackFromHost(host_ready_data1, data_size); ... //方法四：先构造Buffer，再放入BufferList auto output_buffer = std::make_shared(GetBindDevice()); ... output_bufs->PushBack(output_buffer); } Buffer的拷贝 Buffer的数据拷贝分三种情况：浅拷贝、深拷贝、拷贝Meta。它们的区别如下： 浅拷贝：接口为Copy，拷贝Meta信息和Data数据指针，源Buffer和目标Buffer共享数据内容。 深拷贝：接口为DeepCopy，拷贝Meta信息和Data数据内容，源Buffer和目标Buffer数据完全独立。 拷贝Meta：接口为CopyMeta， 仅拷贝Meta信息，不拷贝Data数据部分。 modelbox::Status ExampleFlowUnit::Process(std::shared_ptr data_ctx) { auto input_bufs = data_ctx->Input(\"input\"); auto output_bufs = data_ctx->Output(\"output\"); auto buffer = input_bufs->At(0); // 浅拷贝， buffer和new_buffer 数据指针指向同一份数据 auto new_buffer = buffer->Copy(); // 深拷贝， buffer和new_buffer 数据指针指向不同数据，数据内容一样 auto new_deep_buffer = buffer->DeepCopy(); output_bufs->Build({1}); auto out_buffer = output_bufs->At(0); // 仅拷贝Buffer Meta信息， out_buffer和buffer Meta信息完全一致 out_buffer->CopyMeta(buffer); ... } 更多Buffer操作见API接口， Buffer的异常处理见异常章节。 DataContext与SessionContext 功能单元上下文包含：会话上下文|SessionContext和数据上下文|DataContext DataContext 数据上下文：DataContext是提供给当前功能单元处理数据时的临时获取BufferList 功能单元处理一次Stream流数据，或一组数据的上下文，当数据生命周期不再属于当前功能单元时，DataContext生命周期也随之结束。 生命周期：功能单元内部，从流数据进入功能单元到处理完成。 使用场景：通过DataContext->Input接口获取输入端口BufferList；通过DataContext->Output接口获取输出端口BufferList对象；通过DataContext->SetPrivate接口设置临时对象；DataContext->GetPrivate接口获取临时对象。 SessionContext 会话上下文： SessionContext主要供调用图的业务使用，业务处理数据时，设置任务基本状态对象。 生命周期：多功能单元之间生效，任务级别。一次图的输入数据（ExternalData），从数据进入Flow，贯穿整个图，一直到数据处理完成结束。 使用场景：例如HTTP服务同步响应场景，首先接收到HTTP请求后转化成Buffer数据，然后通过ExternalData->GetSessionContext接口获取到SessionContext，接着调用SessionContext->SetPrivate设置响应的回调函数，之后通过ExternalData->Send接口把Buffer数据发送到flow中；经过中间的业务处理功能单元；最后HTTP响应功能单元中在业务数据处理完成后，再调用SessionContext->GetPrivate获取响应回调函数，发送HTTP响应。至此SessionContext也结束。 DataContext 和 SessionContext提供了如下功能用于复杂业务的开发： 通过DataContext获取输入输出BufferList 通过输入输出端口名获取输入以及输出数据 modelbox::Status ExampleFlowUnit::Process(std::shared_ptr data_ctx) { // 通过端口输入输出端口名获取BufferList，端口名分别为input, output auto input_bufs = data_ctx->Input(\"input\"); auto output_bufs = data_ctx->Output(\"output\"); ... } 通过DataContext保存本功能单元Stream流级别数据 对于Stream流的一组数据，在本功能单元内DataPre、每次Process、 DataPost接口内可以通过SetPrivate接口设置数据来保存状态和传递信息，通过GetPrivate获取数据。如DataPre和Process间的数据传递，上一次Process和下一次Process间的数据传递。具体使用样例如下： modelbox::Status ExampleFlowUnit::DataPre(std::shared_ptr data_ctx) { // 保存Stream流级别上下文对象。 data_ctx->SetPrivate(\"key\", value); ... return modelbox::STATUS_OK; } modelbox::Status ExampleFlowUnit::Process(std::shared_ptr data_ctx) { auto inputs = ctx->Input(\"input\"); auto outputs = ctx->Output(\"output\"); // 获取Stream流级别上下文对象。 auto param = data_ctx->GetPrivate(\"key\"); ... // 保存Stream流级别上下文对象。 data_ctx->SetPrivate(\"key\", value); ... return modelbox::STATUS_OK; } modelbox::Status ExampleFlowUnit::DataPost(std::shared_ptr data_ctx) { // 获取Stream流级别上下文对象。 auto decoder = data_ctx->GetPrivate(\"key\"); ... return modelbox::STATUS_OK; } 通过DataContext获取输入输出端口Meta信息 除了Buffer外，开发者可以通过输入输出端口Meta传递信息，前一个功能单元设置输出Meta，后面功能单元获取输入Meta。端口的Meta信息传递不同与Buffer Meta，Buffer Meta是数据级别， 而前者是Stream流级别。 modelbox::Status ExampleFlowUnit::Process(std::shared_ptr data_ctx) { // 获取输入端口Meta里的字段信息 auto input_meta = data_ctx->GetInputMeta(\"input\"); auto value = std::static_pointer_cast(input_meta->GetMeta(\"key\")); ... // 设置Meta到输出端口 auto output_meta = std::make_shared(); output_meta->SetMeta(\"key\", value); data_ctx->SetOutputMeta(\"output\", output_meta); return modelbox::STATUS_OK; } 通过DataContextStream判断流异常 判断Stream流数据中处理是否存在异常，Stream流包含多个Buffer时，只要有一个Buffer存在异常即认为Stream流存在异常。 modelbox::Status ExampleFlowUnit::Process(std::shared_ptr data_ctx) { auto res = data_ctx->HasError(); ... return modelbox::STATUS_OK; } 通过DataContext发送事件 在开发功能单元时，存在通过功能单元自驱动的场景。如视频解码时，在输入一次视频地址数据后，后续在没有数据驱动的情况下需要反复调度解封装功能单元。 此时需要通过在功能单元中发送事件，来驱动调度器在没有数据的情况下继续调度该功能单元。 modelbox::Status ExampleFlowUnit::Process(std::shared_ptr data_ctx) { ... auto event = std::make_shared(); data_ctx->SendEvent(event); //返回值需要是 STATUS_CONTINUE，已保证本功能单元继续调度 return STATUS_CONTINUE; } 通过SessionContext存储任务级别全局数据 存储任务的全局变量，可用于在多个功能单元之间共享数据。SessionContext的全局数据的设置和获取方式如下： modelbox::Status ExampleFlowUnit1::Process(std::shared_ptr data_ctx) { auto session_cxt = data_ctx->GetSessionContext(); session_cxt->SetPrivate(\"key\", value); ... return modelbox::STATUS_OK; } modelbox::Status ExampleFlowUnit2::Process(std::shared_ptr data_ctx) { auto session_cxt = data_ctx->GetSessionContext(); auto value = session_cxt->GetPrivate(\"key\"); ... return modelbox::STATUS_OK; } 功能单元编译运行 ModelBox框架C++工程统一使用CMake进行编译，通过命令行或者可视化UI创建的功能单元中默认包含CMakeLists.txt文件，主要功能如下： 设置功能单元名称 链接功能单元所需头文件 链接功能单元所需库 设置编译目标为动态库 指定功能单元安装目录 功能单元编译生成的so命名需要以libmodelbox-开头，否则ModelBox无法扫描。 通常情况开发cpu业务功能单元，开发者无需修改CMakeLists.txt即可完成编译，当存在引入第三方库时、设置cuda/ascend类型、修改编译选项等等诉求时需要自行修改。生成的so安装路径一般也无需修改，如果开发者需要改动，则需要将路径添加到图的扫描路径driver.dir中。 功能单元功能测试 ModelBox框架提供了基于Gtest的单元测试框架， 开发者可以编写测试用例进行功能单元的基础功能测试。 测试用例需要放在 $Project/test/flowunit/目录下，测试用例基本写作基本步骤如下： 构建流程图并运行，流程图的开始和结尾通过input、output端口连接，用于图与外部程序的数据交互。中间业务部分可以是单个功能单元，也可以是多个功能单元。 构造输入Buffer并发送至流程图的input端口 通过流程图的output端口获取输出结果，并进行预期校验 样例如下： class ExampleFlowUnitTest : public testing::Test { public: ExampleFlowUnitTest() : mock_modelbox_(std::make_shared()) {} protected: virtual void SetUp(){}; virtual void TearDown() { mock_modelbox_->Stop(); }; std::shared_ptr GetMockModelbox() { return mock_modelbox_; } private: std::shared_ptr mock_modelbox_; }; TEST_F(ExampleFlowUnitTest, TestCase1) { // 构建流程图 const std::string test_lib_dir = TEST_LIB_DIR; std::string toml_content = R\"( [log] level=\"DEBUG\" [driver] dir=[\")\" + test_lib_dir + \"\\\"]\\n \" + R\"([graph] graphconf = '''digraph demo { input1[type=input] resize_test[type=flowunit, flowunit=resize_test, device=cpu, deviceid=0, label=\" | \", image_width=128, image_height=128,batch_size=5] output1[type=output] input1 -> resize_test:in_1 resize_test:out_1 -> output1 }''' format = \"graphviz\" )\"; // 运行流程图 auto mock_modelbox = GetMockModelbox(); auto ret = mock_modelbox->BuildAndRun(\"graph_name\", toml_content, 10); EXPECT_EQ(ret, STATUS_SUCCESS); // 构造输入Buffer，包含Meta数据描述信息和Data数据主体 auto ext_data = mock_modelbox->GetFlow()->CreateExternalDataMap(); EXPECT_NE(ext_data, nullptr); auto buffer_list = ext_data->CreateBufferList(); EXPECT_NE(buffer_list, nullptr); auto img = cv::imread(std::string(TEST_ASSETS) + \"/test.jpg\"); buffer_list->Build({img.total() * img.elemSize()}); auto buffer = buffer_list->At(0); buffer->Set(\"width\", img.cols); buffer->Set(\"height\", img.rows); buffer->Set(\"width_stride\", img.cols * 3); buffer->Set(\"height_stride\", img.rows); buffer->Set(\"pix_fmt\", std::string(\"bgr\")); memcpy(buffer->MutableData(), img.data, img.total() * img.elemSize()); // 发送Buffer到图的input端口，端口名与流程图中input端口名一致 auto status = ext_data->Send(\"input1\", buffer_list); EXPECT_EQ(status, STATUS_OK); status = ext_data->Shutdown(); EXPECT_EQ(status, STATUS_OK); // 通过output端口等待获取输出Buffer，端口名与流程图中output端口名一致 std::vector> output_buffer_lists = mock_modelbox->GetOutputBufferList(ext_data, \"output1\"); // 校验输出结果 EXPECT_EQ(output_buffer_lists.size(), 1); auto output_buffer_list = output_buffer_lists[0]; EXPECT_EQ(output_buffer_list->Size(), 1); auto output_buffer = output_buffer_list->At(0); int32_t width; int32_t height; auto exists = output_buffer->Get(\"width\", width); EXPECT_EQ(exists, true); exists = output_buffer->Get(\"height\", height); EXPECT_EQ(exists, true); } 测试用例的运行可以通过命令行，也可以通过vscode等IDE功能运行，方便调试。具体运行命令如下： $Project/build/test/unit/unit --gtest_filter=ExampleFlowUnitTest.* ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/flowunit/python.html":{"url":"use-modelbox/standard-mode/flowunit/python.html","title":"Python功能单元","keywords":"","body":"Python开发功能单元 本章节主要介绍Python功能单元的开发流程。在开发之前，可以从功能单元概念章节了解功能单元的执行过程。 功能单元创建 ModelBox提供了多种方式进行Python功能单元的创建： 通过UI创建 可参考可视化编排服务的任务编排页面章节操作步骤。 通过命令行创建 ModelBox提供了模板创建工具，可以通过ModelBox Tool工具产生Python功能单元的模板，具体的命令为 modelbox-tool template -flowunit -project-path [project_path] -name [flowunit_name] -lang python -input name=in1,device=cpu -output name=out1 该命令将会在$project_path/src/flowunit目录下创建名为flowunit_name的Python功能单元。 ModelBox框架在初始化时，会扫描/path/to/flowunit/$flowunit_name目录中的toml后缀的文件，并读取相关的信息，具体可通过ModelBox Tool工具查询。 创建完成的C++功能单元目录结构如下： [flowunit-name] |---CMakeList.txt # 用于CPACK 打包 |---[flowunit-name].toml # 功能单元属性配置文件 |---[flowunit-name].py # 接口实现文件 功能单元属性配置 在开发功能单元时，应该先明确开发功能单元处理数据的类型、业务的场景。再根据需求来合理配置功能单元类型和属性。具体功能单元类型，请查看功能单元类型章节。在确认功能单元类型后，需要对功能单元进行参数的配置。 Python功能单元典型的属性配置代码如下： # 基础配置 [base] name = \"flowunit-name\" # 功能单元名称 device = \"cpu\" # 功能单元运行的设备类型，Python 功能单元仅支持cpu类型。需要操作数据时，如果前面功能单元输出数据为非cpu，ModelBox框架会自动搬移至cpu version = \"1.0.0\" # 功能单元组件版本号 description = \"description\" # 功能单元功能描述信息 entry = \"python-module@ExampleFlowunit\" # Python 功能单元入口函数 type = \"python\" # Python功能单元时，此处为固定值 # 工作模式 stream = true # 是否是Stream类型功能单元 condition = false # 是否是条件功能单元 collapse = false # 是否是合并功能单元 expand = false # 是否是拆分功能单元 # 自定义配置项 [config] item = value # 输入端口描述 [input] [input.input1] # 输入端口编号，格式为input.input[N] name = \"input\" # 输入端口名称 type = \"datatype\" # 输入端口数据类型 # 输出端口描述 [output] [output.output1] # 输出端口编号，格式为output.output[N] name = \"output\" # 输出端口名称 type = \"datatype\" # 输出端口数据类型 功能单元逻辑实现 功能单元接口说明 功能单元提供的接口如下，开发者按需实现： 接口 功能说明 是否必须 使用说明 FlowUnit::open 功能单元初始化 否 实现功能单元的初始化，资源申请，配置参数获取等 FlowUnit::close 功能单元关闭 否 实现资源的释放 FlowUnit::process 功能单元数据处理 是 实现核心的数据处理逻辑 FlowUnit::data_pre 功能单元Stream流开始 否 实现Stream流开始时的处理逻辑，功能单元属性 base.stream = true 时生效 FlowUnit::data_post 功能单元Stream流结束 否 实现Stream流结束时的处理逻辑，功能单元数据处理类型是base.stream = true 时生效 功能单元初始化/关闭接口 对应的需要实现的接口为FlowUnit::open, FlowUnit::close接口，实现样例如下： import _flowunit as modelbox class ExampleFlowUnit(modelbox.FlowUnit): def open(self, config): # 获取流程图中功能单元配置参数值，进行功能单元的初始化 config_item = config.get_float(\"key\", \"default_value\") ... return modelbox.Status.StatusCode.STATUS_SUCCESS def close(self): # 释放功能单元的公共资源 ... return modelbox.Status.StatusCode.STATUS_SUCCESS Open函数将在图初始化的时候调用，config为功能单元的配置参数，可调用相关的接口获取配置，返回modelbox.Status.StatusCode.STATUS_SUCCESS，表示初始化成功，否则初始化失败。 数据处理接口 对应的需要实现的接口为FlowUnit::process接口，其为功能单元最核心函数，输入数据的处理、输出数据的构造都在此函数中实现。接口处理流程大致如下： 通过配置的输入输出端口名，从DataContext中获取输入BufferList、输出BufferList对象 循环处理每一个输入Buffer数据，默认STREAM类型一次只处理一个数据，不必循环 将输入Buffer转换为numpy、string等常用对象，并编写业务处理逻辑。 将业务处理结果返回的结果数据调用self.create_buffer转换为Buffer 设置输出Buffer的Meta信息。 将输出Buffer放入输出BufferList中。 返回成功后，ModelBox框架将数据发送到后续的功能单元。 实现样例如下： import _flowunit as modelbox class ExampleFlowUnit(modelbox.FlowUnit): def process(self, data_ctx): # 获取输入输出BufferList对象，\"input\", \"output\"为对应功能单元Port名称，可以有多个。 # 此处的\"input\"和\"output\"必须与toml的端口名称一致 inputs = data_ctx.input(\"input\") outputs = data_ctx.output(\"output\") # 循环处理每一个input输入，并产生相关的输出结果，默认情况一次处理一个Buffer，则可去掉循环 for buffer in inputs: # Buffer对象转为numpy对象 np_in_data = np.array(buffer, copy= False) # 业务逻辑处理 np_out_data = process_data(np_in_data) # numpy对象转为Buffer out_buffer = self.create_buffer(np_out_data) # 设置Buffer Meta out_buffer.set(\"key\", value) # 将输出Buffer放入到输出Bufferlist中 outputs.push_back(out_buffer) return modelbox.Status.StatusCode.STATUS_SUCCESS process的返回值说明如下： STATUS_SUCCESS: 返回成功，将输出Buffer发送到后续功能单元处理。 STATUS_CONTINUE: 返回成功，暂缓发送输出Buffer的数据。 STATUS_SHUTDOWN: 停止数据处理，终止整个流程图。 其他: 停止数据处理，当前数据处理报错。 Stream流数据开始/结束接口 Stream数据流的概念介绍可参考数据流章节。对应需实现的接口为FlowUnit::data_pre、FlowUnit::data_post，此接口针对Stream类型的功能单元生效。开发者可以data_pre阶段设置Stream流级别信息存储在DataContext，在process或者data_post使用或者更新。 使用场景及约束如下： FlowUnit::data_pre、FlowUnit::data_post 阶段无法操作Buffer数据，仅用于 FlowUnit::process中需要用到的一些资源的初始化，如解码器等 FlowUnit::data_pre、FlowUnit::data_post 不能有长耗时操作，比如文件下载、上传等，会影响并发性能 实现样例如下： 对应需实现的接口为data_pre、data_post，此接口Stream模式可按需实现。实现样例如下： import _flowunit as modelbox class ExampleFlowUnit(modelbox.FlowUnit): def data_pre(self, data_ctx): # 保存Stream流级别信息。 data_ctx.set_private_string(\"key\", value) ... return modelbox.Status.StatusCode.STATUS_SUCCESS def process(self, data_ctx): inputs = data_ctx.input(\"input\") outputs = data_ctx.output(\"output\") # 获取Stream流级别信息。 value = data_ctx.get_private_string(\"key\") ... # 更新数据。 value = data_ctx.set_private_string(\"key\") ... return modelbox.Status.StatusCode.STATUS_SUCCESS def data_post(self, data_ctx): # 获取Stream流级别信息。 value = data_context.get_private_string(\"key\") ... return modelbox.Status.StatusCode.STATUS_SUCCESS Buffer操作 在实现核心数据逻辑时，需要对输入Buffer获取数据，也需要将处理结果通过输出端口往后传递。Buffer包含了Meta数据描述信息和Data数据主体两部进行操作，Buffer的详细介绍看参考基础概念的Buffer章节。ModelBox提供了常用的Buffer接口用于实现复杂的业务逻辑。 Python的Buffer处理与C++存在差异，由于Buffer类型为ModelBox特有类型，在Python中不通用，所以Python功能单元获取输入 Buffer后需要将Data内容转换为基础类型、string、numpy等常用数据类型，再进行操作。 获取输入Buffer信息 开发者可以根据功能单元属性配置中的输入端口名称获取输入数据队列BufferList，再获取单个Buffer对象即可获取Buffer的数据和 Meta信息。 样例如下： def process(self, data_ctx): input_bufs = data_ctx.input(\"in\") output_bufs = data_ctx.output(\"out\") for input_buf in input_bufs: # 获取Buffer Data数据并转化为numpy对象(输入数据为numpy类型) image = np.array(input_buf) # 获取Buffer Data数据并转化为string对象(输入数据为string类型) ss = input_buf.as_object() # 获取Buffer Meta信息 value = input_buf.get(\"key\") ... return modelbox.Status() 输入Buffer透传给输出端口 此场景是将输入Buffer直接作为输出Buffer向后传递，此时Buffer的Data、Meta等全部内容都将保留。此场景一般用于不需要实际访 问数据的功能单元，如视频流跳帧。 def process(self, data_ctx): input_bufs = data_ctx.input(\"in\") output_bufs = data_ctx.output(\"out\") for input_buf in input_bufs: output_bufs.push_back(input_buf) return modelbox.Status() 创建输出Buffer 业务处理完毕后处理结果一般是常用数据类型，开发者可以使用create_buffer接口将其转换为Buffer类型数据。 def process(self, data_ctx): input_bufs = data_ctx.input(\"input\") output_bufs = data_ctx.output(\"output\") for input_buf in input_bufs: # 若input_buf为string对象，ss即为ss ss = input_buf.as_object() ... #业务处理 ss += \" response\" # 创建输出buffer，并且push给output_bufs out_buf = self.create_buffer(ss) output_bufs.push_back(out_buf) return modelbox.Status() Buffer的拷贝 Python里不涉及Buffer类型拷贝，仅提供Buffer Meta的拷贝，接口为copy_meta， 仅拷贝Meta信息，不拷贝Data数据部分。 def process(self, data_ctx): buf_list = data_ctx.input(\"input\") for buf in buf_list: ... infer_data = np.ones((5,5)) new_buffer = self.create_buffer(infer_data) # 拷贝Buffer Meta status = new_buffer.copy_meta(buf) ... return modelbox.Status() 更多Buffer操作见API接口， Buffer的异常处理见异常章节。 DataContext与SessionContext 功能单元上下文包含：会话上下文|SessionContext和数据上下文|DataContext DataContext 数据上下文：DataContext是提供给当前功能单元处理数据时的临时获取BufferList 功能单元处理一次Stream流数据，或一组数据的上下文，当数据生命周期不再属于当前功能单元时，DataContext生命周期也随之结束。 生命周期：功能单元内部，从流数据进入功能单元到处理完成。 使用场景：获取输入、输出端口的BufferList对象；存储和获取本功能单元Stream流级别信息。 SessionContext 会话上下文： SessionContext主要供调用图的业务使用，业务处理数据时，设置任务基本状态对象。 生命周期：多功能单元之间生效，任务级别。一次图的输入数据（ExternalData），从数据进入Flow，贯穿整个图，一直到数据处理完成结束。 使用场景：保持任务级别状态信息，如任务级别参数等。 DataContext 和 SessionContext提供了如下功能用于复杂业务的开发： 通过DataContext获取输入输出BufferList 通过输入输出端口名获取输入以及输出数据 def process(self, data_ctx): # 通过端口输入输出端口名获取BufferList，端口名分别为input, output input_bufs = data_ctx.input(\"input\") output_bufs = data_ctx.output(\"output\") ... 通过DataContext保存本功能单元Stream流级别数据 对于Stream流的一组数据，在本功能单元内data_pre、每次process、 data_post接口内可以通过set接口设置数据来保存状态和传递信息，通过get获取数据。如data_pre和process间的数据传递，上一次process和下一次process间的数据传递。具体使用样例如下： def data_pre(self, data_ctx): # 保存Stream流级别信息。 data_ctx.set_private_string(\"key\", value) ... return modelbox.Status.StatusCode.STATUS_SUCCESS def process(self, data_ctx): inputs = data_ctx.input(\"input\") outputs = data_ctx.output(\"output\") # 获取Stream流级别信息。 value = data_ctx.get_private_string(\"key\") ... # 更新数据。 value = data_ctx.set_private_string(\"key\") ... return modelbox.Status.StatusCode.STATUS_SUCCESS def data_post(self, data_ctx): # 获取Stream流级别信息。 value = data_context.get_private_string(\"key\") ... return modelbox.Status.StatusCode.STATUS_SUCCESS 通过DataContext获取输入输出端口Meta信息 除了Buffer外，开发者可以通过输入输出端口Meta传递信息，前一个功能单元设置输出Meta，后面功能单元获取输入Meta。端口的Meta信息传递不同与Buffer Meta，Buffer Meta是数据级别， 而前者是Stream流级别。 def process(self, data_ctx): // 获取输入端口Meta里的字段信息 input_meta = data_ctx.get_input_meta(\"input\") value = input_meta.get_private_string(\"key\") ... // 设置Meta到输出端口 output_meta = modelbox.DataMeta() output_meta.set_private_string(\"key\", value); res = data_ctx.set_output_meta(\"output\", input_meta) return modelbox.Status() 通过DataContextStream判断流异常 判断Stream流数据中处理是否存在异常，Stream流包含多个Buffer时，只要有一个Buffer存在异常即认为Stream流存在异常。 def process(self, data_ctx): if data_ctx.has_error(): error = data_ctx.get_error() print(error.get_description(), type(error)) ... return modelbox.Status() 通过DataContext发送事件 在开发功能单元时，存在通过功能单元自驱动的场景。如视频解码时，在输入一次视频地址数据后，后续在没有数据驱动的情况下需要反复调度解封装功能单元。 此时需要通过在功能单元中发送事件，来驱动调度器在没有数据的情况下继续调度该功能单元。 def process(self, data_ctx): event = modelbox.FlowUnitEvent() data_ctx.send_event(event) ... return modelbox.Status() 通过SessionContext存储全局数据 存储任务的全局变量，可用于在多个功能单元之间共享数据。SessionContext的全局数据的设置和获取方式如下： class ExampleFlowUnit1(modelbox.FlowUnit): def process(self, data_ctx): session_ctx = data_ctx.get_session_context() session_ctx.set_private_string(\"key\", value) ... return modelbox.Status() class ExampleFlowUnit2(modelbox.FlowUnit): def process(self, data_ctx): session_ctx = data_ctx.get_session_context() value = session_ctx.get_private_string(\"key\") ... return modelbox.Status() 功能单元调试运行 Python功能单元无需编译，通常情况下调试阶段可以将此功能单元所在的文件夹路径配置到流程图的扫描路径driver.dir中，再通过ModelBox-Tool启动流程图运行，流程图启动时会按照配置路径扫描并加载Python功能单元。流程图的运行可参考流程图运行章节如果需要Python断点调试，可见代码调试章节。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/flowunit/inference.html":{"url":"use-modelbox/standard-mode/flowunit/inference.html","title":"推理功能单元","keywords":"","body":"推理功能单元 本章节介绍推理功能单元的开发流程。ModelBox内置了主流的推理引擎，如TensorFlow，TensorRT，LibTorch，Ascend ACL，MindSpore。在开发推理功能单元时，只需要通过配置toml文件，即可完成推理功能单元的开发。 开发之前，可以从功能单元概念章节了解流单的执行过程。 推理功能单元创建 ModelBox提供了多种方式进行推理功能单元的创建： 通过UI创建 可参考可视化编排服务的任务编排页面章节操作步骤。 通过命令行创建 ModelBox提供了模板创建工具，可以通过ModelBox Tool工具产生C++功能单元的模板，具体的命令为 modelbox-tool template -flowunit -project-path [project_path] -name [flowunit_name] -lang infer -virtual-type [type] -model [model_file_path] -copy-model -input name=in1,device=cuda -output name=out1 该命令将会在$project_path]/src/flowunit目录下创建名为flowunit_name的推理功能单元。 创建完成的C++功能单元目录结构如下： [flowunit-name] |---[flowunit-name].toml #推理功能单元配置 |---[model].pb #模型文件 |---[infer-plugin].so #推理自定义插件 ModelBox框架在初始化时，会扫描[some-flowunit]目录中的toml后缀的文件，并读取相关的推理功能单元信息。[infer-plugin].so是推理所需插件，推理功能单元支持加载自定义插件，开发者可以实现tensorRT 自定义算子。 推理功能单元配置 # 基础配置 [base] name = \"flowunit-name\" # 功能单元名称 device = \"cuda\" # 功能单元运行的设备类型，cpu，cuda，ascend等。 version = \"1.0.0\" # 功能单元组件版本号 description = \"description\" # 功能单元功能描述信息 entry = \"model.pb\" # 模型文件路径 type = \"inference\" # 推理功能单元时，此处为固定值 virtual_type = \"tensorrt\" # 指定推理引擎, 取值为tensorflow, tensorrt, torch, acl, mindspore plugin = \"infer-plugin.so\" # 推理自定义引擎插件 仅支持virtual_type为tensorflow, tensorrt # 模型解密配置 （非必须，可删除） [encryption] plugin_name = \"modeldecrypt-plugin\" # 可以修改为自己实现的解密插件名 plugin_version = \"1.0.0\" # 通常情况下，rootkey和passwd需要隐藏，不能配置在此处，实现自己的解密插件，从云端下载或者隐藏在代码内 rootkey = \"encrypt root key\" passwd = \"encrypt password\" # 输入端口描述 [input] [input.input1] # 输入端口编号，格式为input.input[N] name = \"input\" # 输入端口名称, 即模型输入Tensor名称 type = \"datatype\" # 输入端口数据类型, 取值float or uint8 device = \"cpu\" #输入数据存放位置，取值cpu/cuda/ascend，tensorflow框架只支持cpu，其他场景一般和base.device一致，可不填 # 输出端口描述 [output] [output.output1] # 输出端口编号，格式为output.output[N] name = \"output\" # 输出端口名称, 即模型输出Tensor名称 type = \"datatype\" # 输出端口数据类型, 取值float or uint8 编写完成toml文件后，将对应的路径加入ModelBox的图配置中的搜索路径即可使用开发后的推理功能单元。推理功能单元的输入端口和输出端口名称和个数的由toml文件指定，当模型存在多输入或者多输出时，流程图构建时需要针对每个输入端口和输出端口进行接口连接。格式如下： ... face_pre[type=flowunit, flowunit=face_post, device=cpu] model_detect[type=flowunit, flowunit=model_detect, device=cuda] face_post[type=flowunit, flowunit=face_post, device=cpu] ... face_pre:out_port1 -> model_detect:input1 face_pre:out_port2 -> model_detect:input2 model_detect:output1 -> face_post:in_port1 model_detect:output2 -> face_post:in_port2 model_detect:output3 -> face_post:in_port3 ... 模型配置说明 模型文件类型和模型推理引擎一一对应，如下表： 推理引擎 模型格式 tensorflow xxx.pb tensorrt xxx.engine(序列化模型) torch xxx.pt acl xxx.om mindspore xxx.mindir 模型引擎为TensorRT时，可以对应三种模型格式，toml文件的修改如下： 模型类型为uff, 配置文件当中增加 ... [config] uff_input = \"input.1.28.28\" # 输入名称以及输入的shape大小，以.隔开 ... 模型类型为caffe, 配置文件当中修改增加 ... entry = \"xxx.prototxt\" model_file = \"xxx.caffemodel\" ... 模型类型为onnx, 配置文件当中修改 entry = \"xxx.onnx\" 模型类型为TensorRT自己生成的序列化模型, 不论任何后缀直接配置到entry即可 base域下面的plugin选项 plugin即为文件路径下面的so，该so为为自定义ModelBox的tensorflow推理的预处理以及后处理函数，需要自定义实现以下接口(为可选项) // tensorflow class InferencePlugin { ... /** * @brief init plugin * @param config modelbox config, can get key value from the graph toml * @return init result, modelbox status */ virtual modelbox::Status PluginInit(std::shared_ptr config) = 0; /** * @brief before inferencere, preprocess data * @param ctx modelbox datacontext, can get input data from this * @param input_tf_tensor_list tensorflow TF_Tensor*, after preprocess data from ctx, * build input TF_Tensor to inference * @return preprocess result, modelbox status */ virtual modelbox::Status PreProcess(std::shared_ptr ctx, std::vector &input_tf_tensor_list) = 0; /** * @brief after inferencere, postprocess data * @param ctx modelbox datacontext, can get modelbox output object from this * @param output_tf_tensor_list tensorflow TF_Tensor*, after inference output data store in it, * build output bufferlist from it * @return postprocess result, modelbox status */ virtual modelbox::Status PostProcess(std::shared_ptr ctx, std::vector &output_tf_tensor_list) = 0; ... }; TensorRT的自定义算子构建的PluginFactory 目前自带YOLO版本的PluginFactory，只需要在toml配置文件当中增加 [config] plugin = \"yolo\" 后续支持自定义算子的TensorRT插件，编译成动态库，把路径配置在这里 torch模型需要保存成成jit模型，参考sample如下： jit_model = torch.jit.script(Module) jit_model.save(\"save_model.pt\") torch模型的输入输出配置可以自定义名称，在此仅仅为位置占位符，但是需要保证输入输出的顺序一致 模型加解密 模型加密分为2个部分：模型加密工具和模型解密插件。 模型加密工具为modelbox-tool key内，使用ModelBox Tool加密模型后，可获取root key和模型密钥，以及加密后的模型。 ModelBox目前默认自带了模型加密功能，但为了确保模型安全，开发者应该至少实现自己的模型解密插件，至少需要隐藏模型的rootkey和passwd，具体参考修改src/drivers/devices/cpu/flowunit/model_decrypt_plugin/model_decrypt_plugin.cc内的Init函数， rootkey_ = config->GetString(\"encryption.rootkey\"); en_pass_ = config->GetString(\"encryption.passwd\"); 注意事项： encryption.rootkey和 encryption.passwd为加密后的模型解密密钥，但模型加密使用的是对称算法，模型仍然存在被破解的可能性，比如反汇编跟踪调试解密代码。 为保证模型不被非法获取，开发者需要对运行的系统环境进行加固，比如设置bootloader锁，设置OS分区签名校验，移除调试跟踪工具，若是容器的，关闭容器的ptrace功能。 功能单元调试运行 推理功能单元无需编译，通常情况下调试阶段可以将此功能单元所在的文件夹路径配置到流程图的扫描路径driver.dir中，再通过ModelBox-Tool 启动流程图运行，流程图启动时会扫描并加载推理功能单元。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/flowunit/config.html":{"url":"use-modelbox/standard-mode/flowunit/config.html","title":"配置功能单元","keywords":"","body":"配置类功能单元 本章节介绍推配置类功能单元的开发流程。在AI应用开发时，存在较多处理过程完全一样，但输入参数存在差异的处理，如YOLO检测的后处理。为了减少开发者开发工作，ModelBox提供了配置类功能单元，完成特定功能。开发者只需填写配置文件即可完成功能单元开发。下面以YOLO功能单元为例介绍配置类功能单元开发流程。 配置类功能单元创建 ModelBox提供了多种方式进行功能单元的创建，以YOLO功能单元为例： 通过UI创建 可参考可视化编排服务的任务编排页面章节操作步骤。 通过命令行创建 ModelBox提供了模板创建工具，可以通过ModelBox Tool工具产生配置类功能单元的模板，以YOLO功能单元为例具体的命令为 modelbox-tool template -flowunit -project-path [project_path] -name [flowunit_name] -lang yolo -virtual-type [type] -input name=in1,device=cpu -output name=out1 创建完成的配置类功能单元目录结构如下： [flowunit-name] |---CMakeList.txt # 用于CPACK 打包 |---[flowunit-name].toml # 功能单元属性配置文件 功能单元属性配置 每种配置类功能单元是完成特定功能，所以他们的配置参数各不相同，具体参数配置可参考预置功能单元中配置类章节。 功能单元调试运行 配置类功能单元无需编译，通常情况下调试阶段可以将此功能单元所在的文件夹路径配置到流程图的扫描路径driver.dir中，再通过ModelBox-Tool 启动流程图运行，流程图启动时会扫描并加载功能单元。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/flow-run.html":{"url":"use-modelbox/standard-mode/flow-run.html","title":"流程图运行","keywords":"","body":"流程图运行 流程图完成后，有如下两种方式可运行图： 通过modelbox-tool运行 modelbox-tool是ModelBox提供的调试工具，可以快速检查结果是否正确，命令如下： modelbox-tool -verbose -log-level INFO flow -run [path_to_graph] verbose：将输出日志打印到屏幕上； -log-level：设置日志级别，DEBUG, INFO, NOTICE, WARN, ERROR, FATAL； flow：快速运行一个流程，快速验证，具体可参考flow功能； 通过modelbox进程运行 使用ModelBox加载流程图，不仅可以运行流程图，还可以加载自定义插件 需要修改$HOME/modelbox-service/conf/modelbox.conf配置文件以下几个部分： 修改流程图路径server.flow_path： [server] ip = \"127.0.0.1\" port = \"1104\" flow_path = \"/usr/local/etc/modelbox/graph/\" # 修改为自定义流程图目录 添加开发的插件路径plugin.files： [plugin] files = [ \"/usr/local/lib/modelbox-plugin.so\", \"/usr/local/lib/modelbox-plugin-editor.so\" # 添加开发插件路径 ] 设置日志配置： [log] level = \"INFO\" # 设置日志级别 num = 32 # 设置日志归档文件最大个数 path = \"/var/log/modelbox/modelbox.log\" # 设置日志路径 执行$HOME/modelbox-service/modelbox restart命令重启ModelBox服务生效。 更多详细modelbox运行参考modelbox运行。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/service-plugin/service-plugin.html":{"url":"use-modelbox/standard-mode/service-plugin/service-plugin.html","title":"插件开发","keywords":"","body":"ModelBox插件 ModelBox插件介绍 什么是ModelBox插件 ModelBox插件是指基于ModelBox框架对外交互的组件，它可以用来作为AI应用和周边系统对接的桥梁。ModelBox框架提供了ModelBox插件管理和扩展开发能力，用户可以定制化开发属于自己业务的插件来对接第三方平台，ModelBox框架可以将其加载并运行。在ModelBox插件内可以完成流程图的加载和运行、任务的创建和启停，统计数据的收集等。同时，ModelBox框架可以支持多个ModelBox插件的加载。 ModelBox插件使用场景 ModelBox插件在视频场景使用较为普遍，典型使用场景为：视频分析任务需要从外部平台或者组件下发到ModelBox框架进行任务分析时，需要通过ModelBox插件来接受外部的请求并转化为ModelBox框架里的分析任务进行业务分析。同时ModelBox插件也可以实现统计信息的收集并发送给外部运维平台，实现与外部系统的对接。 ModelBox框架提供了预置的ModelBox插件ModelBox Plugin，用于流程图的加载和运行, 见ModelBox Plugin章节。在大部分情况下，可以直接使用ModelBox Plugin完成相应的业务功能，当某些场景下，ModelBox Plugin功能无法满足要求时，需要自定义开发ModelBox插件，下面介绍ModelBox插件的具体开发流程。 ModelBox插件接口说明 插件整体模块图如下： ModelBox API按照类型包含： Plugin：插件创建和启停等重载接口，由开发者实现 接口 接口功能 说明 CreatePlugin 创建ModelBox插件对象，并返回给ModelBox框架 ModelBox框架启动时加载参加时调用 Plugin::Init 实现ModelBox插件初始化逻辑，提供系统配置，插件初始化时调用 ModelBox框架启动时，在CreatePlugin成功后插件初始化调用；不能存在阻塞操作 Plugin::Start 实现ModelBox插件启动逻辑，插件启动时调用 插件启动时调用 Plugin::Stop 实现ModelBox插件停止逻辑，插件停止时调用 ModelBox框架进程退出时插件停止时调用 Job： 任务管理组件 任务管理组件提供任务的添加，删除，查询。ModelBox框架任务管理存在以下几种对象概念： Job：算法服务层面的任务，一个Job加载一个流程图。 JobManager：Job的管理，可以创建Job对象。 Task：处理数据源层面的作业，一个Task即对应一次数据分析，可以是一路视频流的分析，也可以是一个视频文件的分析。Task可以实现数据输入到流程图(需要配合Input节点使用)，也可以实 现配置参数传递到功能单元。 TaskManager：Task的管理，可以创建Task对象。 OneShotTask：继承自Task，一次task，专指只有一次数据输入到流程图的场景，比如输入为一路视频流的地址，只会有一次数据传递给流程图，而后需要等待分析结果。所以OneShotTask还提 供了task状态变化的回调注册接口。 Session：会话信息，一个Task对应存在一个Session，Session中的数据可以在不同功能单元共享访问。 具体接口如下表： 接口 接口功能 JobManager::CreateJob 创建Job JobManager::DeleteJob 删除Job JobManager::GetJob 获取某个Job对象 JobManager::GetJobList 获取全部Job对象列表 JobManager::QueryJobStatus 查询某个Job状态 JobManager::GetJobErrorMsg 获取某个异常Job的错误信息 Job::Init 初始化Job对象 Job::Build Job对象资源申请 Job::Run 运行Job对象 Job::Stop 停止Job对象 Job::GetJobStatus 获取某个Job状态 Job::CreateTaskManger 创建TaskManger TaskManager::Start 启动TaskManager TaskManager::Stop 停止TaskManager TaskManager::CreateTask 创建Task TaskManager::DeleteTaskById 删除某个Task TaskManager::GetTaskById 获取某个Task对象 TaskManager::GetTaskCount 获取Task个数 TaskManager::GetAllTasks 获取所有Task对象 TaskManager::SetTaskNumLimit 设置同时并发的Task最大个数，超过设置最大个数时，ModelBox内部会排队处理 Task::Start 启动Task Task::Stop 停止Task Task::GetUUID 获取Task id Task::CreateBufferList 创建输入的Buffer列表 Task::GetLastError 获取Task错误信息 Task::GetTaskStatus 获取Task状态 Task::GetSessionConfig 获取Session配置对象，可以通过设置自定义参数，传递给需要的功能单元读取 OneShotTask::FillData 发送数据指流程图 OneShotTask::RegisterStatusCallback 注册任务状态回调函数,任务结束或异常时会调用 Config： 配置对象 配置对象提供从服务配置文件中获取配置信息 Listener：HTTP/HTTPS监听组件 Listener监听组件可以注册HTTP服务，监听相关的URI Timer： 定时器组件 定时器组件可以用于启动定时任务 ModelBox插件模板创建 插件开发前，请准备好ModelBox开发环境。开发者可以通过modelbox-tool命名进行ModelBox插件模板工程的创建，创建命令如下： modelbox-tool template -service-plugin -name PluginName 开发者可以通过modelbox-tool命名进行ModelBox插件模板工程的创建，创建命令如下： modelbox-tool template -service-plugin -name PluginName ModelBox插件逻辑实现 ModelBox插件启动入口实现 #include \"modelbox/server/plugin.h\" // 插件需要实现的接口 class ModelBoxExamplePlugin : public Plugin { public: ModelBoxExamplePlugin(){}; ~ModelBoxExamplePlugin(){}; // 插件初始化时调用。 bool Init(std::shared_ptr config) override; // 插件开工启动时调用，非阻塞。 bool Start() override; // 插件停止时调用。 bool Stop() override; }; // 插件创建接口 extern \"C\" { std::shared_ptr CreatePlugin() { return std::make_shared(); }; } ModelBox加载ModelBox插件流程如下： ModelBox Server先调用插件中的CreatePlugin函数创建插件对象。 插件需要在此函数中，创建插件对象，返回智能指针。 再调用Plugin::Init()初始化插件，入参为TOML文件配置。 插件可在初始化函数中，获取配置，并调用插件自身的初始化功能。 再调用Plugin::Start()启动插件。 插件在Start函数中启动插件服务，申请相关的资源；此函数不能阻塞。 进程退出时，调用Plugin::Stop()停止插件功能。 插件在Stop函数中停止插件的功能，并释放相关的资源。 调用ModelBox Server，ModelBox Library相关接口。 插件调用ModelBox Server，以及ModelBox Library的API进行业务控制和运行。具体参考相关的API。 Job创建流程 使用场景为流程图不依赖于外部给其输入，直接加载图配置即可运行场景。如图片推理服务，数据流可由流程图中的HTTP功能单元产生数据，再比如流程图中读本地文件作为数据源的场景。 std::shared_ptr job_; std::shared_ptr job_manager_; bool ModelBoxExamplePlugin::Init(std::shared_ptr config) { //创建JobManager job_manager_ = std::make_shared(); //从配置文件获取图配置，config对象为运行时传入的conf配置文件，默认路为/usr/local/etc/modelbox/modelbox.conf auto graph_path = config->GetString(\"server.flow_path\"); //创建Job job_ = job_manager_->CreateJob(\"my_job\", graph_path); auto ret = job_->Init(); return ret; } bool ModelBoxExamplePlugin::Start() { job_->Build(); job_->Run(); return true; } bool ModelBoxExamplePlugin::Stop() { auto ret = job_->Stop(); ret = job_manager_->DeleteJob(\"my_job\"); return ret; } Task创建流程 使用场景为流程图运行依赖与外部输入的场景，如分析的视频流信息需要由外部传入ModelBox插件，再用ModelBox插件创建Task，并把相应配置参数数据传递到流程图。 由于流程图需要接受插件输入，所以需要首先给流程图配置输入节点： [graph] graphconf = '''digraph demo { input1[type=input, device=cpu, deviceid=0] # 设置图的输入端口，端口名为\"input1\" data_source_parser[type=flowunit, flowunit=data_source_parser, device=cpu, deviceid=0, retry_interval_ms = 1000] videodemuxer[type=flowunit, flowunit=video_demuxer, device=cpu, deviceid=0] videodecoder[type=flowunit, flowunit=video_decoder, device=cpu, deviceid=0,pix_fmt=nv12] ... input1 -> data_source_parser:in_data data_source_parser:out_video_url -> videodemuxer:in_video_url videodemuxer:out_video_packet -> videodecoder:in_video_packet videodecoder:out_video_frame -> ... }''' format = \"graphviz\" void ModelBoxExamplePlugin::ModelBoxTaskStatusCallback(modelbox::OneShotTask *task, modelbox::TaskStatus status) { //实现任务结束或者任务异常时处理逻辑 return; } bool ModelBoxExamplePlugin::Start() { job_->Build(); job_->Run(); //创建task manager，携带最大并发数 auto task_manager = job->CreateTaskManger(10); task_manager->start(); //创建task auto task = task_manager->CreateTask(modelbox::TASK_ONESHOT); auto oneshot_task = std::dynamic_pointer_cast(task); //创建buff数据并发送给流程图 auto buff_list = oneshot_task->CreateBufferList(); auto input_cfg = \"{\\\"url\\\"：\\\"xxxx\\\"}\"; //输入需要的配置信息，由流程图中输入节点决定 auto status = buff_list->Build({input_cfg.size()}); if (status != modelbox::STATUS_OK) { return status; } auto buff = buff_list->At(0); auto ret = memcpy_s(buff->MutableData(), buff->GetBytes(), input_cfg.data(), input_cfg.size()); buff->Set(\"source_type\", std::string(\"url\")); //输入需要的配置信息，由流程图中输入节点决定 std::unordered_map> datas; datas.emplace(\"input1\", buff_list);//输入端口节点，对应流程图中的input类型端口名 status = oneshot_task->FillData(datas); if (status != modelbox::STATUS_OK) { return status; } //填充自定义配置 auto config = oneshot_task->GetSessionConfig(); config->SetProperty(\"nodes.{key}\", \"{vaule}\");//设置属性 //注册Task状态变更回调函数 oneshot_task->RegisterStatusCallback( [&](OneShotTask *task, TaskStatus status) { t->ModelBoxTaskStatusCallback(task, status); return; }); status = oneshot_task->Start(); if (status != modelbox::STATUS_OK) { return status; } //等待task执行结束 auto task_status = iva_task->GetTaskStatus(); while (task_status != TaskStatus::STOPPED && task_status != TaskStatus::ABNORMAL && task_status != TaskStatus::FINISHED) { sleep(1); task_status = iva_task->GetTaskStatus(); } //删除任务 task_manager->DeleteTaskById(oneshot_task->GetTaskId()); return true; } ModelBox插件编译运行 ModelBox插件目前只能通过C++开发，插件开发完成后，需要编译为SO文件，并将路径配置加入ModelBox配置文件的plugin.files配置项插件配置列表中，开发态配置文件默认路径为$HOME/modelbox-service/conf/modelbox.conf，详细说明可参加流程图运行章节。 [plugin] files = [ \"/usr/local/lib/modelbox-plugin.so\", #由于不同操作系统目录结构存在差异，此路径也可能为 /usr/local/lib64/modelbox-plugin.so \"/xxx/xxx/example-plugin.so\" ] 配置说明： modelbox-plugin.so为系统预置插件，可根据需要添加 插件按从上到下的顺序加载。 若采用tar.gz包安装的服务，modelbox.conf配置文件在对应的服务目录中。 开发者可扩展增加toml的配置项，在ModelBoxExamplePlugin::Init接口的configuration对象中获取即可。 插件加入配置文件后，执行 $HOME/modelbox-service/modelbox restart 重启ModelBox Server生效， 同时ModelBox插件日志将统一收集到ModelBox日志。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/debug/debug.html":{"url":"use-modelbox/standard-mode/debug/debug.html","title":"调试","keywords":"","body":"调试 ModelBox提供了多种调试方法，包含了业务运行，性能，和代码的调试。 各个组件调试方法 各个组件的调试方法参考下表： 组件 语言 调试方法 C++功能单元 C++ GDB，日志，Profiling。 Python功能单元 Python PDB，日志，Profiling。 自定义插件 C++ GDB，日志。 上述表格中，使用GDB、PDB调试的，可以配合IDE完成。 调试指导 调试方法 说明 连接 代码调试 代码级别的调试方法，主要使用现有的调试工具，IDE进行调试。 指导 日志 使用运行日志，业务代码使用log类函数打印相关的日志。 指导 性能优化 提供Profiling工具，对图的执行进行数据打点，并输出甘特图供性能分析调试。 指导 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/debug/code-debug.html":{"url":"use-modelbox/standard-mode/debug/code-debug.html","title":"代码调试","keywords":"","body":"代码调试 代码调试使用对应语言的调试方法即可，C++使用gdb，Python使用pdb。 GDB调试方法 C++调试使用GDB即可，在调试前，需要将对应的软件编译为DEBUG版本。 编译debug版本 使用CMake的编译参数，编译为DEBUG版本。 mkdir build cd build cmake -DCMAKE_BUILD_TYPE=Debug .. make -j32 配置调试流程图 配置构造一个简单的flow流程图，确保被调测组件能被调用。 启动调试 若是调试功能单元，可以使用ModelBox Tool辅助调试。 GDB命令 gdb modelbox-tool set args -verbose -log-level info flow -run [path/to/graph.conf] b [some-func] r 上述命令意思为： 使用gdb启动modelbox-tool 设置运行参数-verbose -log-level info表示显示日志，及设置日志级别 flow -run [path/to/graph.conf]表示运行的调测流程图。 b [some-func]对指定的函数进行断点。 r 运行命令 注意：如果使用镜像开发，gdb提示无权限，则需要使用特权容器，具体参考这里的gdb调试设置 vscode vscode调试，可以先下载GDB插件，再配置调试文件.vscode/launch.json，设置program和args两个配置项如下。 \"program\": \"modelbox-tool\", \"args\": [ \"-verbose\", \"-log-level\", \"info\", \"flow\", \"-run\", \"[path/to/graph.toml]\" ], 设置完成后，使用vscode的F5功能键进行调试 Python调试方法 Python调试时，则需要先设置环境变量MODELBOX_DEBUG_PYTHON=yes后，直接使用IDE调试，其调试方法和标准的Python脚本类似。 环境变量可通过Python脚本，或启动进程前的shell命令设置。 Python中设置启用 import os # 设置环境变量 os.environ['MODELBOX_DEBUG_PYTHON']=\"yes\" # 执行流程图 flow = modelbox.Flow() ... 环境变量中启用 export MODELBOX_DEBUG_PYTHON=yes Python功能单元调试 Python代码编写的功能单元需要从Python启动后，设置上述环境变量才能调试。但ModelBox也提供了专门用于调试Python功能单元的命令modelbox-python-debug。 具体操作方法为： IDE或调试工具设置启动程序为modelbox-python-debug; modelbox-python-debug启动toml文件的流程图。 modelbox-python-debug --flow [path/to/toml] IDE或调试工具打断点，启动调试。 Vscode调试 vscode调试，可以配置调试文件.vscode/launch.json，设置program和args两个配置项如下。 { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Python: modelbox\", \"type\": \"python\", \"request\": \"launch\", \"program\": \"/usr/local/bin/modelbox-python-debug\", \"args\": [ \"--flow\", \"[/path/to/toml]\" ], \"console\": \"integratedTerminal\" } ] } 将[/path/to/toml]替换为实际的toml文件路径，并对需要调试的Python功能单元设置断点。设置完成后，使用vscode的F5功能键进行调试。 注意： 若启用失败，则需要先安装pydevd包。 pip install pydevd ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/debug/log.html":{"url":"use-modelbox/standard-mode/debug/log.html","title":"日志","keywords":"","body":"日志 ModelBox提供了运行日志，对应的组件在运行时会输出相关的日志信息。 日志的基本流程 C++，Python功能单元、ModelBox库、插件调用ModelBox的日志函数后，由ModelBox的Logger将数据发送到Appender，Appender可注册不同的类型。 ModelBox的日志级别分为DEBUG, INFO, NOTICE, WARN, ERROR, FATAL。 ModelBox Server日志 ModelBox Server和ModelBox Tool中内置了File日志组件，在运行时，会将对应的日志记录到相关的文件中。对应的日志路径，配置方法如下： 进程 日志路径 级别设置 ModelBox Server /var/log/modelbox/modelbox.log /usr/local/etc/modelbox/modelbox.conf配置文件中，log字段的level。 ModelBox Tool /var/log/modelbox-tool.log modelbox-tool命令参数。 默认console日志 ModelBox Library在未设置输出appender的情况下，所有打印输出到console，且默认情况下日志输出关闭，若要设置日志级别，可以通过环境变量设置。可设置的变量值为DEBUG, INFO, NOTICE, WARN, ERROR, FATAL。 export MODELBOX_CONSOLE_LOGLEVEL=INFO 日志SDK ModelBox日志提供了日志输出接口，日志appender捕获接口；功能单元，ModelBox库，插件使用日志接口输出日志，业务模块使用appender捕获日志到对应的日志组件。 日志输出信息包括 level: 日志级别 file: 日志文件 lineno: 行号 func: 函数名称 msg: 日志内容 appender可以按需求输出日志。 不同语言的SDK日志调用接口，日志捕获接口如下： C++ C++日志调用 C++调用日志时，需要包含头文件，然后使用类似std::cout的语法输出日志。 #include void LogExample() { MBLOG_DEBUG C++日志捕获 C++提供了日志接口logger，只需要实现logger中的方法，即可将日志重定向。 class Logger { public: // vprint接口 virtual void Vprint(LogLevel level, const char *file, int lineno, const char *func, const char *format, va_list ap); // print接口 virtual void Print(LogLevel level, const char *file, int lineno, const char *func, const char *msg); // 设置日志级别 virtual void SetLogLevel(LogLevel level); // 获取日志级别 virtual LogLevel GetLogLevel() = 0; }; // 注册日志函数 ModelBoxLogger.SetLogger(logger); 流程： 编写自定义日志对象，从Logger派生，实现相关的接口 初始化时，调用ModelBoxLogger.SetLogger(logger)注册日志处理函数。 调用ModelBoxLogger.GetLogger->SetLogLevel(level)设置日志级别。 Python Python日志调用 Python输出日志时，需要包含ModelBox包，使用上类似，print函数。 import modelbox modelbox.debug(\"this is debug\") modelbox.info(\"this is info\") modelbox.notice(\"this is notice\") modelbox.warn(\"this is warning\") modelbox.error(\"this is error\") modelbox.fatal(\"this is fatal\") Python日志捕获 # 导入相关的包 import modelbox import datetime __log = modelbox.Log() # 日志捕获函数 def LogCallback(level, file, lineno, func, msg): # 输出日志信息 print(\"[{time}][{level}][{file}:{lineno}] {msg}\".format( time=datetime.datetime.now(), level=level, file=file, lineno=lineno, msg=msg )) # 日志注册函数 def RegLog(): # 注册日志函数 __log.reg(LogCallback) # 设置日志级别为INFO __log.set_log_level(modelbox.Log.Level.INFO) # 注册自定义日志 RegLog() 流程 编写自定义函数，函数原型为logfunc(level, file, lineno, func, msg)。 初始化日志对象modelbox.Log()。 将logfunc调用modelbox.Log::reg注册为日志处理函数 调用modelbox.Log::set_log_level(modelbox.Log.Level)设置日志级别。 Java Java日志调用 Java输出日志时，需要包含ModelBox包，使用上类似，print函数。 import com.modelbox.Log Log.debug(\"this is debug\") Log.info(\"this is info\") Log.notice(\"this is notice\") Log.warn(\"this is warning\") Log.error(\"this is error\") Log.fatal(\"this is fatal\") Java日志捕获 // 导入相关的包 import com.modelbox.Log; class CustomLog extends Log { public void print(LogLevel level, String file, int lineno, String func, String msg) { String timeStamp = new SimpleDateFormat(\"yyyy-MM-dd HH.mm.ss.SSS\").format(new Date()); System.out.printf(\"[%s][%s][%17s:%-4d] %s\\n\", timeStamp, level, file, lineno, msg); lastMsg = msg; } public String lastMsg; } public void initLog() { // 注册自定义log TestLog log = new CustomLog(); Log.regLog(log); } public void deinitLog() { Log.unRegLog(); } 流程 编写自定义日志对象，从Log派生，实现print接口 初始化时，调用Log.regLog(log)注册日志处理函数。 调用Log.getLogger->setLogLevel(level)设置日志级别。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/debug/profiling.html":{"url":"use-modelbox/standard-mode/debug/profiling.html","title":"性能优化","keywords":"","body":"性能统计 ModelBox提供了统计功能单元，以及运行任务的时间统计功能，开发者或维护人员可以开启性能统计功能，对功能单元或运行状态进行调试和维护。 性能统计操作流程 配置流程图。 配置文件中指定启动Profiling。 运行流程图。 获取统计信息。 chrome浏览器打开chrome://tracing/。 优化代码，重新分析。 性能满足要求后，结束。 启用性能统计 启动ModelBox的性能统计功能，只需要在Flow的toml配置文件中增加如下配置，即可启用。 [profile] performance=true # 启用performance trace=true # 启用traceing dir=\"/tmp/modelbox/perf\" # 设置跟踪文件路径 通过配置profile和trace开关启用性能统计，dir配置存储跟踪文件路径；配置启动后，启动运行流程图，profile会每隔60s记录一次统计信息，trace会在任务执行过程中和结束时，输出统计信息。 显示性能统计 运行流程图后，会周期生成timeline性能相关的json文件，通过将json文件加载到chrome trace viewer中即可查看timeline信息。 打开chrome浏览器。 浏览器中输入chrome://tracing/。 点击界面中的Load按钮，加载trace的json文件。 加载成功后，将看到类似下面的timeline视图。视图提供了选择统计、平移、缩放、时间间隔等基本功能可用于分析性能瓶颈 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/deployment/deployment.html":{"url":"use-modelbox/standard-mode/deployment/deployment.html","title":"部署","keywords":"","body":"部署 打包安装 ModelBox运行 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/deployment/pack.html":{"url":"use-modelbox/standard-mode/deployment/pack.html","title":"打包安装","keywords":"","body":"打包安装 ModelBox工程采用CMake内置的CPack工具打包，将功能单元、流程图、插件（可选）等打包成rpm/deb/tar包，可一键安装，方便生产环境的部署和使用。 打包 打包内容包括下面几个部分： 内容 是否必选 流程图 是 功能单元 是 插件 否 打包步骤： 在项目工程中新建build目录； 在build目录中通过cmake ..、make package打包成rpm/deb/tar包。 下面介绍不同组件的打包内容： 流程图 将开发好的流程图配置文件打包。 功能单元 将业务开发好的功能单元打包，不同类型功能单元需要打包的内容不同。 C++功能单元打包的是编译好的动态链接库； Python功能单元打包的是配置文件与Python源码； 推理功能单元打包的是配置文件与模型文件； 插件 若业务开发了自定义插件，则可将插件编译成动态链接库进行打包。 ModelBox配置 启动ModelBox需要ModelBox配置文件，包含主服务配置、服务启动参数、插件参数等配置； 一个典型modelbox.conf配置文件如下图所示，一般只需修改server.flow_path路径即可： [server] ip = \"127.0.0.1\" port = \"1104\" flow_path = \"/usr/local/etc/modelbox/graph/\" # 流程图目录 # [acl] # allow = [ # \"127.0.0.1/8\" # ] [control] enable = false listen = \"/var/run/modelbox/modelbox.sock\" [plugin] files = [ \"/usr/local/lib/modelbox-plugin.so\", \"/usr/local/lib/modelbox-plugin-editor.so\" ] [editor] enable = false root = \"/usr/local/share/modelbox/www\" solution_graphs = \"/usr/local/share/modelbox/solution/graphs\" [log] level = \"INFO\" num = 32 path = \"/var/log/modelbox/modelbox.log\" [include] files = [ \"/usr/local/etc/modelbox/conf.d/*.conf\", ] 参数分为两部分：主服务配置项、内置插件配置项，下面详细介绍各配置。 主服务配置项 主服务配置主要配置插件列表，日志级别信息，具体配置项如下： 配置项 配置功能 plugin.files ModelBox Server插件列表，顺序加载。 log.level ModelBox服务日志级别，默认为INFO，支持DEBUG, INFO, NOTICE, WARN, ERROR, FATAL, OFF，如果指定OFF，将关闭日志打印。 log.num ModelBox服务日志归档文件个数最大值，默认为32，当归档日志超过该阈值时，最旧归档日志文件将删除。 log.path ModelBox服务日志文件路径，默认为/var/log/modelbox/modelbox.log。如果修改该配置项，需要保证日志目录存在且具有可读写权限。 include.files ModelBox服务配置的子配置路径，当子配置存在字段和主配置相同时，取子配置的值。 control.enable ModelBox服务调试开关，当开启时可调试。 control.listen ModelBox服务调试sock路径。 内置插件服务配置 除上述配置外，其他配置均为插件配置。ModelBox服务支持灵活的ModelBox服务插件加载，ModelBox启动后，会按照plugin.files配置的插件，顺序加载插件，各插件的配置参考各自插件配置参数。当前ModelBox的可视化编排及流程图的restful api及通过服务插件实现, 每个插件有各自配置字段： modelbox-plugin插件的配置，可参考服务安装配置。 modelbox-plugin-editor插件的配置，可参考可视化编排服务。 安装 安装步骤： 选择对应的运行镜像; 在运行镜像中rpm/deb可通过对应命令直接安装，tar包在根目录解压即可。默认安装目录为/opt/modelbox/application/$project_name/目录，这个目录下包括流程图（graph）、功能单元（flowunit）、插件。 ModelBox配置文件建议安装在/usr/local/etc/modelbox/modelbox.conf路径下。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/standard-mode/deployment/server.html":{"url":"use-modelbox/standard-mode/deployment/server.html","title":"ModelBox运行","keywords":"","body":"ModelBox运行 通常情况下，ModelBox可以看作是一个应用服务。当需要运行ModelBox时，需要启动ModelBox Server服务。ModelBox Server是最基本也是最重要的服务，ModelBox Server提供基础配置流程图的加载、可视化编排UI服务、流程图Restful API等能力。开发者只需将flow流程图配置文件放到指定的目录下，即可实现flow作为服务的功能。 启动服务 有两种方式启动ModelBox服务： 通过modelbox命令启动 此方法可在运行镜像启动时添加如下启动脚本/命令，来执行如下命令启动ModelBox服务： modelbox -c [path_to_modelbox_conf] -fV 通过modelbox命令可启动ModelBox服务进程，常用参数说明如下： -c可指定modelbox.conf配置文件，具体参数配置可参考服务配置章节； -f表示modelbox进程在前台执行； -V表示将日志打印到屏幕； 更多命令可通过modelbox -h查询。 通过systemd启动 此方法在运行镜像启动时会默认通过systemd启动ModelBox服务，默认读取的ModelBox配置路径为/usr/local/etc/modelbox/modelbox.conf，开发者可替换/修改该的配置文件即可。 ModelBox Server服务使用标准的systemd unit管理，启动管理服务，使用systemd命令管理。 当运行环境支持Systemd时，可通过如下命令对ModelBox服务进行操作： sudo systemctl status modelbox.service：查看ModelBox服务的状态。 sudo systemctl stop modelbox.service：停止ModelBox服务。 sudo systemctl start modelbox.service：启动ModelBox服务。 sudo systemctl restart modelbox.service：重启ModelBox服务。 如无systemd管理机制时，可以使用SysvInit命令管理ModelBox服务，命令如下： sudo /etc/init.d/modelbox [start|status|stop] 此方式相比systemd，缺失了进程的监控，所以建议优先使用systemd启动ModelBox服务。 如需要监控机制，可以使用ModelBox Manager来管理，可以从ModelBox Manager来启动ModelBox服务，命令如下 sudo /etc/init.d/modelbox-manager [start|status|stop] ModelBox服务启动参数配置 ModelBox Server服务启动参数配置项目如下： 配置项 配置功能 MODELBOX_OPTS ModelBox服务启动时会加载该变量的内容作为启动参数。如果开发者需要重新指定其他的ModelBox服务运行配置时，可修改该变量的值实现。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/sdk-mode/sdk-mode.html":{"url":"use-modelbox/sdk-mode/sdk-mode.html","title":"SDK模式","keywords":"","body":"SDK API集成 ModelBox提供了SDK API的方式进行AI应用的构建，在SDK API的模式下，ModelBox将流程图的管理、数据输入输出的能力以API的方式开放出来，开发者可以轻松的将流程图集成到自己的应用中，并通过数据输入输出接口使用流程图。目前已提供接口的语言有：C++、Python、Java。 通过SDK API集成方式开发的通常步骤如下： 安装对应语言的SDK包。 通过API接口构建流程图实例。 在需要调用流程图的位置，使用流程图的数据输入接口，将数据传入流程图中处理。 在读取结果的位置，使用流程图的数据输出接口，获取流程图的输出数据。 各语言的使用方式相近，具体接口介绍参照如下链接： C++ SDK API接口 Python SDK API接口 Java SDK API接口 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/sdk-mode/c++.html":{"url":"use-modelbox/sdk-mode/c++.html","title":"C++","keywords":"","body":"C++开发方式 开发前请先准备好ModelBox开发环境，详见环境准备章节。 C++ SDK API接口说明 ModelBox提供了流程图的创建、运行、关闭等基础接口。下面是C++中使用的API列表： API接口 参数说明 函数说明 Flow::Init configfile: 指定config文件的路径format： 指定图文件的格式，可选项为 FORMAT_AUTO,FORMAT_TOML，FORMAT_JSON 初始化ModelBox服务，主要包含功能如下：1. 读取driver参数，获取driver的扫描路径2. 扫描指定路径下的driver文件，并创建driver实例3. 加载流程图并转换为ModelBox可识别的模型4. 初始化设备信息，性能跟踪和数据统计单元 Flow::Init name: 指定的图的名称graph: 存储图的字符串format：指定图的格式 与上面Init的区别是，上面通过读取文件的方式，而此函数通过读取字符串的方式，其他功能相同 Flow::Init is: 图的输入流istreamfname: 输入的图名称 功能与上面Init相同， 区别在于输入的是流保存的图信息 Flow::Init config: Configuration指针，存储图信息 功能同上 Flow::Build / 用于构建图，将图模型转为可以运行的Node节点并且建立好数据通道 Flow::Run / 图的运行： 同步方式，图运行完成后返回 Flow::RunAsync / 图的运行： 异步运行， 调用后直接返回， 通过调用Wait()函数判断运行是否结束 Flow::Wait millisecond: 超时时间， 以毫秒为单位ret_val: 图运行的结果 等待图运行结束，当图的运行时间超过millisecond表示的时间时，则强制停止图的运行，并返回TIMEOUT Flow::Stop() / 强制停止运行中的图 Flow::CreateExternalDataMap / 当图中的第一个节点为input节点时， 使用此函数可以创建一个输入的ExternalDataMap， 开发者可以通过向ExternalDataMap数据中赋值并传递数据给Input节点。具体使用方法可参考章节 C++开发调用流程图时，需要先安装C++的运行包，然后再编写C++函数，调用Flow执行API执行流程图。Flow流程图接口调用过程如下图所示： 安装C++ SDK包 开发流程图，配置基础部分和图部分。 调用Flow::init接口，输入流程图文件。 调用Flow::build初始化流程图。 调用Flow::run_async，异步执行流程图。 数据输入，数据处理，结果获取。 调用Flow::Stop释放图资源。 流程图配置 SDK模式的流程图的开发和标准模式基本一样，具体开发介绍见流程图开发章节。SDK模型区别可以通过设置input和output端口作为外部数据的输入和输出。具体配置如下： [driver] dir=\"\" [graph] graphconf = '''digraph demo { input1[type=input] # 定义input类型端口，端口名为input1，用于外部输入数据 resize[type=flowunit, flowunit=resize, device=cuda] model_detect[type=flowunit, flowunit=model_detect, device=cuda] yolobox_post[type=flowunit, flowunit=yolobox_post, device=cpu] output1[type=output] # 定义output类型端口，端口名为output1，用于外部获取输出结果 input1 -> resize:in_image resize:out_image -> model_detect:in model_detect:output -> yolobox_post:in yolobox_post:out -> output1 }''' format = \"graphviz\" 如上图，input1和output1端口作为图的输入和输出，如果需要设置多个外部输入输出端口，可按照图配置规则配置多个。 流程图运行 导入ModelBox包 编写时，需要引入头文件，并在编译时链接ModelBox库。 #include 图创建初始化和启动 modelbox::Flow CreateFlow(const std::string &file) { // 创建Flow执行对象 auto flow = std::make_shared(); // 输入流程图配置文件 MBLOG_INFO Init(file); if (!ret) { MBLOG_ERROR Build(); if (!ret) { MBLOG_ERROR RunAsync(); return flow; } 外部数据交互 业务数据往往需要输入给流程图进行处理，同时处理完成后需要获取结果。一次数据的发送和结果过程如下： modelbox::Status SendExternalData(std::shared_ptr ext_data, void *data, int len) { // 申请外部数据对象 auto output_buf = ext_data->CreateBufferList(); // 申请内存，并设置内容 output_buf->Build({len}); auto buff = (int*)output_buf->MutableData(); memcpy(buff, data, len); // 将数据发送到input1端口 auto status = ext_data->Send(\"input1\", output_buf); if (!status) { return {status, \"send data to input failed.\"}; } // 关闭输入 status = ext_data->Close(); if (!status) { return {status, \"external data close failed.\"}; } return modelbox::STATUS_OK; } modelbox::Status RecvExternalData(std::shared_ptr ext_data) { OutputBufferList map_buffer_list; // 接收数据 while (true) { auto status = ext_data->Recv(map_buffer_list); if (status != STATUS_SUCCESS) { if (status == STATUS_EOF) { // 数据处理结束 break; } // 处理出错，关闭输出。 auto error = ext_data->GetLastError(); ext_data->Shutdown(); MBLOG_ERROR GetDesc(); break; } // 处理结果数据 auto buffer_list = map_buffer_list[\"output1\"]; //开发者自定义结果处理逻辑 ProcessData(buffer_list); } return modelbox::STATUS_OK; } // 数据发送获取 modelbox::Status Process(std::shared_ptr flow, void *data, int len) { ... // 创建外部输入句柄 auto ext_data = flow->CreateExternalDataMap(); // 发送数据到流程图 SendExternalData(ext_data, data, len); // 获取输出结果并处理 RecvExternalData(ext_data); ... } 图的资源释放 void FlowStop(std::shared_ptr flow) { // 结束执行 flow->Stop(); } 开发者可以根据自身业务，选择在合适的地方调用图的启动停止和数据发送。如果用户业务是多线程时，可以将flow对象可作为多线程共享对象，每个线程都往同一流程图发生数据，这样可以充分利用ModelBox的bacth并发能力。 C++日志 默认情况，ModelBox的SDK输出日志到console，业务需要注册相关的日志处理函数，注册方法可参考日志章节。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/sdk-mode/python.html":{"url":"use-modelbox/sdk-mode/python.html","title":"Python","keywords":"","body":"Python开发方式 开发前请先准备好ModelBox开发环境，详见环境准备章节。 Python SDK API接口说明 类 方法 参数 功能 modelbox.FlowGraphDesc set_queue_size queue_size: 流程图中节点之间的数据队列大小 设置流程图中节点之间的数据队列大小 set_batch_size batch_size: 流程图中节点处理的批数据大小 设置流程图中节点处理的批数据大小 set_drivers_dir drivers_dir_list: 流程图中节点的额外扫描目录 设置流程图中节点的额外扫描目录 set_skip_default_drivers is_skip: 是否跳过ModelBox默认的扫描路径 设置是否跳过ModelBox默认的扫描路径 set_profile_dir profile_dir: 性能分析文件存储的目录 设置性能分析文件存储的目录 set_profile_trace_enable profile_trace_enable: 追踪执行信息的开关 设置是否启动追踪执行信息 add_input input_name: 图输入端口名称 为图添加输入端口 add_output output_name: 图输出端口名称source_node_port: 作为图输出端口的节点输出端口 为图添加输出端口 add_output output_name: 图输出端口名称source_node: 作为图输出端口的单端口节点 为图添加输出端口 add_node flowunit_name: 使用的功能单元名称device: 节点依赖的设备config: 节点的配置列表source_node_ports: 其他节点输出端口与本节点输入端口的连接关系 添加一个功能单元作为流程图的节点 add_node flowunit_name: 使用的功能单元名称device: 节点依赖的设备config: 节点的配置列表source_node: 与当前节点连接的单输出节点 添加一个功能单元作为流程图的节点 add_node flowunit_name: 使用的功能单元名称device: 节点依赖的设备source_node_ports: 其他节点输出端口与本节点输入端口的连接关系 添加一个功能单元作为流程图的节点 add_node flowunit_name: 使用的功能单元名称device: 节点依赖的设备source_node: 与当前节点连接的单输出节点 添加一个功能单元作为流程图的节点 add_node flowunit_name: 使用的功能单元名称device: 节点依赖的设备config: 节点的配置列表 添加一个功能单元作为流程图的节点 modelbox.Flow init configfile: 指定config文件的路径format： 指定图文件的格式，可选项为 FORMAT_AUTO,FORMAT_TOML，FORMAT_JSON 初始化ModelBox服务，主要包含功能如下：1. 读取driver参数，获取driver的扫描路径2. 扫描指定路径下的driver文件，并创建driver实例3. 加载流程图并转换为ModelBox可识别的模型4. 初始化设备信息，性能跟踪和数据统计单元 init name: 指定的图的名称graph: 存储图的字符串format：指定图的格式 与上面init的区别是，上面通过读取文件的方式，而此函数通过读取字符串的方式，其他功能相同 init config: Configuration指针，存储图信息 功能同上 init flow_graph_desc: 图描述 功能同上 start_run 无 启动流程图 stop 无 停止流程图 create_stream_io 无 在流程图中创建一个数据流的输入输出句柄 modelbox.FlowStreamIO create_buffer 无 创建空buffer用于存储数据 create_buffer data: Python Buffer Protocol类型的数据 根据Python Buffer Protocol类型的data创建一个buffer, 如numpy的ndarray create_buffer data: string类型的数据 根据string类型的data创建一个buffer send input_name: 图的输入端口名buffer: ModelBox的Buffer数据 发送数据到图的输入端口 send input_name: 图的输入端口名data: Python Buffer Protocol类型的数据 发送数据到图的输入端口 send input_name: 图的输入端口名data: string类型的数据 发送数据到图的输入端口 recv output_name: 图的输出端口名buffer: 输出数据timeout: 等待超时 接受图的输出数据 recv output_name: 图的输出端口名timeout: 等待超时 接受图的输出数据 recv output_name: 图的输出端口名 接受图的输出数据 close_input 无 结束输入的数据流，将会告知ModelBox输入数据是否已经完毕 modelbox.Model __init__ path: 模型路径name: 模型配置中描述的名称max_batch_size: 一次推理的最大batchdevice: 设备类型，\"cpu\", \"gpu\", \"ascend\"可选device_id: 设备ID 构建ModelBox单模型推理对象 start 无 启动ModelBox单模型推理对象 stop 无 停止ModelBox单模型推理对象 infer data_list: 每个端口的输入组成的列表，与定义的端口顺序一致 单batch放入数据，底层执行推理时会自动合并batch infer_batch data_list: 每个端口的多输入组成的列表，与定义的端口顺序一致 便于将一批数据送入，底层会按照模型的batch配置决定执行时的真实batch Python开发AI应用的大致流程如下： 安装Python SDK包。 开发流程图之外的应用部分。 开发流程图的流程，以及流程中使用的功能单元。 使用Flow的接口创建流程图对象。 使用流程图对象写入数据。 在使用数据的地方读取数据。 不再使用流程图时，可调用stop释放图资源。 使用场景 通过流程图配置初始化流程图对象 SDK模式的流程图的开发和标准模式基本一样，具体开发介绍见流程图开发章节。SDK模型区别可以通过设置input和output端口作为外部数据的输入和输出。具体配置如下： [driver] dir=\"\" [graph] graphconf = '''digraph demo { input1[type=input] # 定义input类型端口，端口名为input1，用于外部输入数据 resize[type=flowunit, flowunit=resize, device=cuda] model_detect[type=flowunit, flowunit=model_detect, device=cuda] yolobox_post[type=flowunit, flowunit=yolobox_post, device=cpu] output1[type=output] # 定义output类型端口，端口名为output1，用于外部获取输出结果 input1 -> resize:in_image resize:out_image -> model_detect:in model_detect:output -> yolobox_post:in yolobox_post:out -> output1 }''' format = \"graphviz\" 如上图，input1和output1端口作为图的输入和输出，如果需要设置多个外部输入输出端口，可按照图配置规则配置多个。 配置好流程图之后，使用API初始化流程图对象 import modelbox flow = modelbox.Flow() flow.init(flow_file_path) 通过流程图描述初始化流程图对象 通过API直接描述流程图 import modelbox # set graph config graph_desc = modelbox.FlowGraphDesc() graph_desc.set_queue_size(32) graph_desc.set_batch_size(16) graph_desc.set_skip_default_drivers(False) graph_desc.set_drivers_dir([\"/xxx/xxx/\"]) graph_desc.set_profile_dir(\"/tmp/app/\") graph_desc.set_profile_trace_enable(True) input = graph_desc.add_input(\"input1\") resize = graph_desc.add_node(\"resize\", \"cuda\", [\"image_width=128\", \"image_height=128\"], input) model_detect = graph_desc.add_node(\"model_detect\", \"cuda\", resize) yolobox_post = graph_desc.add_node(\"yolobox_post\", \"cpu\", model_detect) graph_desc.add_output(\"output1\", yolobox_post) # start flow flow = modelbox.Flow() ret = flow.init(graph_desc) if ret.code() != modelbox.Status.StatusCode.STATUS_SUCCESS: print(\"init flow failed, err {}\".format(ret.errormsg())) return ret = flow.start_run() if ret.code() != modelbox.Status.StatusCode.STATUS_SUCCESS: print(\"start flow failed, err {}\".format(ret.errormsg())) return 流程图数据输入输出 通过上述两个方式构建好flow后，就可以使用flow进行数据处理了。 stream_io = flow.create_stream_io() # write img img = cv.imread(\"path.jpg\") ## method 1 buffer = stream_io.create_buffer(img) buffer.set(\"meta\", \"meta\") stream_io.send(\"input1\", buffer) ## method 2 stream_io.send(img) # write numpy tensor = numpy.ones((10, 10, 3)) ## method 1 buffer = stream_io.create_buffer(tensor) buffer.set(\"meta\", \"meta\") stream_io.send(\"input1\", buffer) ## method 2 stream_io.send(tensor) # write str url = \"rtsp://x.x.x.x/x.sdp\" ## method 1 buffer = stream_io.create_buffer(url) buffer.set(\"meta\", \"meta\") stream_io.send(\"input1\", buffer) ## method 2 stream_io.send(url) # write list floats = numpy.array([1.0, 2.0, 3.0, 4.0]) ## method 1 buffer = stream_io.create_buffer(floats) buffer.set(\"meta\", \"meta\") stream_io.send(\"input1\", buffer) ## method 2 stream_io.send(\"input1\", floats) # recv buffer result = stream_io.recv(\"output1\") # buffer to numpy data = numpy.array(result) # buffer to str data2 = str(result) 使用快捷的单模型推理接口 对于想快速使用ModelBox对模型进行推理场景，可以使用如下的方式 import modelbox # load model model = modelbox.Model(path, name, max_batch_size, device, device_id) model.start() # single batch inference data1_np = numpy.ones((3, 3, 3)) data2_np = numpy.ones((10, 10, 3)) input_list = [data1_np, data2_np] output_list = model.infer(input_list) output_buffer = output_list[output_index] output_np = numpy.array(output_buffer) # multi batch inference data1_batch_np = [numpy.ones((3, 3, 3)), numpy.ones((3, 3, 3))] data2_batch_np = [numpy.ones((10, 10, 3)), numpy.ones((10, 10, 3))] input_batch_list = [data1_batch_np, data2_batch_np] output_batch_list = model.infer_batch(input_batch_list) output_buffer = output_batch_list[output_index][batch_index] output_np = numpy.array(output_buffer) 日志接口 应用程序可以设置modelbox的日志级别，并且可以使用modelbox的日志系统 # set log level modelbox.set_log_level(modelbox.Log.Level.DEBUG) modelbox.set_log_level(modelbox.Log.Level.INFO) modelbox.set_log_level(modelbox.Log.Level.NOTICE) modelbox.set_log_level(modelbox.Log.Level.WARN) modelbox.set_log_level(modelbox.Log.Level.ERROR) modelbox.set_log_level(modelbox.Log.Level.FATAL) modelbox.set_log_level(modelbox.Log.Level.OFF) modelbox.debug(\"\") modelbox.info(\"\") modelbox.notice(\"\") modelbox.warn(\"\") modelbox.error(\"\") modelbox.fatal(\"\") ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"use-modelbox/sdk-mode/java.html":{"url":"use-modelbox/sdk-mode/java.html","title":"Java","keywords":"","body":"Java开发方式 开发前请先准备好ModelBox开发环境，详见环境准备章节。 Java SDK API接口说明 ModelBox提供了流程图的创建、运行、关闭等基础接口。下面是Java中使用的API列表： API接口 参数说明 函数说明 Flow::init configfile: 指定config文件的路径 初始化ModelBox服务，主要包含功能如下：1. 读取driver参数，获取driver的扫描路径2. 扫描指定路径下的driver文件，并创建driver实例3. 加载流程图并转换为ModelBox可识别的模型4. 初始化设备信息，性能跟踪和数据统计单元 Flow::init name: 指定的图的名称graph: 存储图的字符串format：指定图的格式 与上面Init的区别是，上面通过读取文件的方式，而此函数通过读取字符串的方式，其他功能相同 Flow::startRun / 图的运行： 异步运行， 调用后直接返回， 通过调用Wait()函数判断运行是否结束 Flow::waitFor millisecond: 超时时间， 以毫秒为单位ret_val: 图运行的结果 等待图运行结束，当图的运行时间超过millisecond表示的时间时，则强制停止图的运行，并返回TIMEOUT Flow::stop() / 强制停止运行中的图 Flow::createExternalDataMap / 当图中的第一个节点为input节点时， 使用此函数可以创建一个输入的ExternalDataMap， 开发者可以通过向ExternalDataMap数据中赋值并传递数据给Input节点。 Flow::createStreamIO / 功能类似createExternalDataMap， 但更加简单易用。当图中的第一个节点为input节点时， 使用此函数可以创建一个输入的FlowStreamIO， 开发者可以通过向FlowStreamIO数据中赋值并传递数据给Input节点。 Java开发调用流程图时，需要先安装Java的运行包，然后再编写Java函数，调用Flow执行API执行流程图。 安装Java SDK包 开发流程图，配置基础部分和图部分。 调用Flow::init接口，输入流程图文件。 调用Flow::startRun初始化，并执行流程图。 数据输入，数据处理，结果获取。 调用Flow::stop释放图资源。 流程图配置 SDK模式的流程图的开发和标准模式基本一样，具体开发介绍见流程图开发章节。SDK模型区别可以通过设置input和output端口作为外部数据的输入和输出。具体配置如下： [driver] dir=\"\" [graph] graphconf = '''digraph demo { input1[type=input] # 定义input类型端口，端口名为input1，用于外部输入数据 resize[type=flowunit, flowunit=resize, device=cuda] model_detect[type=flowunit, flowunit=model_detect, device=cuda] yolobox_post[type=flowunit, flowunit=yolobox_post, device=cpu] output1[type=output] # 定义output类型端口，端口名为output1，用于外部获取输出结果 input1 -> resize:in_image resize:out_image -> model_detect:in model_detect:output -> yolobox_post:in yolobox_post:out -> output1 }''' format = \"graphviz\" 如上图，input1和output1端口作为图的输入和输出，如果需要设置多个外部输入输出端口，可按照图配置规则配置多个。 流程图运行 maven引入modelboxSDK包 com.modelbox modelbox 1.0.0 导入ModelBox包 编写时，需要引入头文件，并在编译时链接ModelBox库。 import com.modelbox.Buffer; import com.modelbox.Flow; import com.modelbox.FlowStreamIO; import com.modelbox.Log; 图创建初始化和启动 public Flow CreateFlow(String file) throws ModelBoxException { // 创建Flow执行对象 Flow flow = new Flow(); // 输入流程图配置文件 Log.info(\"run flow \" + file); flow.init(file); // 异步执行 flow.startRun(); return flow; } 外部数据交互 发送数据到Modelbox框架处理： public void Process() { try { // 创建Flow执行对象 Flow flow = CreateFlow(\"path/to/graph.toml\") // 初始化输入流对象 FlowStreamIO streamio = flow.CreateStreamIO(); // 发送数据到Modelbox框架 JSONObject json_data = new JSONObject(\"{\\\"msg\\\":\\\"hello world\\\"}\"); Log.info(\"send message: \" + json_data.toString()); streamio.send(\"input\", json_data.toString().getBytes()); // 结束输入 streamio.closeInput(); // 接收处理结果 while (true) { // 获取处理结果。 Buffer outdata = streamio.recv(\"output\", 1000 * 10); if (outdata == null) { // 处理结束，返回 break; } // 处理出错 if (outdata.hasError()) { Log.error(\"recv error: \" + outdata.getErrorCode()); break; } // 获取到结果 String str = new String(outdata.getData()); Log.info(\"Message is: \" + str); } } catch (ModelBoxException e) { // 错误处理 Log.error(e.getMessage()); } 开发者可以根据自身业务，选择在合适的地方调用图的启动停止和数据发送。如果用户业务是多线程时，可以将flow对象可作为多线程共享对象，每个线程都往同一流程图发生数据，这样可以充分利用ModelBox的bacth并发能力。 Java日志 默认情况，ModelBox的SDK输出日志到console，业务需要注册相关的日志处理函数，注册方法可参考日志章节。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"other-features/other-features.html":{"url":"other-features/other-features.html","title":"特性列表","keywords":"","body":"高级特性 AI应用开发中，可能还会使用到如下特性，可按需进行相关内容的阅读。 多设备开发 异常 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"other-features/device/device.html":{"url":"other-features/device/device.html","title":"多设备开发","keywords":"","body":"多设备开发 ModelBox支持应用在多设备上运行，其中包括Huawei Ascend、 Nvidia CUDA的支持。 功能单元与设备关系 应用由功能单元编排而成，因此应用的多设备运行，实际由功能单元在设备上的运行实现。在ModelBox中，定义每个功能单元需要声明其运行依赖的设备类型，且只能选择一种设备类型。例如框架中预置的resize功能单元，分别实现了依赖CPU、CUDA、Ascend的三个版本。这样每个功能单元就有了设备类型的属性。 在功能单元代码执行前，框架会自动设定当前线程的硬件上下文为指定设备的，如果需要额外获得本次执行指定的设备编号，可以通过成员变量获得Device ID，例如预置的torch推理功能单元中使用了Device ID指定了模型加载的设备。 功能单元设备类型的作用 设备类型的声明包含以下几个作用： 功能单元的实现与设备相关，框架可以通过扫描当前运行环境中的硬件是否存在，决定功能单元的加载。 功能单元声明与设备相关后，其输入默认会搬移到当前设备上，构建输出的内存空间时，分配的存储空间也在当前设备上。 应用程序编排时，通过功能单元名和设备类型，即可选择指定的功能单元实现。 功能单元与硬件Stream接口 Nvidia GPU和Huawei Ascend硬件接口均提供了异步接口，在ModelBox中，对硬件上的内存管理均使用到了硬件Stream接口。在ModelBox中，发生数据从CPU拷贝到GPU或者Ascend的动作时，新的设备Buffer将会绑定一个Stream，此拷贝动作记录在Stream中，之后的功能单元进行处理时，也可以将异步动作施加到Stream上，而无需同步硬件的Stream。当数据要离开设备时，框架会在拷贝完毕后，自动进行必要的同步。 因此在编写GPU和Ascend功能单元时，需要注意，如果输入端口存在设备上的内存时，需要集成自设备功能单元接口，简化Stream的操作，如果输入端口均为CPU内存时，则需集成通用的功能单元接口。 具体设备上的开发如下： 设备 说明 链接 Ascend Huawei Ascend 链接 CUDA Nvidia CUDA 链接 多设备配置 多设备的配置是体现在图上的，在进行图编排时，应用需要对每个节点选择功能单元名，功能单元设备，以及设备的编号，这样就可以明确的指定该节点使用的功能单元实现及硬件情况。 除了选择特定设备外，ModelBox也支持了多设备自动选择的能力： 指定功能单元、设备类型，不指定设备号，ModelBox会根据扫描到的指定设备类型的数量，自动使用这些设备上的功能单元处理数据。 指定功能单元，不指定设备类型、设备号，ModelBox首先根据功能单元名称，查找所有同名的不同设备类型的实现，然后查看环境中的设备类型，确定可用的功能单元，再根据每个类型的设备数量，实例化对应数量的功能单元。最后自动使用这些设备上的功能单元进行数据处理。 配置样例: 通过deivce和deviceid指定单个设备 resize[type=flowunit, flowunit=resize, device=cuda, deviceid=0, image_width=480, image_height=320] 通过device指定单个设备 resize[type=flowunit, flowunit=resize, device=\"cuda:0\", image_width=480, image_height=320] 通过device指定同类型多个设备 resize[type=flowunit, flowunit=resize, device=\"cuda:0,1,3\", image_width=480, image_height=320] 通过device指定多个类型的多个设备 resize[type=flowunit, flowunit=resize, device=\"cuda:0,1,3;ascend:2,3,5\", image_width=480, image_height=320] ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"other-features/device/ascend.html":{"url":"other-features/device/ascend.html","title":"Huawei Ascend","keywords":"","body":"Huawei Ascend加速卡 Huawei Ascend ACL支持接口编程，ACL接口相关的介绍，请点击此处 ModelBox为更好的支持Stream并发编程，默认情况下，ModelBox的Ascend ACL接口全部采用Stream模式，开发者需要在编程时，使用Ascend ACL的Stream接口以提升性能。 Ascend ACL功能单元接口 ModelBox框架会自动管理Stream，开发功能单元时，开发者可以通过Process的入参获取到Stream，之后可以用于ACL接口的调用中。 在实现功能单元之前，Ascend ACL相关的功能单元，需要从AscendFlowUnit派生，并实现AscendProcess接口。 #include class ExampleAscendFlowUnit : public modelbox::AscendFlowUnit { public: ExampleAscendFlowUnit() = default; virtual ~ExampleAscendFlowUnit() = default; // 数据处理接口，需要实现AscendProcess，第二个参数为Ascend ACL Stream。 virtual modelbox::Status AscendProcess(std::shared_ptr data_ctx, aclrtStream stream); }; 除AscendProcess以外，其他接口和通用功能单元一致，AscendProcess接口如下： modelbox::Status ExampleAscendFlowUnit::AscendProcess( std::shared_ptr data_ctx, aclrtStream stream) { auto inputs = ctx->Input(\"input\"); auto outputs = ctx->Output(\"output\"); // 申请内存 std::vector data_size_list(1, 2, 3); outputs->Build(data_size_list); // 循环处理每个输入数据，并产生相关的输出结果。 for (size_t i = 0; i Size(); ++i) { // 获取数据元数据信息 auto meta = inputs[i].Get(\"Meta\", \"Default\"); // 获取输入，输出的内存指针。输入为const只读数据，输出为可写入数据。 auto input_data = inputs[i].ConstData(); auto output_data = outputs[i].MutableData(); // 使用Stream处理数据 // aclmdlExecuteAsync(model_id_, input.get(), output.get(), stream); // 设置输出Meta outputs[i].Set(\"Meta\", \"Meta Data\"); } return modelbox::STATUS_OK; } 由于不确定输入数据是否是异步执行的数据，如果AscendProcess里需要调用ACL同步接口，则需要在调用前，先调用aclrtSynchronizeStream(stream)进行数据同步。 modelbox::Status ExampleAscendFlowUnit::AscendProcess( std::shared_ptr data_ctx, aclrtStream stream) { auto inputs = ctx->Input(\"input\"); auto outputs = ctx->Output(\"output\"); // 同步数据 aclrtSynchronizeStream(stream); // 申请内存 std::vector data_size_list; for (auto &input : *inputs) { data_size_list.push_back(input->GetBytes()); } outputs->Build(data_size_list); // 循环处理每个输入数据，并产生相关的输出结果。 for (size_t i = 0; i Size(); ++i) { // 获取输入，输出的内存指针。输入为const只读数据，输出为可写入数据。 auto input_data = inputs[i].ConstData(); auto output_data = outputs[i].MutableData(); // 同步拷贝输入到输出 aclrtMemcpy(output_data, outputs[i]->GetBytes(), input_data, inputs[i]->GetBytes(), aclrtMemcpyKind::ACL_MEMCPY_DEVICE_TO_DEVICE); } return modelbox::STATUS_OK; } 数据处理时，Ascend Stream会自动由ModelBox框架生成，再调用Ascend ACL接口时，直接使用此Stream对象即可。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"other-features/device/cuda.html":{"url":"other-features/device/cuda.html","title":"Nvidia CUDA","keywords":"","body":"Nvidia CUDA加速卡 Nvidia CUDA支持Stream并发编程，什么是Stream可参考此处 ModelBox为更好的支持Stream并发编程，默认情况下，ModelBox的CUDA接口全部采用Stream模式，开发者需要在编程时，使用CUDA的Stream接口以提升性能。 CUDA功能单元接口 ModelBox框架会自动管理Stream，开发功能单元时，开发者可以通过process的入参获取到Stream，之后可以用于CUDA接口的调用中。 在实现功能单元之前，cuda类型的功能单元，需要从CudaFlowUnit派生，并实现CudaProcess接口。 #include \"modelbox/device/cuda/device_cuda.h\" class ExampleCudaFlowUnit : public modelbox::CudaFlowUnit { public: ExampleCudaFlowUnit() = default; virtual ~ExampleCudaFlowUnit() = default; // 数据处理接口，需要实现CudaProcess，第二个参数为CUDA Stream。 virtual modelbox::Status CudaProcess(std::shared_ptr data_ctx,cudaStream_t stream); }; 除CudaProcess以外，其他接口和通用功能单元一致，CudaProcess接口如下： modelbox::Status ExampleCudaFlowUnit::CudaProcess( std::shared_ptr data_ctx, cudaStream_t stream) { auto inputs = ctx->Input(\"input\"); auto outputs = ctx->Output(\"output\"); // 申请内存 std::vector data_size_list(1, 2, 3); outputs->Build(data_size_list); // 循环处理每个输入数据，并产生相关的输出结果。 for (size_t i = 0; i Size(); ++i) { // 获取数据元数据信息 auto meta = inputs[i].Get(\"Meta\", \"Default\"); // 获取输入，输出的内存指针。输入为const只读数据，输出为可写入数据。 auto input_data = inputs[i].ConstData(); auto output_data = outputs[i].MutableData(); // 使用Stream处理数据 // kernel3 >> ( …, … ) ; // 设置输出Meta outputs[i].Set(\"Meta\", \"Meta Data\"); } return modelbox::STATUS_OK; } 由于不确定输入数据是否是异步执行的数据，如果CudaProcess里需要调用ACL同步接口，则需要在调用前，先调用cudaStreamSynchronize(stream)进行数据同步。 modelbox::Status ExampleCudaFlowUnit::CudaProcess( std::shared_ptr data_ctx, cudaStream_t stream) { auto inputs = ctx->Input(\"input\"); auto outputs = ctx->Output(\"output\"); // 同步数据 cudaStreamSynchronize(stream); // 申请内存 std::vector data_size_list; for (auto &input : *inputs) { data_size_list.push_back(input->GetBytes()); } outputs->Build(data_size_list); // 循环处理每个输入数据，并产生相关的输出结果。 for (size_t i = 0; i Size(); ++i) { // 获取输入，输出的内存指针。输入为const只读数据，输出为可写入数据。 auto input_data = inputs[i].ConstData(); auto output_data = outputs[i].MutableData(); // 同步拷贝输入到输出 cudaMemcpy(output_data, input_data, inputs[i]->GetBytes(), cudaMemcpyDeviceToDevice); } return modelbox::STATUS_OK; } 数据处理时，Stream会自动由ModelBox框架生成，再调用CUDA接口时，直接使用此Stream对象即可。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"other-features/exception.html":{"url":"other-features/exception.html","title":"异常","keywords":"","body":"异常处理 应用运行时，可能会遇到如下载文件失败、解码异常等错误，这些错误可能导致数据流程无法进行下去，异常处理提供了接口和处理机制来帮助开发者处理这些错误，让应用继续正确执行。 异常工作机制 应用采用图编排模式时，数据是从前向后传递的，当前面的节点处理数据出现错误时，可能无法忽略，需要后续的节点感知，并处理。与编程语言类似，ModelBox设计了一套错误的抛出、传递、捕获的规则，使得错误可以由专门的节点统一处理。 首先应当明确的是，异常仅在一个session内部处理，多个session相互不影响。下文将按照异常的抛出、传递、捕获来讲解异常的机制和使用。 异常的抛出 当功能单元处理数据错误时，认为数据的错误无法恢复，需要后续的流程进行处理时，要将错误输出。错误输出的方式有如下两种： 对于单个Buffer输出错误时，调用Buffer::SetError接口，设置错误的详细信息，错误即可发出。 modelbox::Status Flowunit::Process(std::shared_ptr data_ctx) { auto outputs = data_ctx->Output(\"out1\"); outputs->Build({1}); auto buffer1 = outputs->At(0); buffer1->SetError(\"Custom.Downloader.NoSuchFile\", \"File xxxx is not exist\"); return STATUS_OK; } 对于多个Buffer同时设置错误时，调用BufferList::SetError接口，设置错误的详细信息，错误即可发出。 modelbox::Status Flowunit::Process(std::shared_ptr data_ctx) { auto outputs = data_ctx->Output(\"out1\"); outputs->Build({1, 1}); outputs->SetError(\"Custom.Crop.OutOfBound\", \"Crop area width xxx is great than image width xxx\"); return STATUS_OK; } 异常的传递 对于带有错误的Buffer，后续的节点默认是会有框架进行透传处理，直到结束，开发者对错误的Buffer无感知。 异常的捕获 当需要对错误的Buffer进行感知时，就需要通过如下步骤才能对错误Buffer进行处理 明确的声明当前功能单元支持异常处理 cpp功能单元在实现中开启 MODELBOX_FLOWUNIT(ExampleFlowUnit, desc) { desc.SetExceptionVisible(true); } Python功能单元在功能单元配置文件中开启 exception_visible = true 编排节点时，在节点配置上设置需要异常处理 get_exception[..., is_exception_visible=true] 获取异常 cpp功能单元中获取 modelbox::Status Flowunit::Process(std::shared_ptr data_ctx) { auto inputs = data_ctx->Input(\"in1\"); for (auto buffer : inputs) { if (buffer->HasError()) { MBLOG_ERROR GetErrorCode() GetErrorMsg(); } } } Python功能单元中获取 def process(self, data_ctx): inputs = data_ctx.input(\"in1\") for input in inputs: if buffer.has_error(): print(\"error is \" + buffer.get_error_code() + \", \" + buffer.get_error_msg()) 异常捕获中的可见性说明 之前数据流中提到，expand和condition会使得数据流的层级标记下降一次，这个层级不仅影响了匹配，也作用于异常的捕获。核心点是，高层级的错误，低层不可见；低层级的错误，高层可见。 上图中，异常可见情况如下： A抛出异常，可在B、K中捕获 若B向C抛出异常，可在C、D、E、I、K中捕获；若B向J抛出异常，可在J、K中捕获 C抛出异常，可在D、E、I、K中捕获 E抛出异常，可在F、G、H、I、K中捕获 F抛出异常，可在G、H、I、K中捕获 G抛出异常，可在H、I、K中捕获 H抛出异常，可在I、K中捕获 I抛出异常，可在K中捕获 J抛出异常，可在K中捕获 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"tools/tools.html":{"url":"tools/tools.html","title":"配套工具","keywords":"","body":"配套工具 目前提供了如下配套工具： 名称 功能 modelbox-tool 开发、维护的常用命令, 可用于开发调试。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"tools/modelbox-tool/modelbox-tool.html":{"url":"tools/modelbox-tool/modelbox-tool.html","title":"ModelBox Tool","keywords":"","body":"ModelBox Tool ModelBox Tool是ModelBox套件集成的一个开发、维护工具，提供了开发、维护的常用命令, 可用于开发调试。 在功能上，ModelBox Tool包含了如下功能 功能 功能说明 help 显示帮助信息 driver 查看Driver列表及其功能 flow 快速运行一个流程，快速验证 key 密码加解密，模型加解密 server 查看Log，Stack，Slab和Statistics信息 develop 创建开发编排环境 ModelBox为标准的命令行工具，可以使用modelbox-tool -h查看详细的帮助说明。 help功能 用于显示ModelBox各命令组帮助信息 查询各命令组帮助信息 modelbox-tool help driver modelbox-tool help flow modelbox-tool help key modelbox-tool help server modelbox-tool help develop modelbox-tool help template driver功能 用于查询ModelBox Driver相关的信息。 此命令组为modelbox-tool driver，格式如下： modelbox-tool driver [-type flowunit] [-path dir1,dir2] [-details [-name name]] [-conf path/to/graph.conf] -path为添加扫描路径，不携带时默认扫描系统目录，携带时，扫描系统目录和新增目录。 -name 为过滤参数，不携带时默认列出所有扫描到的结果，可以输入功能单元名称进行过滤，也可以输入cpu、cuda、ascend进行设备类型过滤。 查询列表 在开发过程中，可能需要查询图中需要的插件的列表，这时ModelBox Tool可以用于查询当前图的情况。 常用命令如下： 查询当前系统中已经安装可用的driver： modelbox-tool driver -info 查询当前系统中已经安装可用的flowunit列表： modelbox-tool driver -info -type flowunit 查询系统路径和指定图文件的flowunit列表 modelbox-tool driver -info -type flowunit -conf [path/to/graph.conf] 查询系统路径和指定路径使用的flowunit modelbox-tool driver -info -type flowunit -path [path/to/flowunits] 查询单个flowunit的详细信息 modelbox-tool driver -info -type flowunit -details -name [FlowunitName] -path [path/to/flowunit] 查询详细信息 如需要查询具体功能单元的功能说明，输入，输出名称和选项设置，可以用ModelBox Tool查询详细参数。 常用命令如下： 查询当前系统中driver的详细信息： modelbox-tool driver -info -details -name [name] 查询当前系统中所有功能单元的详细信息： modelbox-tool driver -info -type flowunit -details -name cuda 查询指定名称的功能单元详细信息 modelbox-tool driver -info -type flowunit -details -name [name] -path [path/to/flowunit] Flow功能 流程图相关的功能，用于测试，验证图是否配置正确。 此命令组为modelbox-tool flow 运行调测流程图 在开发过程中，可能需要临时调试图以及对应的功能单元，这时，可以使用modelbox-tool flow命令组的命令。 执行图 modelbox-tool flow -run [path/to/graph.toml] 工具执行后的运行日志，存储在/var/log/modelbox/modelbox-tool.log中。如果需要修改日志级别，或将日志输出到屏幕上，可参考后续章节的内容。 template功能 创建代码模板，用于开发准备。可以使用modelbox-tool template命令组,格式如下： modelbox-tool template [options] 创建工程模板 modelbox-tool template -project -name [name] 创建C++功能单元模板 modelbox-tool template -flowunit -lang c++ -name [name] -input name=[port_name],device=[cpu/cuda/ascend/...] -output name=[port_name],device=[cpu/cuda/ascend/...] 创建Python功能单元模板 modelbox-tool template -flowunit -lang python -name [name] -input name=[port_name],device=[cpu/cuda/ascend/...] -output name=[port_name],device=[cpu/cuda/ascend/...] 创建推理功能单元模板 modelbox-tool template -flowunit -lang infer -name [name] --virtual-type [tensorflow/tensorrt/torch/acl/mindspore] -model [model_path] -copy-model -input name=[port_name],device=[cpu/cuda/ascend/...] -output name=[port_name],device=[cpu/cuda/ascend/...] 创建YOLO功能单元模板 modelbox-tool template -flowunit -lang yolo -name [name] -virtual-type yolov3_postprocess -input name=[port_name],device=[cpu/cuda/ascend/...] -output name=[port_name],device=[cpu/cuda/ascend/...] 创建服务插件模板 modelbox-tool template -service-plugin -name [name] 通常情况下，先创建工程模板，再在工程对应目录创建功能单元或者服务插件。 Key功能 key功能包括了模型加解密，密码加密等功能。 此命令组为modelbox-tool key 密码加密 某些情况，需要对存储在本地文件，或图中的密码等敏感信息，加密。 键盘输入密码： modelbox-tool key -pass 标准输入输入密码： modelbox-tool key -pass modelbox-tool key -pass 环境变量输入密码加密 MODELBOX_PASSWORD=\"pass\" modelbox-tool key -pass 注意 默认情况下，加密的密码和设备绑定，若需要和设备无关，则需要增加-n参数。 密码安全性上，键盘输入最可靠，其次是标准输入，环境变量形式不推荐。 编程接口，可以使用popen执行命令，然后write密码到管道中。 模型加密 如需要对模型文件进行加密，则可以使用modelbox-tool key -model命令组对模型文件进行加密。 对指定模型文件进行加密 modelbox-tool key -model [path/to/model] 执行后，输入密码，工具加密后，会输出加密文件，以及加密密钥。 注意： 默认情况下，加密的密码和设备绑定，若需要和设备无关，则需要增加-n参数。 模型安全性上，并不能确保模型文件100%不被获取到明文，为保证模型的安全性，应该从系统角度考虑安全性。具体可参考推理功能单元的说明。 Server功能 查看Log，Stack，Slab和Statistics信息。 此命令组为modelbox-tool server server功能需要进行配置 Log 动态设置日志级别 此命令组为modelbox-tool server log, 命令格式如下： modelbox-tool server log --setlevel [level] modelbox-tool server log --getlevel Stack 查看modelbox线程栈信息 此命令组为modelbox-tool server stack Slab 查看内存碎片 此命令组为modelbox-tool server slab, 命令格式如下： modelbox-tool server slab modelbox-tool server slab --device --type [cuda/cpu] --id [id] Statistics 查看统计信息 此命令组为modelbox-tool server stat, 命令格式如下： modelbox-tool server stat --all modelbox-tool server stat --node [name] Develop 创建开发运行环境，用于启动ModelBox编排UI，连接UI进行工程创建。此命令组为modelbox-tool develop, 命令格式如下： modelbox-tool develop -s modelbox-tool develop -q 具体参数说明如下： 参数 功能说明 -s 在$HOME/modelbox目录下创建开发环境，并启动支持编排的服务。 -q 查询对应开发环境的状态和信息。 --home 指定开发环境服务的路径，默认情况为$HOME/modelbox ModelBox Tool主配置 ModelBox Tool可以支持修改日志级别，输出形式，和日志文件路径，在执行命令时，可以通过如下参数修改 modelbox tool main options: -verbose output log to screen. -log-level log level: DEBUG, INFO, NOTICE, WARN, ERROR, FATAL. -log-path log file: default : /var/log/modelbox/modelbox-tool.log 注意，使用时，上述参数需要紧接modelbox-tool命令后，不能放到子命令组阶段，如 modelbox-tool -verbose [-log-level DEBUG] [-log-path filepath] flow -run [/path/to/graph.toml] 具体参数说明如下： 参数 功能说明 -verbose 是否将日志输出到屏幕 -log-level 输出日志级别，可以为debug, info, notice, warn, error, fatal -log-path 输出日志文件，默认为/var/log/modelbox/modelbox-tool.log ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"plugins/plugins.html":{"url":"plugins/plugins.html","title":"预置插件","keywords":"","body":"预置插件 ModelBox提供了如下预置插件： ModelBox Plugin 可视化编排插件 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"plugins/modelbox-plugin.html":{"url":"plugins/modelbox-plugin.html","title":"ModelBox Plugin","keywords":"","body":"ModelBox Plugin插件 ModelBox Plugin插件用于对外提供服务，管理并运行流程图。此插件内置在ModelBox Server中。 默认情况下，可以直接使用此插件执行相关的流程图功能。例如，新建任务后，将使用插件运行流程图，并将结果对外输出。 ModelBox Plugin功能说明： ModelBox Plugin主要提供两个功能。 添加配置文件管理流程图。 调用REST API执行流程图。 ModelBox Plugin插件配置 ModelBox Plugin插件配置文件和ModelBox Server主配置文件相同，即为/usr/local/etc/modelbox/modelbox.conf, ModelBox Plugin插件的配置项目如下： 配置项目 配置说明 server.ip ModelBox Plugin绑定的管理IP地址，默认为127.0.0.1 server.port ModelBox Plugin绑定的管理端口，默认为1104 server.flow_path ModelBox Plugin加载flow配置文件的扫描路径。默认为/usr/local/etc/modelbox/graph acl.allow ModelBox Plugin REST API访问控制列表，仅允许列表中的IP访问，若为空则允许所有IP 为确保ModelBox Plugin插件生效，请确保插件在modelbox.conf配置文件的plugin.files配置项中配置/usr/local/lib/modelbox-plugin.so插件，并在配置完成后，重启ModelBox服务。 添加配置文件管理流程图 ModelBox Plugin支持通过添加流程图配置文件的形式自动执行流程图，默认情况下的流程图配置文件路径为/usr/local/etc/modelbox/graph, 配置文件的存放目录为： /usr/local/etc/modelbox/graph |-some-flow1.toml |-some-flow2.toml . . . 配置文件复制到图存储目录后，可执行ModelBox Server服务重启命令systemctl restart modelbox生效. 注意： ModelBox服务加载该目录下的所有文件作为flow作业，如果加载失败将跳过该flow，文件名将作为flow的作业名 文件名不要包含特殊符号，并且后缀名为.toml。 如果修改该配置项，需要保证指定的目录存在并具有读权限，否则将加载失败。 路径可通过server.flow_path参数修改。 图形化运行流程图 请参考可视化编排服务。 REST API管理执行流程图 ModelBox Server启动后之后，ModelBox Plugin就开始对外提供服务，服务的endpoint为http://server.ip:server.port，其中server.ip和server.port为ModelBox服务运行配置中的配置项，默认为http://127.0.0.1:1104，服务的path为/v1/modelbox/job，业务可通过发送REST请求到插件管理流程图。 REST API ModelBox服务当前提供动态增加flow作业，动态删除flow作业，查询所有flow作业列表，查询flow作业状态。 增加flow作业 REST API URI: http://server.ip:server.port/v1/modelbox/job/ METHOD: PUT REST API BODY { \"job_id\": \"flow2\", \"job_graph\": \"xxxxx\" } job_id： flow名字，用户自定义，建议不要包含特殊字符。 job_graph：toml格式的图信息，graph的配置详见图 例子 命令：curl -X PUT --data @flow-example http://127.0.0.1:1104/v1/modelbox/job flow-example文件内容： { \"job_id\": \"flow-example\", \"job_graph\": { \"graph\": { \"format\":\"graphviz\", \"graphconf\": [ \" digraph demo { \", \" httpserver_sync_receive[type=flowunit, flowunit=httpserver_sync_receive, device=cpu, deviceid=0, endpoint=\\\"http://127.0.0.1:8080/example\\\", max_requests=10, time_out=5000]\", \" httpserver_sync_reply[type=flowunit, flowunit=httpserver_sync_reply, device=cpu, deviceid=0]\", \" httpserver_sync_receive:out_request_info -> httpserver_sync_reply:in_reply_info\", \" }\" ] }, \"driver\": { \"dir\": \"\" } } } 返回值 正常返回HTTP code 201。 异常返回错误json： { \"error_code\": \"[some error code]\", \"error_msg\" : \"[some error message]\" } error_code：错误码，参考错误码 error_msg：错误码对应的消息。 查询flow作业状态 REST API URI: http://server.ip:server.port/v1/modelbox/job/[flow-name] METHOD: GET REST API RESPONSE 正常返回HTTP code 200，如下json： { \"job_status\": \"RUNNING\", \"job_id\": \"[flow-name]\" } job_status： flow的状态代码。 job_id：flow名称。 例子 命令：curl http://127.0.0.1:1104/v1/modelbox/job/flow-example 删除flow作业 REST API URI: http://server.ip:server.port/v1/modelbox/job/[flow-name] METHOD: DELETE 返回值 正常返回HTTP code 204。 异常返回错误json。 例子 命令： curl -X DELETE http://127.0.0.1:1104/v1/modelbox/job/flow-example 查询所有flow作业列表 REST API URI: http://server.ip:server.port/v1/modelbox/job/list/all METHOD: GET 例子 命令: curl http://127.0.0.1:1104/v1/modelbox/job/list/all REST API BODY { \"job_list\": [ { \"job_status\": \"RUNNING\", \"job_id\": \"[flow-name1]\" }, { \"job_status\": \"RUNNING\", \"job_id\": \"[flow-name2]\" } ] } job_list: flow列表。 状态码 状态码 状态码说明 CREATEING 正在创建任务 RUNNING 任务正在执行 SUCCEEDED 任务执行成功 FAILED 任务执行失败 PENDING 等待执行 DELETEING 正在删除任务 UNKNOWN 未知状态 NOTEXIST 任务不存在 错误码 当前支持的错误码： 错误码 错误码说明 MODELBOX_001 server internal error MODELBOX_002 request invalid, no such job MODELBOX_003 request invalid, can not get jobId MODELBOX_004 request invalid, can not get graph MODELBOX_005 request invalid, job already exist MODELBOX_006 request invalid, invalid command ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"plugins/editor.html":{"url":"plugins/editor.html","title":"可视化编排插件","keywords":"","body":"可视化编排插件 ModelBox提供了在线可视化编排的工具——Editor，在开发时，可使用此工具，提升开发效率。 编排插件是什么 编排插件是用来在Editor可视化界面上，编排流程图并自动生成相对应的图代码的快速开发工具。 使用编排插件开发流程 安装ModelBox server服务。 配置ModelBox Server。 配置启用编排插件。 浏览器访问Editor界面。 业务进行编排操作。 下发编排任务。 编排插件集成在ModelBox Server中，默认情况下，编排插件未启用。可以参考下方编排插件配置章节来启用编排插件并加载Editor界面。 编排插件配置 ModelBox Server安装完成后，编排插件会通过插件的形式由ModelBox Server加载，并在网页浏览器上提供在线可视化编排插件。 对应插件路径为\"/usr/local/lib/modelbox-plugin-editor.so\"(#由于不同操作系统目录结构存在差异，此路径也可能为 \"/usr/local/lib64/modelbox-plugin-editor.so\"，下文涉及系统lib库路径的地方均存在系统路径差异)。 编排插件的配置文件路径为$HOME/modelbox-service/conf/modelbox.conf，其配置项目如下： 配置项目 配置说明 editor.enable 是否启用Editor工具 editor.ip Editor工具监听IP，默认为127.0.0.1。不指定的情况下，和server.ip一致 editor.port Editor工具监听端口，默认为1104，不指定情况下，和server.port一致 editor.root Editor前端UI路径，默认为/usr/local/share/modelbox/www editor.demo_root Editor demo路径，默认为/usr/local/share/modelbox/demo 通过如下命令，可开启基于Web的可视化编辑工具——Editor： modelbox-tool develop -s 命令执行后，将在用户$HOME/modelbox-service创建运行目录，并开启http编排服务，可使用对应主机的IP地址，和开启的端口号（默认端口号为1104）访问Editor界面。 如果访问被拒绝，可以尝试检查并修改ACL配置，并重启ModelBox服务生效，详见访问控制列表章节。 Editor配置 若需要定制化编排服务启动参数，可以修改配置文件，具体修改流程如下： 打开$HOME/modelbox-service/conf/modelbox.conf，修改其中的配置项： [server] ip = \"0.0.0.0\" port = \"1104\" flow_path = \"$HOME/modelbox-service/graph\" [plugin] files = [ \"/usr/local/lib/modelbox-plugin.so\", \"/usr/local/lib/modelbox-plugin-editor.so\" ] [control] enable = true listen = \"$HOME/modelbox-service/run/modelbox.sock\" [acl] allow = [ \"127.0.0.1/8\", # ADD CLIENT HOST HERE \"192.168.59.145\" ] [editor] enable = true # ip = \"127.0.0.1\" # port = \"1104\" root = \"/usr/local/share/modelbox/www\" demo_root = \"/usr/local/share/modelbox/demo\" [log] # log level, DEBUG, INFO, NOTICE, WARN, ERROR, FATAL, OFF level = \"INFO\" # log archive number num = 32 # log file path path = \"$HOME/modelbox-service/log/modelbox.log\" 重启ModelBox Server服务使配置生效。 $HOME/modelbox-service/modelbox restart 或者 $HOME/modelbox-service/modelbox-manager restart 访问控制列表 访问控制列表ACL（Access Control List）是由一条或多条规则组成的集合，里面配置了允许访问Editor的IP地址。 可以通过修改配置文件，来修改ACL列表，具体流程如下： 打开$HOME/modelbox/modelbox.conf，修改其中的配置项： 假设打开编排UI的机器的IP地址为10.11.12.13 [acl] allow = [ \"10.11.12.13\", ] 如果没有配置任何访问白名单，则允许所有人皆可访问。 # [acl] # allow = [ # \"10.11.12.13\", # ] 重启ModelBox Server服务使配置生效。 访问编排服务 服务启动成功后，可使用浏览器访问服务，输入对应的网址即可，如：http://[host]:1104/editor/，成功后，将显示如下界面： 在主页中，分别可以链接到示例展示，任务编排，任务管理。右上角可以可查看帮助文档以及API。 示例展示 该页面分为5个功能区域，其对应的功能如下： 区域1，导航页面。 区域2，基本编排操作区域，包含对6号区域的放大，缩小，重置大小，居中显示，垂直/水平显示，运行图。 区域3，示例相关功能，可以选择示例以及打开指引。 区域4，图形化编排界面，使用鼠标可以控制组件链接和移动。Ctrl+鼠标左键可以拖动画布。 区域5，对应文本化编排界面，可使用标准的DOT语法进行文本编辑。 成功加载所选示例，并点击图中节点时，将显示右侧配置面板。可根据自己的需求对各个节点进行配置。 配置完成后，即可点击区域2中的“运行”按钮，将下发编排任务，并自动跳转至任务管理页面查看任务状态。 快捷键说明： 放大缩小：鼠标滚轮，或键盘，-，=按键。 全选：ctrl+a 撤销：ctrl+z 重做：ctrl+u 取消选择：escape 注意事项： 对应网址的端口号以Docker启动脚本中的 EDITOR_MAP_PORT 为准，默认端口号为1104。 任务编排页面 该页面是进行图编排、设置的主要界面。 进入界面后，即可点击项目来新建或者打开一个项目。 随后，可以通过拖拽至编排界面或者双击左侧功能单元列表上所需要的功能单元，将功能单元显示在编排界面上。 如果需要自定义新的功能单元，可以点击功能单元来创建。 当图编排完成之后，可以通过图属性设置相关属性。最后，点击项目下面的保存即可将项目信息保存至后端。 如果需要运行项目，就点击工具栏上的运行按钮即可。 项目 项目下拉菜单在工具栏的左侧的第一个位置，其有四个功能，分别为： 新建项目 依次输入项目名称以及项目路径，并选择相对应的项目模板，点击确认即可创建一个新的项目。项目路径如果不存在，将会自动创建。 打开项目 输入项目路径，点击确认即可打开项目。 保存 将更改的内容保存至后端。 关闭 将会清空保存在浏览器中的项目数据。 功能单元 新建单元 依次选择功能单元类型，名称，处理类型。 端口可通过选择输入\\输出，端口名称，处理类型，点击添加来增加功能单元的端口。 再选择功能类型。功能类型相关的介绍可以参考功能单元开发。 刷新单元 如果在后端对功能单元进行了更改，可用该功能直接加载更新后的功能 任务管理页面 该页面除了可以查看运行中的任务状态，还可以对任务进行调试。 调试功能有：api调试与转base64 api调试 选择相关模板， 修改Request中的Header和Body部分，发送请求之后得到的Reponse将显示在页面上。 也可以不加载模板，直接进行调试。 转base64 选择需要转成base64格式的文件，即可在页面右侧得到base64代码。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"flowunits/flowunits.html":{"url":"flowunits/flowunits.html","title":"预置功能单元","keywords":"","body":"预置功能单元 ModelBox预置了多个常用功能单元，可用于完成AI应用的基本流程，开发者可以直接使用。 功能单元列表 按业务类型分类，ModelBox主要预置功能单元如下表所示。 业务分类 功能单元名称 功能单元类型 功能简介 输入类 video_input NORMAL 获取视频输入地址 输入类 httpserver_async NORMAL 收发HTTP异步请求 输入类 httpserver_sync_receive NORMAL 接受HTTP同步请求 输入类 data_source_parse STREAM 解析数据源，仅对接华为云ModelArts使用 输入类 data_source_generator NORMAL 产生数据源，给data_source_parse提供模拟输入，本地调试用 输出类 httpserver_sync_reply STREAM 回复HTTP同步请求 输出类 output_broker STREAM 将算法处理结果输出到外部 视频类 video_demuxer STREAM 视频解封装 视频类 video_decoder STREAM 视频解码 视频类 video_encoder STREAM 视频编码 图像类 resize NORMAL 图像尺寸调整 图像类 padding NORMAL 图像填充 图像类 crop NORMAL 对图片进行裁剪 图像类 mean NORMAL 图像减均值 图像类 normalize NORMAL 图像归一化 图像类 image_rotate NORMAL 图像旋转 图像类 color_convert NORMAL 对图片进行颜色通道转换 图像类 image_decoder NORMAL 图像解码 图像类 image_preprocess NORMAL 图像尺寸调整 图像类 draw_bbox NORMAL 在图像上画框 通用类 buff_meta_mapping STREAM 做元数据映射 虚拟类 inference NORMAL 模型推理虚拟功能单元模板, 用于创建推理功能单元 虚拟类 yolo_postprocess NORMAL yolo后处理模板,用于创建YOLO模型后处理功能单元 虚拟类 input 不涉及 虚拟输入功能单元，用于接受图外部输入 虚拟类 output 不涉及 虚拟输出功能单元，用于数据输出到图外部 常用数据类型 ModelBox框架定义了一些通用数据类型，用于规定预置功能单元的输入数据和输出数据格式要求，每一种数据类型规定了该类型的Buffer数据应该携带的Buffer Meta字段信息。 Tensor数据类型 含义：基础数据类型。 Buffer Meta字段信息： 参数名称 参数类型 参数含义 shape vector 多维数据的每一维取值 type 可取值：ModelBoxDataType::MODELBOX_UINT8、ModelBoxDataType::MODELBOX_FLOAT Buffer Data数据类型 图片数据类型 含义：描述一张图片的属性，图片数据类型包含Tensor数据类型信息。 Buffer Meta字段信息： 参数名称 参数类型 参数含义 width int32_t 图片宽 height int32_t 图片高 width_stride int32_t 对齐后的图片宽，目前仅用于ascend类型Buffer height_stride int32_t 对齐后的图片高，目前仅用于ascend类型Buffer channel int32_t 图像通道数 pix_fmt string 图像格式，取值范围：\"rgb\"、\"bgr\"、\"nv12\"、\"rgbp\"、\"bgrp\"、\"gray\" layout int32_t 图片布局，取值范围：hwc、hcw shape vector 多维数据的每一维取值 type ModelBoxDataType::MODELBOX_UINT8 Buffer Data数据类型 视频帧数据类型 含义：描述视频解码后的每帧图片的属性，包含视频信息和图片数据类型信息。 Buffer Meta字段信息： 参数名称 参数类型 参数含义 index int64_t 帧编号 rate_num int32_t 帧率分子，帧率为rate_num/rate_den rate_den int32_t 帧率分母, 帧率为rate_num/rate_den rotate_angle int32_t 画面旋转角度 duration int64_t 视频时长 url int32_t 视频源路径 timestamp int64_t 时间戳 eos int32_t 结束标识 width int32_t 图片宽 height int32_t 图片高 width_stride int32_t 对齐后的图片宽，目前仅用于ascend类型Buffer height_stride int32_t 对齐后的图片高，目前仅用于ascend类型Buffer channel int32_t 图像通道数 pix_fmt string 图像格式，取值范围：\"rgb\"、\"bgr\"、\"nv12\"、\"rgbp\"、\"bgrp\"、\"gray\" layout int32_t 图片布局，取值范围：hwc、hcw shape vector 多维数据的每一维取值 type ModelBoxDataType::MODELBOX_UINT8 Buffer Data数据类型 视频包数据类型 含义：描述视频解封装后的数据包，用于视频解码。 Buffer Meta字段信息： 参数名称 参数类型 参数含义 pts int64_t 显示时间 dts int64_t 解码时间 rate_num int32_t 帧率分子，帧率为rate_num/rate_den rate_den int32_t 帧率分母, 帧率为rate_num/rate_den duration int64_t 视频时长 time_base double 基准时间，单位为秒 width int32_t 视频宽 height int32_t 视频高 矩形框数据类型 含义：描述矩形区域。 Buffer Meta字段信息：无 Buffer Data信息存放结构如下： typedef struct RoiBox { int32_t x, y, w, h; } ; 检测矩形框类型 含义：描述YOLO检测的结果，包含矩形区域、置信度、分类结果。 Buffer Meta字段信息：无 Buffer Data信息信息存放结构如下。 typedef struct BoundingBox { public: float x; float y; float w; float h; int32_t category; float score; }; HTTP请求数据类型 含义：描述HTTP请求数据类型。 Buffer Meta字段信息： 参数名称 参数类型 参数含义 size size_t 请求体数据大小 method string 请求方法 url string 请求url headers map 请求头信息 endpoint string 请求endpoint HTTP请求响应数据类型 含义：描述HTTP请求响应数据类型。 Buffer Meta字段信息： 参数名称 参数类型 参数含义 status int32_t 返回状态码，暂不支持 headers map 请求头信息,暂不支持 查询功能单元命令 开发者可以通过ModelBox Tool命令查询各个功能单元的详细信息，包括功能介绍、cpu/cuda类型、输入要求、输出信息、配置项、约束等。命令如下： 查询当前系统目录下所有可以加载的功能单元列表： modelbox-tool driver -info -type flowunit 查询单个功能单元详细信息： modelbox-tool driver -info -type flowunit -detail -name xxx 查询当前系统目录和用户自定义路径下所有可以加载的功能单元列表： modelbox-tool driver -info -type flowunit -path xxx 命令帮助信息： modelbox-tool driver 以resize功能单元为例，查询详细结果字段含义如下： [root@996a6346d170 modelbox]$ modelbox-tool driver -info -type flowunit -detail -name resize -------------------------------------- flowunit name : resize # flowunit名称 type : cpu # flowunit类型：cpu：普通CPU; cuda：Nvidia GPU; ascend： Ascend D310推理加速卡 driver name : resize # driver名称：C++场景一个driver对应一个so，一个driver可以包含多个flowunit version : 1.0.0 descryption : @Brief: A resize flowunit on cpu # flowunit 功能简介 @Port paramter: the input port buffer type and the output port buffer type are image. The image type buffer contain the following meta fields: # flowunit 输入输出数据格式 Field Name: width, Type: int32_t Field Name: height, Type: int32_t Field name: width_stride, Type: int32_t Field name: height_stride, Type: int32_t Field name: channel, Type: int32_t Field name: pix_fmt, Type: string Field name: layout, Type: int32_t Field name: shape, Type: vector Field name: type, Type: ModelBoxDataType::MODELBOX_UINT8 @Constraint: the field value range of this flowunit support：'pix_fmt': [rgb_packed,bgr_packed], 'layout': [hwc]. # flowunit 使用约束 group : Image # flowunit 使用约束 inputs : # flowunit 输入端口列表 input index : 1 name : in_image # 输入端口名称 type : # 输入端口类型，预留 device : cpu # 输入端口数据存放设备要求 outputs : # flowunit 输出端口列表 output index : 1 name : out_image # 输出端口名称 device : cpu # 输出端口数据存放位置 options : # flowunit支持的图配置参数 option : 1 name : image_width # 配置参数名称 default : 640 # 配置参数默认值 desc : the resize width # 配置参数含义描述 required : true # 配置参数是否必填 type : int # 配置参数类型 option : 2 name : image_height default : 480 desc : the resize height required : true type : int option : 3 name : interpolation default : inter_linear desc : the resize interpolation method required : true type : list # 配置参数枚举类型 inter_area : inter_area # 配置参数枚举含义 inter_cubic : inter_cubic inter_lanczos4 : inter_lanczos4 inter_linear : inter_linear inter_max : inter_max inter_nearest : inter_nearest warp_fill_outliers : warp_fill_outliers warp_inverse_map : warp_inverse_map ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"flowunits/flowunits-input.html":{"url":"flowunits/flowunits-input.html","title":"输入类","keywords":"","body":"输入类功能单元 输入类功能单元主要用于设置或者对接数据源，如视频流、文件、HTTP等，作为业务流的输入。 video_input 功能描述 用于设置视频文件、视频流数据源信息。 设备类型 cpu 输入端口 无 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_video_url string cpu 数据源地址 配置参数 参数名称 参数类型 是否必填 参数含义 source_url string 是 数据源地址，如文件路径、RTSP流地址 repeat uint64_t 否 并发路数，默认为1 约束说明 无 使用样例 video_input[type=flowunit, flowunit=video_input, device=cpu, deviceid=0, repeat=4, source_url=\"/xxx/xxx.mp4\"] videodemuxer[type=flowunit, flowunit=video_demuxer, device=cpu, deviceid=0, queue_size_event=1000, label=\" | \"] videodecoder[type=flowunit, flowunit=video_decoder, device=cuda, deviceid=0, pix_fmt=\"nv12\"] ... video_input:out_video_url -> videodemuxer:in_video_url videodemuxer:out_video_packet -> videodecoder:in_video_packet videodecoder:out_video_frame -> ... httpserver_async 功能描述 提供HTTP异步服务能力，接受请求后立即回复。 设备类型 cpu 输入端口 无 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_request_info HTTP请求数据类型 cpu 接受HTTP请求信息 配置参数 参数名称 参数类型 是否必填 参数含义 endpoint string 是 HTTP地址 max_requests uint64_t 否 并发最大请求数，默认值为1000 keepalive_timeout_sec uint64_t 否 请求保活时间，单位为秒，默认值为200 cert string 否 openssl证书路径，HTTPS时使用 key string 否 openssl私钥路径，HTTPS时使用 passwd string 否 经过加密的openssl密码，HTTPS时使用. 密码可使用ModelBox-tool加密，详细见ModelBox Tool密码加密章节 key_pass string 否 用于解密openssl密码的密钥，HTTPS时使用。详细见ModelBox Tool密码加密章节 约束说明 无 使用样例 无 httpserver_sync_receive 功能描述 提供HTTP同步请求的接受能力。 设备类型 cpu 输入端口 无 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_request_info HTTP请求数据类型 cpu 接受HTTP请求信息 配置参数 参数名称 参数类型 是否必填 参数含义 endpoint string 是 HTTP地址 max_requests uint64_t 否 并发最大请求数，默认值为1000 time_out_ms uint64_t 否 请求超时时间，单位为毫秒，默认值为5000 keepalive_timeout_sec uint64_t 否 请求保活时间，单位为秒，默认值为200 cert string 否 openssl证书路径，HTTPS时使用 key string 否 openssl私钥路径，HTTPS时使用 passwd string 否 经过加密的openssl密码，HTTPS时使用. 密码可使用ModelBox-tool加密，详细见ModelBox Tool密码加密章节 key_pass string 否 用于解密openssl密码的密钥，HTTPS时使用。详细见ModelBox Tool密码加密章节 约束说明 httpserver_sync_receive 需要和 httpserver_sync_reply 组合使用，使用方式可见样例。 使用样例 httpserver_sync_receive[type=flowunit, flowunit=httpserver_sync_receive, device=cpu, time_out_ms=5000, endpoint=\"http://0.0.0.0:8080\", max_requests=100] mnist_preprocess[type=flowunit, flowunit=mnist_preprocess, device=cpu] mnist_infer[type=flowunit, flowunit=mnist_infer, device=cpu, deviceid=0, batch_size=1] mnist_response[type=flowunit, flowunit=mnist_response, device=cpu] httpserver_sync_reply[type=flowunit, flowunit=httpserver_sync_reply, device=cpu] httpserver_sync_receive:out_request_info -> mnist_preprocess:in_data mnist_preprocess:out_data -> mnist_infer:input mnist_infer:output -> mnist_response:in_data mnist_response:out_data -> httpserver_sync_reply:in_reply_info data_source_parse 功能描述 提供解析视频源的能力，如obs, vcn, vis, restful, url等，用于对接华为云ModelArts推理服务。 设备类型 cpu 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_data string cpu 数据源配置信息 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_video_url string cpu 视频或视频流地址 配置参数 参数名称 参数类型 是否必填 参数含义 retry_enable bool 否 是否需要重连 retry_interval_ms int32_t 否 重连间隔时间，单位为毫秒 retry_count_limit int32_t 否 重连次数，-1为无限重连 约束说明 data_source_parse 需要配合华为云ModelArts推理服务和ModelArts插件配合使用。data_source_parse 一般后面接videodemuxer，用于视频解封装和解码。 使用样例 input1[type=input, device=cpu, deviceid=0] data_source_parser[type=flowunit, flowunit=data_source_parser, device=cpu, deviceid=0, retry_interval_ms = 1000] videodemuxer[type=flowunit, flowunit=video_demuxer, device=cpu, deviceid=0] videodecoder[type=flowunit, flowunit=video_decoder, device=cpu, deviceid=0,pix_fmt=nv12] ... input1 -> data_source_parser:in_data data_source_parser:out_video_url -> videodemuxer:in_video_url videodemuxer:out_video_packet -> videodecoder:in_video_packet videodecoder:out_video_frame -> ... data_source_generator 功能描述 提供data_source_parse的输入信息。 设备类型 cpu 输入端口 无 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_data string cpu 数据源配置信息 配置参数 参数名称 参数类型 是否必填 参数含义 source_type string 是 数据源类型，取值范围：\"url\" url string 是 数据源地址 url_type string 是 文件或者视频流类型，取值范围：\"file\"、\"stream\", 类型不同会影响解封装失败重连策略，视频流类型断流会自动重连，而文件类型则不会重连 约束说明 data_source_generator 需要配合data_source_parse配合使用，主要用于本地调试。 使用样例 data_source_gengerator[type=flowunit, flowunit=data_source_generator, device=cpu, deviceid=0, source_type=\"url\", url=\"http://0.0.0.0:8080/video\", url_type=\"file\"] data_source_parser[type=flowunit, flowunit=data_source_parser, device=cpu, deviceid=0] videodemuxer[type=flowunit, flowunit=video_demuxer, device=cpu, deviceid=0] videodecoder[type=flowunit, flowunit=video_decoder, device=cpu, deviceid=0,pix_fmt=nv12] ... data_source_gengerator:out_data -> data_source_parser:in_data data_source_parser:out_video_url -> videodemuxer:in_video_url videodemuxer:out_video_packet -> videodecoder:in_video_packet videodecoder:out_video_frame -> ... ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"flowunits/flowunits-output.html":{"url":"flowunits/flowunits-output.html","title":"输出类","keywords":"","body":"输出类功能单元 输出类功能单元主要用于业务处理结果的发送。 httpserver_sync_reply 功能描述 提供HTTP同步请求的回复能力。 设备类型 cpu 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_reply_info HTTP请求响应数据类型 cpu 回复HTTP请求信息 输出端口 无 配置参数 无 约束说明 httpserver_sync_reply 需要和 httpserver_sync_receive 组合使用，使用方式可见样例。 使用样例 httpserver_sync_receive[type=flowunit, flowunit=httpserver_sync_receive, device=cpu, time_out_ms=5000, endpoint=\"http://0.0.0.0:8080\", max_requests=100] mnist_preprocess[type=flowunit, flowunit=mnist_preprocess, device=cpu] mnist_infer[type=flowunit, flowunit=mnist_infer, device=cpu, deviceid=0, batch_size=1] mnist_response[type=flowunit, flowunit=mnist_response, device=cpu] httpserver_sync_reply[type=flowunit, flowunit=httpserver_sync_reply, device=cpu] httpserver_sync_receive:out_request_info -> mnist_preprocess:in_data mnist_preprocess:out_data -> mnist_infer:input mnist_infer:output -> mnist_response:in_data mnist_response:out_data -> httpserver_sync_reply:in_reply_info output_broker 功能描述 提供HTTP同步请求的回复能力。 设备类型 cpu 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_output_info Buffer Meta信息:字段名称：output_broker_names 字段类型：string 字段含义：输出目标类型 Buffer Data信息:待发送数据 cpu 需要发送的数据信息 输出端口 无 配置参数 参数名称 参数类型 是否必填 参数含义 retry_count_limit int64_t 否 失败重试次数 retry_interval_base_ms uint64_t 否 第一次重试间隔时间，单位为毫秒 retry_interval_increment_ms uint64_t 否 重试间隔递增时间，单位为毫秒 retry_interval_limit_ms uint64_t 否 最大重试间隔时间，单位为毫秒 约束说明 output_broker 需要配合华为云ModelArts推理服务和ModelArts插件配合使用。 使用样例 无 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"flowunits/flowunits-video.html":{"url":"flowunits/flowunits-video.html","title":"视频类","keywords":"","body":"视频类功能单元 视频类功能单元主要用于视频编解码。 video_demuxer 功能描述 用于设置视频文件或者视频流的解封装。 设备类型 cpu 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_video_url string cpu 数据源地址 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_video_packet 视频包数据类型 cpu 解封装后的视频包，一次输入产生多次输出，直至解封装完成 配置参数 无 约束说明 video_demuxer 一般后面连接 video_decoder ，用于视频解码功能。 使用样例 video_input[type=flowunit, flowunit=video_input, device=cpu, deviceid=0, label=\"\", repeat=4, source_url=\"/xxx/xxx.mp4\"] videodemuxer[type=flowunit, flowunit=video_demuxer, device=cpu, deviceid=0, queue_size_event=1000, label=\" | \"] videodecoder[type=flowunit, flowunit=video_decoder, device=cuda, deviceid=0, label=\" | \", pix_fmt=\"nv12\"] ... video_input:out_video_url -> videodemuxer:in_video_url videodemuxer:out_video_packet -> videodecoder:in_video_packet videodecoder:out_video_frame -> ... video_decoder 功能描述 用于设置视频文件或者视频流的解码。 设备类型 cpu、cuda、ascend 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_video_packet 视频包数据类型 cpu 解封装后的视频包 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_video_frame 视频帧数据类型 与功能单元设备类型一致 解码后的视频帧信息 配置参数 参数名称 参数类型 是否必填 参数含义 pix_fmt string 是 解码图片的格式，取值范围为\"nv12\", \"rgb\", \"bgr\"。注意ascend只支持\"nv12\"格式 约束说明 video_decoder 一般在前面连接 video_demuxer ，用于视频解码功能。 Ascend硬件只支持解码图片格式为\"nv12\"。 使用样例 video_input[type=flowunit, flowunit=video_input, device=cpu, deviceid=0, label=\"\", repeat=4, source_url=\"/xxx/xxx.mp4\"] videodemuxer[type=flowunit, flowunit=video_demuxer, device=cpu, deviceid=0, queue_size_event=1000, label=\" | \"] videodecoder[type=flowunit, flowunit=video_decoder, device=cuda, deviceid=0, label=\" | \", pix_fmt=\"nv12\"] ... video_input:out_video_url -> videodemuxer:in_video_url videodemuxer:out_video_packet -> videodecoder:in_video_packet videodecoder:out_video_frame -> ... video_encoder 功能描述 用于设置视频文件或者视频流的编码。 设备类型 cpu 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_video_frame 图片数据类型 cpu 解封装后的视频包 输出端口 无 配置参数 参数名称 参数类型 是否必填 参数含义 default_dest_url string 是 视频编码的流路径或者文件路径 format string 是 编码输出类型，取值范围为\"rtsp\", \"flv\", \"mp4\" encoder string 否 视频编码格式，默认值为 \"mpeg4\" 约束说明 无 使用样例 ... videoencoder[type=flowunit, flowunit=video_encoder, device=cpu, deviceid=0, encoder=mpeg4, format=mp4, default_dest_url=\"/tmp/car_detection_result.mp4\"] ... -> videoencoder:in_video_frame ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"flowunits/flowunits-image.html":{"url":"flowunits/flowunits-image.html","title":"图像类","keywords":"","body":"图像操作类功能单元 resize 功能描述 对图片进行缩放操作。 设备类型 cpu、cuda、ascend 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_image 图片数据类型 与功能单元设备类型一致 源图片信息 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_image 图片数据类型 与功能单元设备类型一致 结果图片信息 配置参数 参数名称 参数类型 是否必填 参数含义 image_width uint32_t 是 缩放后的图片宽 image_height uint32_t 是 缩放后的图片高 interpolation uint32_t 否 插值方法，不同硬件取值范围不同。 cpu场景：\"inter_nearest\"、\"inter_linear\"、\"inter_cubic\"、\"inter_area\"、\"inter_lanczos4\"、\"inter_max\"、\"warp_fill_outliers\"、\"warp_inverse_map\", 默认值为\"inter_nearest\" cuda场景：\"inter_nn\"、\"inter_linear\"、\"inter_cubic\"、\"inter_super\"、\"inter_lanczos\"， 默认值为\"inter_nn\" ascend场景：\"default\"、\"bilinear_opencv\"、\"nearest_neighbor_opencv\"、\"bilinear_tensorflow\"、\"nearest_neighbor_tensorflow\"，默认值为\"default\" 约束说明 由于底层实现差异，不同硬件支持插值方式不同。 ascend硬件当前只支持输入图片格式为\"nv12\" 使用样例 无 padding 功能描述 对图片进行缩放操作。 设备类型 cpu、cuda、ascend 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_image 图片数据类型 与功能单元设备类型一致 源图片信息 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_image 图片数据类型 与功能单元设备类型一致 结果图片信息 配置参数 参数名称 参数类型 是否必填 参数含义 image_width uint32_t 是 填充后的图片宽 image_height uint32_t 是 填充后的图片高 vertical_align string 否 纵向的对齐方式，取值范围：\"top\"、\"center\"、\"bottom\",默认为\"top\" horizontal_align string 否 横向的对齐方式，取值范围：\"left\"、\"center\"、\"right\",默认为\"left\" padding_data string 否 填充的像素值，格式：\"255,255,0\"， 参数顺序和数据维度对应。默认为\"0,0,0\" need_scale bool 否 是否需要改变大小，默认为ture interpolation uint32_t 否 插值方法，不同硬件取值范围不同。 cpu场景：\"inter_nearest\"、\"inter_linear\"、\"inter_cubic\"、\"inter_area\"、\"inter_lanczos4\"、\"inter_max\"、\"warp_fill_outliers\"、\"warp_inverse_map\", 默认值为\"inter_nearest\" cuda场景：\"inter_nn\"、\"inter_linear\"、\"inter_cubic\"、\"inter_super\"、\"inter_lanczos\"， 默认值为\"inter_nn\" ascend场景：\"default\"、\"bilinear_opencv\"、\"nearest_neighbor_opencv\"、\"bilinear_tensorflow\"、\"nearest_neighbor_tensorflow\"，默认值为\"default\" 约束说明 由于底层实现差异，不同硬件支持插值方式不同。 Ascend硬件当前只支持输入图片格式为\"nv12\" 使用样例 无 crop 功能描述 对图片进行缩放操作。 设备类型 cpu、cuda、ascend 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_image 图片数据类型 与功能单元设备类型一致 源图片信息 in_region 矩形框数据类型 cpu 裁剪区域 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_image 图片数据类型 与功能单元设备类型一致 结果图片信息 配置参数 无 约束说明 Ascend硬件当前只支持输入图片格式为\"nv12\" 使用样例 无 normalize 功能描述 对数据进行归一化。 设备类型 cpu、cuda 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_data Tensor数据类型 与功能单元设备类型一致 源数据 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_data 图片数据类型 与功能单元设备类型一致 结果后数据,输出Buffer数据类型为ModelBoxDataType::MODELBOX_FLOAT 配置参数 参数名称 参数类型 是否必填 参数含义 standard_deviation_inverse string 是 归一化参数, 参数格式：\"0.003921568627451,0.003921568627451,0.003921568627451\" ，0.00392156862745为1/255， 参数顺序和数据维度对应 约束说明 无 使用样例 无 mean 功能描述 对数据进行减均值操作。 设备类型 cpu、cuda 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_data Tensor数据类型 与功能单元设备类型一致 源数据 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_data 图片数据类型 与功能单元设备类型一致 结果后数据, 输出Buffer数据类型为ModelBoxDataType::MODELBOX_FLOAT 配置参数 参数名称 参数类型 是否必填 参数含义 mean string 是 减均值参数, 参数格式：\"124.5, 116.5, 104.5\" ，参数顺序和数据维度对应 约束说明 无 使用样例 无 color_convert 功能描述 对图片进行颜色通道转换。 设备类型 cuda 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_image 图片数据类型 与功能单元设备类型一致 源图片信息 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_image 图片数据类型 与功能单元设备类型一致 结果图片信息 配置参数 参数名称 参数类型 是否必填 参数含义 out_pix_fmt string 是 转换后的通道格式，取值范围：\"bgr\", \"rgb\", \"gray\" 约束说明 支持场景：\"rgb\" 转 \"bgr\"、\"bgr\" 转 \"rgb\"、 \"rgb\" 转 \"gray\"、 \"bgr\" 转 \"gray\"、 \"gray\" 转 \"bgr\"、 \"gray\" 转 \"rgb\" 使用样例 无 image_rotate 功能描述 对图片进行旋转。 设备类型 cpu、cuda 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_image 图片数据类型 与功能单元设备类型一致 源图片信息 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_image 图片数据类型 与功能单元设备类型一致 结果图片信息 配置参数 参数名称 参数类型 是否必填 参数含义 rotate_angle int_32 否 按顺时针旋转角度，取值范围：90, 180, 270。 如果不填该参数时，默认根据输入Buffer Meta携带的\"rotate_angle\"字段旋转。可用于视频解码携带\"rotate\"信息的场景 约束说明 无 使用样例 无 image_decoder 功能描述 对图片解码。 设备类型 cpu、cuda 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_encoded_image vector 待解码图片的二进制数据 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_image 图片数据类型 与功能单元设备类型一致 结果图片信息 配置参数 参数名称 参数类型 是否必填 参数含义 pix_fmt string 是 解码后的通道格式，取值范围：\"bgr\", \"rgb\", \"nv12\"。cuda场景不支持\"nv12\" 约束说明 cuda场景图片解码格式只支持\"bgr\", \"rgb\"，不支持\"nv12\"。 使用样例 无 image_preprocess 功能描述 对图片做预处理：包含减均值、归一化、通道转换 。 设备类型 cuda 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_image 图片数据类型 与功能单元设备类型一致 源图片信息 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_data Tensor数据类型 与功能单元设备类型一致 ,输出Buffer数据类型为ModelBoxDataType::MODELBOX_FLOAT 配置参数 参数名称 参数类型 是否必填 参数含义 output_layout string 是 输出数据的布局类型，取值范围：\"hwc\", \"chw\" mean string 是 减均值参数, 参数格式：\"124.5, 116.5, 104.5\" ，参数顺序和数据维度对应 standard_deviation_inverse string 是 归一化参数, 参数格式：\"0.003921568627451,0.003921568627451,0.003921568627451\" ，0.00392156862745为1/255， 参数顺序和数据维度对应 约束说明 输入图片布局仅支持hwc 使用样例 无 draw_bbox 功能描述 在图片上画框, 一般用于YOLO物体检测结果在原图上的显示。 设备类型 cpu 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_image 图片数据类型 与功能单元设备类型一致 源图片信息 in_region vector矩形框数据类型> cpu 待画框区域列表 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_image 图片数据类型 与功能单元设备类型一致 结果图片信息 配置参数 无 约束说明 无 使用样例 无 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"flowunits/flowunits-generic.html":{"url":"flowunits/flowunits-generic.html","title":"通用类","keywords":"","body":"通用类功能单元 buff_meta_mapping 功能描述 提供Buffer Meta 字段名称和值的转换功能。 设备类型 cpu 输入端口 端口名称 数据格式 数据存放设备类型 端口含义 in_data 无限制 与功能单元设备类型一致 源数据 输出端口 端口名称 数据格式 数据存放设备类型 端口含义 out_data 无限制 与功能单元设备类型一致 转换数据 配置参数 参数名称 参数类型 是否必填 参数含义 src_meta string 是 源数据Meta字段名称 dest_meta string 是 转换后数据Meta字段名称 rules string 否 src_meta转换为dest_meta时值的替换规则, 格式为： \"1=2,3=4,5=6\"表示将源数据src_meta字段值如果是1，则dest_meta值修改为2，如果是3则修改为4，如果是5则修改为6。再如\"abc=efg\"，则表示源数据src_meta字段值如果值是\"abc\"则dest_meta值修改为\"efg\",不填则不进行值的替换，仅将字段名src_meta替换为dest_meta 约束说明 无 使用样例 ... meta_mapping[type=flowunit, flowunit=buff_meta_mapping, device=cpu, deviceid=0, src_meta=\"src_name\", dest_meta=\"dest_name\", rules=\"abc=efg\"] ... ...->meta_mapping:in_data meta_mapping:out_data -> ... ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"flowunits/flowunits-virtual.html":{"url":"flowunits/flowunits-virtual.html","title":"配置类","keywords":"","body":"配置类功能单元 配置类功能单元是只由ModelBox提供的一类特殊的功能单元。 配置类功能单元是基于一些相似功能的通用功能单元抽象出来的一个功能单元模板，它不能直接使用，需要通过配置文件实例化之后就可以当作正常功能使用。当前模板类功能单元主要有：inference、yolo_postprocess。 yolo_postprocess 功能描述 提供YOLO模型后处理的模板，只需要填写配置文件，即可实现YOLO模型的后处理功能单元 设备类型 cpu 使用说明 实现自定义功能单元：编写配置文件，配置文件说明如下 # 基础配置 [base] name = \"yolobox_name\" # 功能单元名称 version = \"1.0.0\" # 功能单元组件版本号 description = \"a common cpu yolobox flowunit\" # 功能单元功能描述信息 type = \"yolo_postprocess\" # yolo_postprocess类型功能单元标识 virtual_type = \"yolov3_postprocess\" # YOLO后处理类型，当前只支持yolov3_postprocess，后续可扩展其他YOLO版本类型 device = \"cpu\" # 当前只支持cpu # YOLO后处理配置 [config] input_width = 800 input_height = 480 class_num = 1 score_threshold = [\"0.6\",\"0.7\"] nms_threshold = [\"0.45\",\"0.3\"] yolo_output_layer_num = 2 yolo_output_layer_wh = [\"25\",\"15\",\"50\",\"30\"] anchor_num = [\"4\",\"4\"] anchor_biases = [\"100.0\",\"72.0\",\"173.12\",\"55.04\",\"165.12\",\"132.0\",\"280.0\",\"252.0\",\" 10.0\",\"8.0\",\"20.0\",\"16.0\",\"30.0\",\"24.0\",\"67.0\",\"56.0\"] # 功能单元输入配置 [input] [input.input1] name = \"in_1\" [input.input2] name = \"in_2\" # 功能单元输出配置 [output] [output.output1] name = \"out_1\" 图连接：编写完成toml文件后，将对应的路径加入ModelBox的图配置中的搜索路径即可使用开发后的yolo_postprocess功能单元。yolo_postprocess功能单元的输入端口和输出端口名称和个数的由toml文件指定，当存在多输入或者多输出时，图构建时需要针对每个输入端口和输出端口进行接口连接。 约束说明 无 使用样例 ... face_pre[type=flowunit, flowunit=face_post, device=cpu] model_detect[type=flowunit, flowunit=model_detect, device=cuda] yolobox_name[type=flowunit, flowunit=face_post, device=cpu] ... face_pre:out_port1 -> model_detect:input1 face_pre:out_port2 -> model_detect:input2 model_detect:output1 -> yolobox_name:in_port1 model_detect:output2 -> yolobox_name:in_port2 yolobox_name:out_port1 -> ... ... ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"cases/cases.html":{"url":"cases/cases.html","title":"样例","keywords":"","body":"样例 ModelBox还提供了以下几个经典案例可供参考： 名称 描述 hello-world 简易的HTTP处理样例 树莓派开发板Mnist 在树莓派开发板开发Mnist识别 car-detection 车辆检测 emotion-detection 表情识别 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"cases/hello-world.html":{"url":"cases/hello-world.html","title":"hello world","keywords":"","body":"Hello World Hello World案例是使用ModelBox搭建一个简易的http请求服务，实现给请求字符串首字母变为大写并增加时间戳的功能。 功能 启动一个HTTP Server监听端口接收HTTP请求，然后从请求体中的msg字段获取输入字符串，将首字母转换为大写并加上时间戳返回给用户。 Request 请求样例： { \"msg\": \"hello world!\" } Response 响应： Hello World! Thu May 30 12:00:00 2022 AI应用开发 环境准备 环境准备可以使用现成ModelBox镜像，也可以从源代码构建ModelBox。本章节使用现成ModelBox镜像开发，如果没有相关的镜像，可以参考编译安装。 使用镜像开发，省去了准备复杂编译环境的巨大工作量，推荐ModelBox开发者直接使用镜像开发，ModelBox镜像相关的指导，可以先参考容器使用章节。 安装启动Docker后，执行下列命令下载Docker镜像 docker pull modelbox/modelbox-develop-tensorrt_7.1.3-cuda_10.2-ubuntu-x86_64:latest 配置并启动容器 可采用一键式脚本快速进入容器。参考一键式启动脚本相关内容。 项目创建与运行 进入容器并且切换至ModelBox开发模式 modelbox-tool develop -s 注意事项： 如果需要通过可视化UI进行图的编排，可参考可视化编排服务章节访问http://[host]:[EDITOR_MAP_PORT]/editor/地址； 如果访问被拒绝，可参考运行编排服务中的访问控制列表相关内容。 连接ModelBox编排服务 服务启动后，可直接连接编排服务，服务启动信息可通过如下命令查询： modelbox-tool develop -q 浏览器访问上述地址的1104端口，注意事项： 如有权限问题，修改conf/modelbox.conf配置文件中的acl.allow数组，增加允许访问的IP范围。 推荐使用vscode的远程开发的终端操作，vscode可以自动建立端口转发。 创建项目工程 点击任务编排 点击项目->新建项目， 新建项目： 输入创建项目的名称:hello-world 路径: /home/[user] 项目模板为: hello world 创建出的文件夹说明可参考工程目录。 运行流程图 在任务编排页面中打开流程图，点击绿色运行按钮可运行流程图； 测试 UI界面测试 在任务管理页面中点击调试可进行api调试，选择hello-world模板，再点击send按钮可进行测试； 脚本测试 可以使用已经准备好测试脚本/usr/local/share/modelbox/demo/hello_world/graph/test_hello_world.py。 直接运行python3 test_hello_world.py得到结果为： Hello World! Thu May 30 12:00:00 2022 流程图开发 流程图编排是根据实际情况将现有业务逻辑拆分为N个功能单元，再将功能单元串联成一个完整的业务的过程。有两种方式可编排流程图，第一种是使用UI进行可视化UI编排，第二种是直接编写图文件。具体可参考流程图开发章节。这里采用第二种方式。 如上图所示，根据业务流程，可将业务划分为3个功能单元，分别为接收HTTP请求，hello world处理，发送HTTP响应。对用图编排文件如下： [graph] format = \"graphviz\" graphconf = '''digraph hello_world_diagraph { node [shape=Mrecord] httpserver_sync_receive[type=flowunit, flowunit=httpserver_sync_receive, device=cpu, time_out_ms=5000, endpoint=\"http://0.0.0.0:7770\", max_requests=100] hello_world[type=flowunit, flowunit=hello_world, device=cpu] httpserver_sync_reply[type=flowunit, flowunit=httpserver_sync_reply, device=cpu] httpserver_sync_receive:out_request_info -> hello_world:in_data hello_world:out_data -> httpserver_sync_reply:in_reply_info } 除了构建图之外，还需要增加必要配置，如功能单元扫描路径，日志级别等，具体可参考样例文件[project_root]/src/graph/hello_world.toml。 功能单元开发 ModelBox提供基础预置功能单元，除此之外还需补充流程图中缺失的功能单元，具体开发可参考功能单元开发章节。 这里接收HTTP请求、发送HTTP响应两个功能单元ModelBox已提供，我们只需实现hello world处理功能单元即可。 hello world功能单元 解析收到的HTTP请求，将首字母转换为大写并加上时间戳。 in_data = data_context.input(\"in_data\") out_data = data_context.output(\"out_data\") for buffer in in_data: # parse request body request_body = json.loads(buffer.as_object().strip(chr(0))) msg = request_body.get(\"msg\") msg = msg.title() msg = addTimestamp(msg) # build output buffer out_string = msg + chr(0) out_buffer = modelbox.Buffer(self.get_bind_device(), out_string.encode('utf-8').strip()) out_data.push_back(out_buffer) ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"cases/mnist-on-sbc.html":{"url":"cases/mnist-on-sbc.html","title":"树莓派开发板Mnist","keywords":"","body":"树莓派开发板中运行mnist Modelbox支持当前主流的几种开发板，比如树莓派4，RK3399，RK3568芯片的Linux系统。本文介绍了，使用已有操作系统，如何从构建ModelBox开始，到训练生成模型，然后在进行推理的全流程。 mnist为一个REST-API服务，通过REST请求，发送base64的手写图片进行推理，REST-API给出推理结果。关于ModelBox中Mnist代码实现，可以先参考第一个应用 编译环境准备 安装依赖的开发库 apt-get update apt-get -y install cmake git wget build-essential npm curl \\ python3 python3-pip python-is-python3 \\ libssl-dev libcpprest-dev libopencv-dev libgraphviz-dev python3-dev \\ libavfilter-dev libavdevice-dev libavcodec-dev pip install requests opencv-python 如上述依赖安装比较慢，可以使用国内的镜像进行安装，具体镜像如下： pip镜像下载： 配置参考： https://mirrors.tuna.tsinghua.edu.cn/help/pypi/ 临时使用参考： pip install -i https://pypi.tuna.tsinghua.edu.cn/simple requests opencv-python apt镜像下载： 配置参考： https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/ https://mirrors.tuna.tsinghua.edu.cn/help/debian/ npm镜像： 配置参考： npm config set registry https://registry.npmmirror.com 下载安装MindSpore-Lite推理引擎 下载aarch64的MindSporeLite： wget https://ms-release.obs.cn-north-4.myhuaweicloud.com/1.7.0/MindSpore/lite/release/linux/aarch64/mindspore-lite-1.7.0-linux-aarch64.tar.gz tar xf mindspore-lite-1.7.0-linux-aarch64.tar.gz mv mindspore-lite-1.7.0-linux-aarch64 /usr/local/ ln -s /usr/local/mindspore-lite-1.7.0-linux-aarch64 /usr/local/mindspore-lite 其他版本下载地址： https://www.mindspore.cn/lite 编译ModelBox 下载并编译ModelBox 主站： git clone https://github.com/modelbox-ai/modelbox.git 国内镜像： git clone https://gitee.com/modelbox/modelbox.git 编译ModelBox: cd modelbox mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Debug make package -j4 如果下载慢，可以切换使用国内镜像： cmake .. -DCMAKE_BUILD_TYPE=Debug -DUSE_CN_MIRROR=yes 安装ModelBox dpkg -i release/*.deb 使用ModelBox编排开发 启动ModelBox编排开发服务 modelbox-tool develop -s 链接ModelBox编排服务 服务启动后，可以直接链接编排服务，服务启动的信息，可以通过如下命令查询： modelbox-tool develop -q 浏览器访问上述地址的1104端口 注意事项： 如有权限问题，修改conf/modelbox.conf配置文件中的acl.allow数组，增加允许访问的IP范围。 推荐使用vscode的远程链接的终端操作，vscode可以自动建立端口转发。远程开发 新建mnist服务 点击任务编排 点击项目->新建项目， 新建项目： 输入创建项目的名称:mnist 路径: /home/[user] 项目模板为: mnist-mindspore 训练模型 使用如下shell命令执行训练： cd ~/mnist/src/flowunit/mnist_infer chmod +x train.sh ./train.sh 启动mnist服务 浏览器打开编排界面，打开mnist项目，点击项目上启动按钮mnist服务。 注意： 若启动失败，请根据界面的提示进行处理。 推理验证： 使用如下命令进行内置的推理验证 cd ~/mnist/src/graph python test_mnist.py ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"cases/car-detection.html":{"url":"cases/car-detection.html","title":"车辆检测","keywords":"","body":"车辆检测 车辆检测案例是对输入视频进行解码，对每一帧图片进行车辆检测，将结果渲染成视频。 功能 输入本地视频文件，识别画面中的车辆，识别结果保存到本地。 模型准备 AI应用开发前需要准备好匹配当前ModelBox版本支持的推理框架和版本的模型文件，这里已经准备好车辆检测torch模型文件。 AI应用开发 首先准备开发环境，然后进入应用开发环节，主要分为流程图编排、功能单元编写、运行与调试、打包部署4个开发步骤。 环境准备 环境准备工作可以参考环境准备，区别是需要选择libtorch镜像： docker pull modelbox/modelbox-develop-libtorch_1.9.1-cuda_10.2-ubuntu-x86_64:latest 项目创建与运行 可参考创建项目，最后可选择创建car_detection项目工程。 本案例是测试本地视频文件，可以测试视频路径可以在video_input节点中设置，结果视频路径在videoencoder节点中设置。 流程图开发 如上图所示，整个流程图分为5个阶段：视频输入/解码、车辆检测预处理、车辆检测推理、车辆检测后处理、视频编码。 视频输入/解码：video_input功能单元用作输入视频配置，后面接视频的解封装、解码功能单元(videodemuxer、videodecoder)得到视频帧; 车辆检测预处理：对视频帧进行预处理(reisze、transpose、normalize); 车辆检测推理：将预处理后的数据交给模型推理(model_inference); 车辆检测后处理：推理后进行后处理，并画出检测框渲染到图像上(yolo_post); 视频编码：最后将渲染结果图编码成视频文件(videoencoder); 整个流程只需要实现蓝色部分功能单元，其他功能单元都在ModelBox中内置，只需修改配置即可使用。具体toml配置文件如下所示： [graph] format = \"graphviz\" graphconf = \"\"\"digraph car_detection { node [shape=Mrecord] video_input[type=flowunit, flowunit=video_input, device=cpu, deviceid=0, source_url=\"/opt/modelbox/demo/video/car_test_video.mp4\"] videodemuxer[type=flowunit, flowunit=video_demuxer, device=cpu, deviceid=0] videodecoder[type=flowunit, flowunit=video_decoder, device=cuda, deviceid=0, pix_fmt=bgr] image_resize[type=flowunit, flowunit=resize, device=cpu, deviceid=0, image_width=512, image_height=288] image_transpose[type=flowunit, flowunit=packed_planar_transpose, device=cpu, deviceid=0] normalize[type=flowunit, flowunit=normalize, device=cpu, deviceid=0, standard_deviation_inverse=\"1,1,1\"] model_inference[type=flowunit, flowunit=car_detect, device=cuda, deviceid=0, batch_size=1] yolox_post[type=flowunit, flowunit=yolox_post, device=cpu, deviceid=0] videoencoder[type=flowunit, flowunit=video_encoder, device=cpu, deviceid=0, encoder=mpeg4, format=mp4, default_dest_url=\"/tmp/car_detection_result.mp4\"] video_input:out_video_url -> videodemuxer:in_video_url videodemuxer:out_video_packet -> videodecoder:in_video_packet videodecoder:out_video_frame -> image_resize:in_image image_resize:out_image -> image_transpose:in_image image_transpose:out_image -> normalize:in_data normalize:out_data -> model_inference:input model_inference:output -> yolox_post:in_feat videodecoder:out_video_frame -> yolox_post:in_image yolox_post:out_data -> videoencoder:in_video_frame }\"\"\" 功能单元开发 开发者只需开发车辆检测推理功能单元(model_inference)、后处理(yolo_post)即可。 车辆检测推理功能单元 ModelBox已经适配了torch推理引擎，推理功能单元只需准备好模型和对应的配置文件即可。 [base] name = \"car_detect\" device = \"cuda\" version = \"1.0.0\" description = \"car detection infer\" entry = \"./yolox_nano_jit_trace_288x512.pt\" type = \"inference\" virtual_type = \"torch\" [input] [input.input1] name = \"input\" type = \"float\" [output] [output.output1] name = \"output\" type = \"float\" 详细代码可参考[project_root]/src/flowunit/car_detect/car_detect.toml。 车辆检测后处理功能单元(yolo_post) 详细代码可参考[project_root]/src/flowunit/yolox_post。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"cases/emotion-detection.html":{"url":"cases/emotion-detection.html","title":"表情识别","keywords":"","body":"表情识别 表情识别案例是对输入视频进行解码，然后对每一帧图片进行人脸检测，如果检测到人脸，则识别人脸表情，最后将结果渲染成视频。 功能 输入本地视频文件，识别画面中的人脸表情，并将识别结果保存到本地。 模型准备 AI应用开发前需要准备好匹配当前ModelBox版本支持的推理框架和版本的模型文件，这里已经准备好人脸检测，表情识别两个torch模型文件。 AI应用开发 首先准备开发环境，备好了之后进入应用开发环节，主要分为流程图编排、功能单元编写、运行与调试、打包部署4个开发步骤。 环境准备 环境准备工作可以参考环境准备，区别是需要选择torch镜像： docker pull modelbox/modelbox-develop-libtorch_1.9.1-cuda_10.2-ubuntu-x86_64:latest 创建项目 可参考创建项目，最后可选择创建emotion_detection项目工程。 本案例是测试本地视频文件，可以测试视频路径可以在video_input节点中设置，结果视频路径在videoencoder节点中设置。 流程图开发 如上图所示，整个流程图分为4个阶段：视频读取/解码、人脸检测、表情识别、结果渲染； 视频读取/解码：video_input功能单元用作输入视频配置，后面接视频的解封装、解码功能单元(videodemuxer、videodecoder)得到视频帧。后续需要进行两次模型推理(人脸检测、表情识别)； 人脸检测：检测视频帧中有没有人脸，进行预处理(custom_reisze、transpose、normalize)、人脸检测模型推理(face_detect)、后处理(face_post)。根据后处理的结果判断是否有人脸，若有人脸则进入表情识别分支，若无人脸则走无人脸分支； 表情识别：一张图可能有多张人脸，需要对每张人脸进行表情识别，这就需要把一个帧图像展开成一个个人脸图片(expand_box)，然后做表情识别预处理(face_resize、face_transpose、face_mean、facenormalize)、推理(emotion_infer)、后处理。然后收集前面展开的每一个人脸推理结果汇聚成一个视频帧的推理结果，这里把后处理与合并两个操作融合成一个功能单元(collapse_emotion)。 结果渲染：人脸图画出检测框渲染到图像上(draw_emotion)，最后将渲染结果图编码成视频文件(videoencoder)。 整个流程只需要实现蓝色部分功能单元，其他功能单元都在ModelBox中内置，只需修改配置即可使用。具体toml配置文件如下所示： [graph] format = \"graphviz\" graphconf = \"\"\"digraph emotion_detection { node [shape=Mrecord] video_input[type=flowunit, flowunit=video_input, device=cpu, source_url=\"/opt/modelbox/demo/video/emotion_test_video.mp4\"] videodemuxer[type=flowunit, flowunit=video_demuxer, device=cpu] videodecoder[type=flowunit, flowunit=video_decoder, device=cuda, pix_fmt=bgr] custom_resize[type=flowunit, flowunit=custom_resize, device=cpu] image_transpose[type=flowunit, flowunit=packed_planar_transpose, device=cpu] mean[type=flowunit, flowunit=mean, device=cpu, mean=\"104, 117, 123\"] normalize[type=flowunit, flowunit=normalize, device=cpu, standard_deviation_inverse=\"1, 1, 1\"] face_detect[type=flowunit, flowunit=face_detect, device=cuda] face_post[type=flowunit, flowunit=face_post, device=cpu, batch_size=1] expand_box[type=flowunit, flowunit=expand_box, device=cpu] face_resize[type=flowunit, flowunit=resize, device=cpu, image_width=224, image_height=224] face_transpose[type=flowunit, flowunit=packed_planar_transpose, device=cpu] face_mean[type=flowunit, flowunit=mean, device=cpu, mean=\"123.675, 116.28, 103.53\"] face_normalize[type=flowunit, flowunit=normalize, device=cpu, standard_deviation_inverse=\"0.0171247538316637, 0.0175070028011204, 0.0174291938997821\"] emotion_infer[type=flowunit, flowunit=emotion_infer, device=cuda, batch_size=1] collapse_emotion[type=flowunit, flowunit=collapse_emotion, device=cpu] draw_emotion[type=flowunit, flowunit=draw_emotion, device=cpu] videoencoder[type=flowunit, flowunit=video_encoder, device=cpu, encoder=mpeg4, format=mp4, default_dest_url=\"/tmp/emotion_detection_result.mp4\"] video_input:out_video_url -> videodemuxer:in_video_url videodemuxer:out_video_packet -> videodecoder:in_video_packet videodecoder:out_video_frame -> custom_resize:in_image custom_resize:out_image -> image_transpose:in_image image_transpose:out_image -> mean:in_data mean:out_data -> normalize:in_data normalize:out_data -> face_detect:input face_detect:out_loc -> face_post:in_loc face_detect:out_conf -> face_post:in_conf face_detect:out_cls -> face_post:in_cls videodecoder:out_video_frame -> face_post:in_image face_post:has_face -> expand_box:in_data expand_box:roi_image -> face_resize:in_image face_resize:out_image -> face_transpose:in_image face_transpose:out_image -> face_mean:in_data face_mean:out_data -> face_normalize:in_data face_normalize:out_data -> emotion_infer:input emotion_infer:confidence -> collapse_emotion:confidence emotion_infer:predicts -> collapse_emotion:predicts collapse_emotion:out_data -> draw_emotion:in_emotion face_post:has_face -> draw_emotion:in_face draw_emotion:out_data -> videoencoder:in_video_frame face_post:no_face -> videoencoder:in_video_frame }\"\"\" 功能单元开发 需要开发的功能单元可参考[project_root]/src/flowunit/目录，这里主要讲解之前案例没有用到的条件功能单元、展开/合并功能单元如何使用： 人脸后处理功能单元(face_post) 这是一个条件功能单元，根据人脸检测模型推理结果判断当前视频帧是否包含人脸。具体toml配置如下： [base] name = \"face_post\" device = \"cpu\" version = \"1.0.0\" description = \"face detection postprocess\" entry = \"face_post@FacePost\" type = \"python\" condition = true # 表示该功能单元为条件功能单元 [config] max_edge = 320 [input] [input.input1] name = \"in_loc\" [input.input2] name = \"in_conf\" [input.input3] name = \"in_cls\" [input.input4] name = \"in_image\" # 条件功能单元的多个输出端口仅能选择一个输出 [output] [output.output1] name = \"has_face\" [output.output2] name = \"no_face\" 详细配置和功能单元代码可参考：[project_root]/src/flowunit/face_post 展开/合并功能单元 展开/合并功能单元一般成对使用。 展开功能单元，在本案例中是将一个视频帧展开成一个个人脸图，具体toml配置如下： [base] name = \"expand_box\" device = \"cpu\" version = \"1.0.0\" description = \"expand each box to emotion detection\" entry = \"expand_box@ExpandBox\" type = \"python\" expand = true # 表示该功能单元为展开功能单元 [input] [input.input1] name = \"in_data\" [output] [output.output1] name = \"roi_image\" 合并功能单元，在本案例中是将前面一帧图像展开的人脸图推理结果合并，具体toml配置如下： [base] name = \"collapse_emotion\" device = \"cpu\" version = \"1.0.0\" description = \"collapse all face emotion\" entry = \"collapse_emotion@CollapseEmotion\" type = \"python\" collapse = true # 表示该功能单元为合并功能单元 [input] [input.input1] name = \"confidence\" [input.input2] name = \"predicts\" [output] [output.output1] name = \"out_data\" 详细配置和功能单元代码可参考：[project_root]/src/flowunit/collapse_emotion ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"solution/solution.html":{"url":"solution/solution.html","title":"ModelBox应用","keywords":"","body":"ModelBox应用 ModelBox应用是ModelBox提供的一些常用解决方案，每个解决方案包含了模型的预处理、推理、后处理，开发者可以直接调用这些API将这些解决方案集成到业务使用。 解决方案列表如下： 案例名称 描述 案例图例 手势识别 识别图片中的手部关键点 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"solution/hand-pose-detection.html":{"url":"solution/hand-pose-detection.html","title":"手势识别","keywords":"","body":"手势识别 手势识别解决方案是ModelBox提供可直接调用的API，开发者集成手势识别solution后，可以完成手势关键点的识别。检测效果如下图所示： 输入 输入类型为ModelBox::Buffer，其中包含Data与Meta两种数据，具体要求如下： Data：图片二进制数据 Meta：无要求 输出 输出类型为ModelBox::Buffer，其中包含Data与Meta两种数据，具体如下： Data：检测后图片，若检测到手，则画出手的框与手指连线；若未检测到手，则为原图。 Meta： width：图片宽度。 height：图片高度。 channel：图片通道数。 pix_fmt：图片格式。 has_hand：值判断是否有检测到手，True为检测到有手，False为未检测到手。为True才会有bboxes与hand_pose参数。 bboxes：检测到手的box坐标。 hand_pose：检测到手指位置坐标，每只手5根手指，每根手指3个关键点坐标。 获取方法 可以通过下面两种方式获取： 安装包下载：进入下载链接，根据系统选择对应的版本进行下载到libtorchModelBox开发镜像中，直接安装后可调用相关接口可以运行。 源码编译：进入解决方案代码仓，克隆代码仓到libtorchModelBox开发镜像中，编译hand_pose_detection解决方案并打包，具体命令如下： git clone https://github.com/modelbox-ai/modelbox-solutions.git cd modelbox-solutions mkdir build cd build cmake .. make package -j16 hand_pose_detection 编译打包完成后，将在release目录下生成对应的安装包，安装在镜像中即可。 使用样例 C++样例 头文件 需要引入如下头文件，并在编译时链接modelbox库： #include #include Solution创建初始化和启动 std::shared_ptr CreateHandPoseDetectionSolution() { ModelBoxLogger.GetLogger()->SetLogLevel(modelbox::LogLevel::LOG_INFO); auto flow = std::make_shared(); modelbox::Status mb_ret; mb_ret = flow->InitByName(\"hand_pose_detection\"); if (mb_ret != modelbox::STATUS_OK) { MBLOG_ERROR StartRun(); if (mb_ret != modelbox::STATUS_OK) { MBLOG_ERROR 外部数据交互 待处理数据的输入，和处理完成后结果获取。 // 数据发送获取 modelbox::Status Process(std::shared_ptr flow, const std::string &test_file) { // 创建输入输出句柄 auto stream_io = flow->CreateStreamIO(); modelbox::Status mb_ret; // 创建输入 auto buffer = stream_io->CreateBuffer(); mb_ret = BuildInputData(test_file, buffer); if (mb_ret != modelbox::STATUS_OK) { MBLOG_ERROR Send(\"input\", buffer); // 获取输出 std::shared_ptr output_buffer; stream_io->Recv(\"output\", output_buffer); mb_ret = ProcessOutputData(output_buffer); if (mb_ret != modelbox::STATUS_OK) { MBLOG_ERROR 创建输入 modelbox::Status BuildInputData(const std::string &img_path, std::shared_ptr &input_buffer) { FILE *pImg = fopen(img_path.c_str(), \"rb\"); if (pImg == nullptr) { MBLOG_ERROR Build((size_t)fSize); auto buffer_data = (char *)input_buffer->MutableData(); fread(buffer_data, fSize, 1, pImg); fclose(pImg); return modelbox::STATUS_OK; } 处理输出 void ProcessOutputData(std::shared_ptr &output_buffer) { bool has_hand; output_buffer->Get(\"has_hand\", has_hand); MBLOG_INFO Get(\"height\", height); output_buffer->Get(\"width\", width); cv::Mat image(height, width, CV_8UC3); memcpy_s(image.data, image.total() * image.elemSize(), output_buffer->ConstData(), output_buffer->GetBytes()); cv::imwrite(\"path_to_result.jpg\", image); } 资源释放 void FlowStop(std::shared_ptr flow) { // 结束执行 flow->Stop(); } Python样例 需要引入的包 import modelbox import cv2 import numpy as np 定义手势识别类 class HandPoseDetection: ## 初始化，设置日志级别 def __init__(self, log_level=modelbox.Log.Level.INFO): self.log = modelbox.Log() self.log.set_log_level(log_level) ## 初始化手势识别Solution def Init(self): self.flow = modelbox.Flow() ret = self.flow.init_by_name(\"hand_pose_detection\") if ret == False: modelbox.error(ret) return ret ret = self.flow.start_run() if ret == False: modelbox.error(ret) return ret ## 设置输入图片路径，输出结果保存路径，返回是否检测到手 def Process(self, input_file, output_path): stream_io = self.flow.create_stream_io() file = open(input_file, \"rb\") input_buffer = stream_io.create_buffer(file.read()) file.close() stream_io.send(\"input\", input_buffer) stream_io.close_input() result = stream_io.recv(\"output\") has_hand = result.get(\"has_hand\") msg = \"has hand: \" + str(has_hand) modelbox.info(\"has hand: \", msg) if has_hand: width = result.get(\"width\") height = result.get(\"height\") channel = result.get(\"channel\") out_img = np.array(result.as_object(), dtype=np.uint8) out_img = out_img.reshape(height, width, channel) cv2.imwrite(output_path, out_img) return has_hand 主函数 if __name__ == '__main__': hand_pose = HandPoseDetection() hand_pose.Init() hand_pose.Process(input_image_path, output_image_path) ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"basic-conception/basic-conception.html":{"url":"basic-conception/basic-conception.html","title":"基础概念","keywords":"","body":"基础概念 ModelBox为更好的支撑应用流程，抽象了许多概念，理解这些概念对于应用问题的解决将会有很大帮助。 本章节主要从下图中的几个概念对ModelBox进行讲解，它们是ModelBox中十分重要的概念，开发者将时刻与它们打交道。 通过ModelBox开发应用时，开发者需要将应用程序的逻辑表达为流程图，应用功能被拆分为多个功能单元。采用这种方式的目的是：对数据流(如视频)处理可以利用多段流水线并行提供吞吐量；通用的功能单元可以复用；通用的流程图可以复用。 数据经由INPUT Node产生，按箭头指向，流向Process Node，Process Node处理数据后，再发送给Sink Node汇总处理结果，这是一个典型的数据处理过程，这个过程中，涉及到了图(Graph)、节点(Node)、端口(Port)、数据流(Stream)、数据块(Buffer)。 流程图 流程图章节将从构成、连接、执行等方面进行介绍，包含了图、节点、端口、边的概念。 功能单元 功能单元是重要的用户接口，应用的功能需通过多个单元才能实现。本章介绍将较为系统的介绍功能单元相关的信息。 数据流 一系列关联的顺序数据实体组成了数据流，在ModelBox中数据流是主要处理对象，比如视频流，音频数据流等。 Buffer 流中包含多个数据实体，单个数据实体在ModelBox中由Buffer承载。单个Buffer包含了数据的元数据Meta部分和数据内容部分，它是数据在Node间的流动实体。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"basic-conception/graph.html":{"url":"basic-conception/graph.html","title":"流程图","keywords":"","body":"流程图 概念介绍 图的组成 ModelBox中，使用图来表达程序的执行逻辑。完整的图由节点和边组成，其中节点细分为三类：输入端口、输出端口、功能节点。 节点： 输入端口：可选，可用于从图外发送输入数据，是图的起始，可以存在多个输入端口，需要保证多个输入端口传入的数据数量是一致的。 输出端口：可选，可用于在图外接受输出输出，是图的结尾，可以存在多个输出端口，需要保证多个输出端口数据是可以匹配的。 功能节点：必选，构成图功能的节点，和流程图类似，每个功能节点表示程序的操作步骤，每个步骤负责处理传入的数据处理并产生输出数据。节点可以定义多个输入端口和输出端口，节点从输入的多个端口中同时获取数据，结果再发送到多个输出端口中。处理时，节点需要关心输入所在的设备以及构建输出时使用的设备。 边：边是有向的，描述了节点之间数据传递的关系。边从前一个节点的输出端口，连接到下一个节点的输入端口。数据从输出端口发出后，由下一个节点输入端口的缓存队列保存数据，数据中包含多条数据流，每个数据流包含多个Buffer，例如视频场景下一个端口接收多个视频，每个视频存在多帧数据。 图的驱动 ModelBox的图中节点执行是由数据驱动的，即单个节点的输入端口数据准备完毕后，便会调用节点的处理函数进行数据处理，并将输出放入到后续节点的输入队列中。 数据准备 节点在输入端口数据准备就绪后，会进入执行。其中准备动作，包含了数据流划分、数据流匹配、按批划分、转移到目标设备等动作。 数据流划分 前面提到单个端口的输入队列中包含多个数据流，所以首先对接收到的Buffer按照从属的流进行划分。 数据流匹配 多个端口之间，流与流存在关联关系，如两个不同输入端口的流曾来自同一个节点的输出，那么这两个端口的流是存在匹配关系的，对于单次数据处理时，这两个端口的两个匹配数据流将会各取一个Buffer用来处理。因此节点的多端口输入数据流的Buffer数量要一致，否则会出现匹配失败。 按批划分 对于支持批处理的节点，可以将匹配好的数据Batch切分，使得一次节点的处理函数调用传入Batch大小的数据进行处理。 转移到目标设备 每个节点无需关心输入数据的来源，只需表明输入端口需要处理的数据应当存放的设备(默认与节点依赖设备一致)，对于不同来源的数据，会由框架进行数据搬移，保证输入的设备正确。 节点与功能单元 节点的实现在ModelBox中称为功能单元，一个功能单元是可以作为图中多个节点来使用，每个节点包含各自的功能单元实例，数据处理时，节点将准备好的数据传入功能单元处理。功能单元的介绍详见功能单元 图的连接 在ModelBox流程图中，数据流从单个节点开始或从输入端口开始。 单个节点作为开始作为开始的节点只能有一个，当前节点作为数据的输入源，如HTTP场景下，第一个节点监听HTTP请求，并将请求作为一路流向后发送处理。 多个输入端口作为开始主要用于自定义应用模式下，数据从图的外部传入，可以将图的外部看作单个起始节点，多个输入端口可以看作这个节点的输出。 流程控制与程序流程图不同的是，暂不支持循环 普通连接：数据从当前节点输出，直接送入后续的节点输入。一个端口的输出可以给多个输入端口，一个输入端口只能接受一个输出端口的数据。 条件连接：需要设置当前节点作为条件节点，对于条件节点，每个输出端口为一个条件分支，一次处理中，所有分支只能有一个分支可以产生结果。例如条件节点输入一个数据流，奇数序号走A端口，偶数走B端口，经过条件节点后，完整的输入数据流被切分为了两个数据流。 对于条件数据流的多端口输出，在后续是可以重新合并回一个完整的数据流，需要将多个分支的数据流，连接到合并节点的同一个输入端口上，且只有条件流的合并才能做到多个输出端口连接到一个输入端口上。 数据流结尾可以是多个节点或者输出端口。 多个节点作为结束数据从前向后，到达最后一个节点，此节点可以将处理的结果传递到图外，如HTTP场景下，最后一个节点将处理结果返回给发送请求的客户端。 多个输出端口作为结束主要用于自定义应用模式下，数据需要传递到图的外部，可以将图的外部看作单个结束节点，多个输出端口可以看作这个节点的输入，因此数据默认是需要匹配的，也可以通过配置说明多个端口的数据流不需要匹配。 图的执行及优先级 图是由数据驱动执行，多个节点可能同时存在数据需要执行，因此，节点需要确定执行的优先级，保证重要的节点先执行。节点的优先级是通过其拓扑结构来确定，越靠近结束的节点优先级越高，一是为了保证数据数据尽快完成，再是防止靠近入口的节点堆积过多数据。有了优先级后，调度器就能够从可执行的节点中挑选高优先级的节点去执行。 节点的优先级如下图所示： 当某个节点被调度后，其状态会置为执行中，此时后续的数据会在输入队列中等待，直到本次执行完成后，才可以被继续调度执行，因此同一时刻，一个节点只会处理一批输入数据。在一批数据的处理中，会根据数据的关联性，决定这批数据执行的并发策略，尽量快速的处理当前批的数据。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"basic-conception/flowunit.html":{"url":"basic-conception/flowunit.html","title":"功能单元","keywords":"","body":"功能单元 功能单元(FlowUnit)，是应用流程中处理数据的基本单元。 功能单元基础概念 在ModelBox中，功能单元被设计为依赖特定设备完成固定功能的代码单元，因此一个功能单元应当有明确的功能名、设备类型，例如ModelBox内置的resize功能单元有cpu、cuda、ascend三个硬件的版本，使用时需要明确指定功能单元名、设备类型、设备号。多设备更多了解请见设备章节。 功能单元的输入数据所在的设备默认与功能单元依赖的设备一致，如有特殊需要，可以对单个输入端口配置特定的设备，框架会保证输入的数据在特定设备上已准备就绪。功能单元的输出端口无需指定设备，输出不会发生数据搬移。功能单元使用输出构建接口创建Buffer时，创建的Buffer所在的设备是当前功能单元声明的依赖设备类型。 图中介绍了节点与功能单元的对应关系。开发者在设置节点时，需要通过在节点上配置功能单元名、设备类型、设备号来指明节点所使用的功能单元。 功能单元实现方式 在ModelBox中，功能单元支持不同的实现方式以适用于不同场景的使用。 实现方式 说明 Inference 推理功能单元，通过模型和简单的配置文件即可完成 Python Python编写功能单元，需要提供Python代码，并提供配置文件 C++ C++编写功能单元，需要编译成动态库 Config 配置功能单元，通过编写特定配置文件即可完成，如YOLO后处理 功能单元的具体开发操作参考功能单元开发 加载运行流程 为方便功能单元的扩展和复用，它被设计为插件的方式进行加载，如下简要介绍其加载的过程。 FlowUnit插件的加载流程如上图： 初始化过程 ModelBox模块先扫描插件目录。 对扫描到的插件，调用DriverDescription获取FlowUnit插件信息。信息包括功能单元名称，功能单元版本号，执行的硬件信息，描述信息，配置参数信息。 当启动Flow时，则调用插件的DriverInit初始化插件。 初始化插件完成后，调用CreateDriverFactor创建插件工厂类。 之后调用FLowUnitFactor::FlowUnitProbe获取FlowUnit信息，信息包括输入，输出参数。 初始化完成后，业务调用Flow初始化图。 在图初始化时，首先通过图中指定的FlowUnit名称选择对应的FlowUnit实例化，调用实例化对象的Flow::Open初始化FlowUnit对象。 图初始化完成后，若有数据达到，则调用当前节点实例的Flow::Process处理数据；若当前功能单元选择的类型是流，还会调用Flow::DataPre接口，再调用Flow::Process接口，流数据结束时，调用Flow::DataPost接口。 当图运行完成后，则调用Flow::Close接口关闭图。 整个Flow结束时，ModelBox模块卸载FlowUnit插件，调用插件的DriverFini函数清理资源。 数据处理 节点存在多种数据处理方式可以选择，功能单元提供了数据处理选项让功能单元开发者来决定对应节点的数据应当如何处理，如下将介绍数据处理的过程及选项的具体作用。 图中每个节点包含多个输入端口，每个输入端口包含一个数据队列，用于缓存待处理的数据，节点一次运行中，从端口缓存数据里读取数据，并进行处理，产生新的多输出端口数据，多个输出端口之间的输出数据量一致。这个过程中，称输出端口数据继承自输入端口数据。 单个端口中包含多个流的数据，因此取出的数据需要先区分所属的数据流。 多个端口的数据流之间存在匹配关系，匹配关系定义为继承路线中，存在相同的数据源。按照匹配关系将多端口的数据流进行匹配后，记录在DataContext中，多组匹配的流组成多个DataContext，准备接下来的处理。 对于多组匹配流的数据，存在两种处理方式：stream类型，表示功能单元处理数据时，希望处理流式数据，关心流的状态，因此多个DataContext会并发操作，单个DataContext的数据会按照batch逐次交给功能单元处理；normal类型，主要用于resize、推理这类关注于数据处理的功能单元，可以混合多个DataContext的数据，然后再按照batch划分为多组，尽可能的并发操作多组batch的数据。对于normal类型的，要求其输入数据与输出数据量一致。 数据处理完毕后，功能单元的处理结果会经过节点的后处理，保证其有序性，开发者在编写功能单元时，只需要关心选择stream或者normal来决定当前处理是否需要保证数据的流式属性。 特殊处理： 以上介绍了基本的数据处理过程，ModelBox还支持了如下几种特殊的处理方式，用于解决特殊场景下的问题。 condition：条件选项，用于解决数据需要选择某个分支进行传递的场景。设置此选项后，功能单元的一次输入batch为1，功能单元需要决定产生的单个输出Buffer从哪一个端口输出。 expand: 数据展开选项，典型场景为视频流处理时，单个图片检测出多个框，开发者希望能标记出每个框所属于的图片，方便后续能够将属于同一个图片的多个框收集起来一起处理。在形式上，展开操作是对于原始数据流的每一个Buffer，执行展开操作，如单图片的多框，完成检测时，多个框还存储在一个Buffer中，执行展开操作后，一个Buffer转换成为一个子数据流，原始数据流就转换成为了多个子数据流。这个过程ModelBox称为数据流层级下降了一次。展开后的子数据流依然是正常处理，只是数据流的关系中增加了一层从属标记。 collapse: 数据合并选项，可以用于合并展开后的子数据流，每次处理默认传入单个子流的数据，对于每个子流，只允许功能单元合并生成单个Buffer。这样就完成了从多个子流合并成为单个数据流的过程。 上文从数据处理的角度，介绍了功能单元的类型，开发者可以根据使用场景选择设置功能单元类型。下文将从功能单元角度进行介绍。 功能单元类型 Normal功能单元 说明：在处理数据时，功能单元只关心数据的批处理，不关心流的属性，且不记录任何状态。设置为该类型时，其一次process调用处理的数据可能来自多个数据处理任务，且process会并发调用，同一个任务内的数据在此时不保证处理的先后。数据处理完毕后会由框架重新收集排序，无需关心当前功能单元对整个数据流后续的影响。功能单元只需要实现Process接口。 约束：输出Buffer的数量必须与输入Buffer的数量一致 举例：框架内预置的resize、crop、推理等数据处理功能单元。 Stream功能单元 说明：在处理数据时，功能单元每次process都是处理当前数据流的数据，针对当前数据流可以保存状态，并且process在数据流上会保持顺序处理。设置为该类型时，框架会保证一个数据流的数据会顺序的进入process，不同数据流的数据会并发进入process，开发者无需关心数据之前是否是有序的，在process此处，已经由框架保证顺序。DataPre用于通知数据流的开始，DataPost用于通知数据流的结束。 举例：框架内预置的demuxer、decoder等流数据处理功能单元。 条件功能单元 说明： 在处理数据时，功能单元关心数据流中的单个Buffer的分支，经过分支后的输出流是输入流的子流，它们只包含了输入流的部分。 约束： 暂不可与条件流程外的完整流进行匹配。 展开功能单元 说明：在处理数据时，功能单元需要对单个Buffer展开成为一个子流，并且期望之后可以通过合并功能单元，将数据按照展开关系进行合并。 约束：功能单元的处理batch只能是1，输出必须有Buffer。经过展开后的流不可与展开前的数据流进行匹配。 合并功能单元 说明：在处理数据时，功能单元需要将展开的数据流进行合并。 约束： 对于一个子流的合并，只能产生一个输出Buffer。 功能单元类别与接口说明 类别 说明 需要重写的接口 Normal功能单元 只关心数据处理 Process Stream功能单元 关心数据流的处理，因此可以获得数据流开始结束的通知 DataPre, Process, DataPost 条件功能单元 为输出的单个Buffer选择一个端口输出 Process 展开功能单元 对每个输入Buffer，展开为一个子流 Process 合并功能单元 对每个子流，合并成为单个Buffer DataPre, Process, DataPost 功能单元类型样例 下图以车辆检测为例子说明涉及到功能单元类型 例子 车辆跟踪推理DEMO，当发现车辆时，对车辆画框图提示。 Flow流程图 流程说明： 将文件数据发送给VideoDemux，VideoDemux将数据解开packet后发送给VideoDecoder。 VideoDecode获取packet并解码为图像。 图像数据分别发送到两个流程，一个发送给ImageResize，一个发送给ImageRender。 ImageResize将图像数据进行resize。 resize后的图像，发送给CarDetect进行模型推理。 推理后的数据发送给Box进行框信息处理。 Box输出框信息。 ImageRender接收两路输入，图像和框图信息，并对图像进行画框。 画框后的图像，输出到编码器VideoEncoder。 VideoEncode对图像进行编码，并发送给RTSP服务器。 流程上，使用了8个功能单元，1个是输入类，3个是处理流数据的视频功能单元，其他是图像处理和推理类的通用功能单元。 上图流程的涉及的功能单元列表以及类别 功能单元名称 功能 功能分类 业务分类 解释 File Reader 读取数据文件 流数据拆分类 输入类 输入是一个URL，输出是一个文件流。 Video Demux 解数据封包 流数据类 视频类 输入是文件流，输出是一组连续的packet流。 Video Decoder 视频解码 流数据类 视频类 输入是packet流，输出是独立的图像数据。 Image Resize 图像大小调整 通用类 图像类 输入是一张图像，输出也是一张图像。 Car Detect 车辆推理 通用类 推理类 输入是Tensor，输出是Tensor。 Box 框选取 通用类 后处理类 输入是Tensor，输出是框信息。 Image Render 合并框图信息 通用类 图像类 输入是两组数据，图像和框图，输出是图像。 Video Encoder 视频编码 流数据类 视频类 输入是多张图像，输出是一个视频流。 要查阅ModelBox所有预置的功能单元，请参考预置功能单元章节。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"basic-conception/stream.html":{"url":"basic-conception/stream.html","title":"数据流","keywords":"","body":"数据流 数据流，在ModelBox中称为Stream，一个数据流，定义为节点和节点的端口之间传递的一组有序的Buffer，这个定义是理解ModelBox中数据流的关键基础。 在此定义下的一个典型数据流处理实例：URL送入demuxer，产生packet数据流；送入decoder，产生视频流；经过resize，产生新的视频流；经过人脸检测，得到人像框流。这个过程中，每个数据流都只存在于节点和节点的端口之间。 Stream特点 数据流在传递中通过有序的Buffer进行承载，进行流处理时，会顺序处理每个Buffer。数据流通过特殊操作后会产生继承层级，如经过expand、condition后，数据流的继承层级会下降一层，这样做是为了显示标记数据流，避免不同层级的数据进行匹配操作，经过collapse、condition的单端口合并之后，数据流的继承层级会回退一层。可以类比为表达式中的左右括号，不同类型的括号同样不可以交叉。 Stream构成 Stream由数据部分和元信息部分构成，数据部分由多个有序的Buffer承载，元信息则记录在数据流上，数据流的产生功能单元在第一个数据产生前可以写入流的元信息，数据流处理功能单元只能读取数据流的元信息。 数据流的处理 下文介绍数据流经过不同功能单元的处理。 数据流的匹配 上图介绍了一种典型的数据流匹配场景，Image Source功能单元功能是产生图片，Inference功能单元的功能是输入图片输出图片中包含人的框，DrawBox功能单元的功能是接受一个输入图像，和输入检测框，输出一个绘制了检测框的图像。对DrawBox来说图像和检测框结果之间就存在匹配关系，两个数据流均来自ImageSource，且两个数据流中的Buffer数量一致，则可以正确完成匹配的过程。 数据流经过Normal功能单元 两个数据流进入节点处理，功能单元的batch设置为2，可以看到数据被划分为最多两个一组，多组数据同时开始处理，生成的结果数据与输入一致，并且整理成独立的输出流输出。 数据流经过Stream功能单元 两个数据流进入节点处理，功能单元的batch设置为2，可以看到数据被划分为最多两个一组，不同的数据流同时处理，同一个数据流的多组batch串行处理，生成的结果数据与输入数量可以不一致。 数据流经过Condition功能单元 单个数据流进入节点处理，功能单元一次处理一个Buffer，多个Buffer同时处理，每个输出Buffer选择一个端口，输出的总量与单端口输入的数量一致。 数据流经过Expand功能单元 单个数据流进入节点处理，功能单元一次处理一个Buffer，多个Buffer同时处理，每个Buffer展开成一个子数据流。 数据流经过Collapse功能单元 多个子数据流进入节点处理，功能单元一次处理一个子流，多个子流的数据同时处理，每个子流只能合并输出一个Buffer。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"basic-conception/buffer.html":{"url":"basic-conception/buffer.html","title":"Buffer","keywords":"","body":"Buffer Buffer作为数据流的唯一载体，在节点的端口之间进行数据的传递，因此功能单元的输入输出均需要与Buffer产生交互。 Buffer定义 Buffer由Meta和Data两个部分组成，Meta存放了每个Buffer的元信息，用于描述Buffer的数据，Meta只存放在主机内存上；Data是Buffer的主体，用于存放数据，Data是具体设备上申请的存储空间，可以是GPU、CPU等硬件上的存储空间。因此功能单元的输入端口可以指定输入Buffer所在的设备，框架会帮忙检查并确保数据存放在指定的设备上。 输入Buffer 功能单元获取输入Buffer时，首先会获取到BufferList，包含了一个batch的数据，BufferList与vector行为一致。获取到输入Buffer后，其数据部分是不允许修改的，只能通过ConstData来获取数据指针，因为一个Buffer被设计为，在产生时写入，后续流程只允许读取。 输出Buffer 输出的Buffer可以来自两种方式，一是将输入直接推到输出；一是在功能单元中构建。对于构建的方式时，功能单元只能构建出功能单元声明的依赖设备上的内存，不能随意选择Buffer所在的设备。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/api.html":{"url":"api/api.html","title":"API","keywords":"","body":"API ModelBox 针对不同模块的开发提供了C++，Python, Java多种编程语言API，具体API的支持语言如下： 模块 说明 C++ Python Java ModelBox FlowUnit API ModelBox功能单元开发API。 ✔️ ✔️ ❌ ModelBox SDK API ModelBox SDK模式开发集成API。 ✔️ ✔️ ✔️ ModelBox Plugin API ModelBox插件开发API。 ✔️ ❌ ❌ ModelBox Device API ModelBox多设备开发API。 ✔️ ❌ ❌ ModelBox Base API ModelBox基础工具API，如日志、配置等等。 ✔️ ✔️(部分支持) ✔️(部分支持) ModelBox各组件整体对外提供的API接口如下： ModelBox FlowUnit API ModelBox功能单元开发接口主要用于完成功能单元业务逻辑，相关接口如下： Buffer: 数据接口，用于承载AI推理数据。 FlowUnit: 功能单元接口，用于扩展新的功能单元组件。 TensorList: Buffer操作接口，支持使用Tensor相关的接口操作Buffer。 DataContext: 支持功能单元开发的接口，用于获取功能单元的输入，输出，和Stream流上下文的储。 SessionContext: 会话上下文，用于存储和会话相关的信息。 ModelBox SDK API ModelBox SDK模式开发接口主要用于完成对流程图SDK启动方式的集成，相关接口如下： Flow: 流程图操作接口，用于加载、启动、停止流程图。 ExternalDataMap: 外部数据与流程图交互接口，用于发送数据到流程图输入和接受处理结果。 ModelBox Plugin API ModelBox插件开发接口主要用于完成自定义插件的扩展，相关接口如下： Plugin: 插件重载接口，用于实现自定义插件。 Job: 任务管理，可以添加，删除，查询图以及对应的任务。 Listener: HTTP Server，可以注册HTTP请求的URL事件。 ModelBox Device API ModelBox多设备开发接口主要完成对不同硬件设备的功能单元，相关接口如下： CudaFlowUnit: Cuda类型功能单元接口。 AscendFlowUnit: Ascend类型功能单元接口。 ModelBox Base API ModelBox基础工具接口，包含了各种支撑业务运行的常用组件，可以在各个开发阶段使用，包括如下件： BlockingQueue，阻塞队列。 Config，图配置读取。 Crypto，数据加解密。 Status，错误返回接口。 Utils，工具函数。 Device， 设备抽象接口。 Timer，定时器组件。 ThreadPool，线程池组件。 Log，日志组件。 Slab，Slab内存缓存组件。 Driver，ModelBox插件接口。 Statistics，统计接口，用于统计，获取相关组件的统计信息。 注意：Python仅包含Log, Status, Config组件接口。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++.html":{"url":"api/c++.html","title":"C++","keywords":"","body":"C++ API 在ModelBox服务启动，并开启Editor编辑器后，可直接使用http://[host]:[port]]/api/访问查看全量详细API。 本章节重点介绍常用的C++ API，如下表所示： 组件名 功能 Buffer ModelBox存储基本数据的数据结构 BufferList ModelBox存储基本数据的数据结构 Configuration ModelBox存放配置项的数据结构 DataContext ModelBox功能单元当中储存数据的数据结构 DataMeta ModelBox挂在端口上面存放数据元信息的数据结构 DriverDesc ModelBox描述插件的描述信息的数据结构 ExternalDataMap ModelBox描述流程图对象输入数据的数据结构 Flow modelbox描述流程图的数据结构 FlowUnit ModelBoxg功能单元单元 FlowUnitDesc ModelBox描述功能单元描述信息的数据结构 FlowUnitEvent ModelBox中描述event的数据结构 FlowUnitPort ModelBox描述功能单元输入输出端口的数据结构 SessionContext ModelBox描述会话上下文的数据结构 Solution ModelBox描述解决方案的数据结构 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_buffer.html":{"url":"api/c++/modelbox_buffer.html","title":"Buffer","keywords":"","body":"modelbox::Buffer 函数 作用 构造方法 Buffer的构造方法 Build Buffer申请特定大小的内存，内存来源于device侧 BuildFromHost Buffer申请特定大小的内存，内存来源于host侧 ConstData 获取当前Buffer的数据的常量指针 MutableData 获取当前Buffer的数据的指针 HasError 判断Buffer是否存在error SetError 给Buffer设置error GetErrorCode 获取error_code GetErrorMsg 获取error_msg GetBytes 获取当前Bufferr的字节数 Get 获取指定key的Meta值 Set 设置指定key的Meta值 CopyMeta 拷贝指定Buffer的所有Meta值给新Buffer 构造方法 构造新的Buffer。 Buffer(); Buffer(const std::shared_ptr& device, uint32_t dev_mem_flags = 0); args: device (modelbox::Device) —— 构造当前Buffer所在的modelbox::Device对象 dev_mem_flags (uint32_t) —— 内存类型，ascend memory的类型时有效。 return: 构造出来的新的Buffer。 note: dev_mem_flags改参数只在ascend的内存上面有区分，0为普通内存类型，1位dvpp内存，其他设备不需要区分 example: #include Status Process(std::shared_ptr data_ctx) { auto buffer = std::make_shared(GetBindDevice()); } result: 构建出了和当前功能单元在同一个设备上面的Buffer对象 Build 通过Buffer对象构建特定大小的内存。 Status Build(size_t size); //data在device侧 Status Build(void* data, size_t data_size, DeleteFunction func = nullptr); args: size (size_t) —— Buffer申请的内存字节大小 data (void*) 构建给Buffer管理的数据指针 data_size (size_t) —— Buffer申请的内存字节大小 func (DeleteFunction ) —— 释放Buffer指针的函数 BuildFromHost // data在host侧 Status BuildFromHost(void* data, size_t data_size, DeleteFunction func = nullptr); args: data (void*) 构建给Buffer管理的数据指针 data_size (size_t) —— Buffer申请的内存字节大小 func (DeleteFunction ) —— 释放Buffer指针的函数 return: modelbox::Status, 构建Buffer的返回的status状态 example: #include Status Process(std::shared_ptr data_ctx) { auto buffer1 = std::make_shared(GetBindDevice()); auto buffer2 = std::make_shared(GetBindDevice()); auto buffer3 = std::make_shared(GetBindDevice()); Status status{STATUS_OK}; status = buffer1->Build(1); auto *data = buffer1->MutableData(); data[0] = 122; auto device_data = std::make_shared(123); status = buffer2->Build(device_data.get(), 1); auto host_data = std::make_shared(124); status = buffer3->Build(host_data.get(), 1, [](void *){}); } result: 构建成功了3个Buffer，通过不同的方式申请内存赋值 ConstData 获取Buffer的原始数据的常量指针 args: 无 return: const void*, 原始数据的常量指针 MutableData 获取Buffer的原始数据的指针 args: 无 return: void*, 原始数据的指针 example: Status Process(std::shared_ptr data_ctx) { //假设该功能单元为1输入1输出，输入端口名为input，输出端口名为output auto input_bufs = data->Input(\"input\"); auto output_bufs = data->Output(\"output\"); for (auto i = 0; i Size(); ++i) { // 指定位置的Buffer auto buffer = input_bufs->At(i); // 数据原始指针 const void* data1 = buffer->ConstData(); } ... // output_bufs已经构建好了内存以及数据 for (auto i = 0; i Size(); ++i) { // 指定位置的Buffer auto buffer = output_bufs->At(i); void* data2 = buffer->MutableData(); } } result: data1和data2为获取Buffer的数据指针 note: input的BufferList当中获取数据用ConstData()接口，output的BufferList当中获取数据使用MutableData()接口 功能单元当中的输入数据是为不可变，输出数据需要构造赋值，为可变。 HasError 判断当前Buffer是否存在处理异常 bool HasError() const; args: 无 return: bool, 当前Buffer是否存在error SetError 设置当前Buffer处理异常信息 void SetError(const std::string& error_code, const std::string& error_msg); args: error_code (string) —— error的错误码 error_msg (string) —— error的错误信息 return: 无 GetErrorCode 获取当前Buffer异常信息的错误码 std::string GetErrorCode() const; args: 无 return: string, 当前Buffer的错误码 GetErrorMsg 获取当前Buffer异常信息的错误信息 std::string GetErrorMsg() const; args: 无 return: string, 当前Buffer的错误信息 example: Status Process(std::shared_ptr data_ctx) { //假设该功能单元为1输入1输出，输入端口名为input，输出端口名为output ... auto output_bufs = data->Output(\"output\"); output_bufs->Build({1,1,1}); for (auto i = 0; i Size(); ++i) { auto buffer = output_bufs->At(i); bool error = true; if (error) { buffer->SetError(\"Sample.FAILED_001\", \"test error\"); } } } Status Process(std::shared_ptr data_ctx) { //假设该功能单元为1输入1输出，输入端口名为input，输出端口名为output auto input_bufs = data->Input(\"input\"); auto output_bufs = data->Output(\"output\"); for (auto i = 0; i Size(); ++i) { auto buffer = input_bufs->At(i); if (buffer->HasError()) { auto error_code = buffer->GetErrorCode(); auto error_msg = buffer->GetErrorMsg(); } } } result: 可以在报错时给Buffer设置error，然后在其他功能单元里面补充获取存在Bufferr的error_code和error_msg GetBytes 获取当前Buffer的字节数 size_t GetBytes() const; args: 无 return: int64, Buffer的字节数 example: Status Process(std::shared_ptr data_ctx) { //假设该功能单元为1输入1输出，输入端口名为input，输出端口名为output ... auto output_bufs = data->Output(\"output\"); output_bufs->Build({1,1,1}); for (auto i = 0; i Size(); ++i) { auto buffer = output_bufs->At(i); auto bytes = buffer->GetBytes(); } } result: 获取该Buffer的字节数 Set 设置当前Buffer的某个Meta值 template void Set(const std::string& key, T&& value) args: key (str) —— Meta的key值 value (template) —— Meta的value值 return: 无 Get 获取当前Buffer的某个Meta值 template bool Get(const std::string& key, T&& value) args: key (str) —— Meta的key值 value (template) —— 接收Meta的value值 return: bool, 是否获取Meta值成功 example: Status Process(std::shared_ptr data_ctx) { // 获取Buffer中的指定Meta值 auto input_bufs = data_ctx->Input(\"input\"); auto buffer = input_bufs->At(0); // 获取Buffer Meta当中的shape信息，res为get的结果 std::vector shape; auto res = buffer->Get(\"shape\", shape); if (!res) { return STATUS_FAULT; } // 更新shape值后重新，设置到out_buffer的Meta当中 auto output_bufs = data_ctx->Output(\"output\"); shape.push_back(1); auto out_buffer = output_bufs->At(0); out_buffer->Set(\"shape\", shape); } CopyMeta 把参数的Buffer Meta信息拷贝给当前Buffer Status CopyMeta(const std::shared_ptr buf, bool is_override = false) args: buf (modelbox::Buffer) —— Meta来源的Buffer is_override (bool) —— Meta是否需要覆盖 return: modelbox::Status 返回CopyMeta接口Status example: Status Process(std::shared_ptr data_ctx) { // 获取Buffer中的指定Meta值 auto input_bufs = data_ctx->Input(\"input\"); auto output_bufs = data_ctx->Output(\"output\"); auto buffer = input_bufs->At(0); output_bufs->Build({1}); auto new_buffer = output_bufs->At(0); new_buffer->CopyMeta(buffer); } result: new_buffer具有和原始Buffer相同的Meta信息 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_bufferlist.html":{"url":"api/c++/modelbox_bufferlist.html","title":"BufferList","keywords":"","body":"modelbox::BufferList 函数 作用 构造方法 BufferList的构造方法 Build BufferList对象申请指定长度大小的内存空间 BuildFromHost BufferList对象申请指定长度大小的内存空间 Size 获取当前BufferList的长度 GetBytes 获取当前BufferList的字节大小 At 获取BufferList当中特定位置的Buffer 遍历 遍历BufferList EmplaceBack 把当前设备的数据塞入BufferList当中 EmplaceBackFromHost 把当前设备的数据塞入BufferList当中 ConstBufferData 返回特定位置的Buffer的数据常量指针 MutableBufferData 返回特定位置的Buffer的数据指针 CopyMeta 把参数的BufferList的Meta信息，copy给当前BufferList Set 设置当前BufferList的某个Meta值 PushBack 将一个Buffer插入到当前的BufferList当中 构造方法 构造BufferList对象 BufferList(); BufferList(const std::shared_ptr& device, uint32_t device_mem_flags = 0); args: device (modelbox::Device) —— 构建BufferList的device对象 device_mem_flags (uint32_t) —— device的内存的flag值，只有ascend才会使用，flag=1指的是dvpp的内存 BufferList(const std::shared_ptr& buffer); BufferList(const std::vector>& buffer_vector); args: buffer (modelbox::Buffer) —— 单Buffer构造BufferList buffer_vector (vector) —— vector的Buffer构建BufferList Build BufferList申请Buffer内存空间 Status Build(const std::vector& data_size_list, bool contiguous = true) args: data_size_list (vector) —— BufferList当中每一个Buffer的size大小 contiguous (bool) —— BufferList当中每个Buffer的数据是否连续，默认连续 return: modelbox.Status 申请结果的状态 example: #include Status Process(std::shared_ptr data_ctx) { auto output_bufs = data_ctx->Output(\"output\"); auto status = output_bufs->Build({1,1,1}); MBLOG_INFO Size(); for (auto &buffer: output_bufs) { MBLOG_INFO GetBytes(); } } result: 3 111 BuildFromHost BufferList从主机内存上申请内存空间 Status BuildFromHost(const std::vector& data_size_list, void* data, size_t data_size, DeleteFunction func = nullptr); args: data_size_list (vector) —— BufferList当中每一个Buffer的size大小 data (void*) —— 需要交给BufferList的管理的host的数据指针 data_size (size_t) —— 数据的大小 func (std::function) —— 析构的函数 return: modelbox.Status 申请结果的状态 example: #include Status Process(std::shared_ptr data_ctx) { auto output_bufs = data_ctx->Output(\"output\"); vector data{122,123,124} auto status = output_bufs->BuildFromHost({1,1,1}, data.data(), 3); MBLOG_INFO Size(); for (auto &buffer: output_bufs) { MBLOG_INFO GetBytes() MutableData()); } } result: 3 1, 122 1, 123 1, 124 Size 获取BufferList的Buffer个数 args: 无 return: size_t GetBytes 获取BufferList字节大小 args: 无 return: size_t At 获取指定位置的Buffer args: pos (size_t) 指定位置 return: 返回指定位置的shared_ptr 遍历 遍历BufferList example: #include Status Process(std::shared_ptr data_ctx) { auto output_bufs = data_ctx->Output(\"output\"); vector data{122,123,124} auto status = output_bufs->BuildFromHost({1,1,1}, data.data(), 3); MBLOG_INFO Size(); for (auto &buffer: output_bufs) { MBLOG_INFO GetBytes() MutableData()); } auto buffer1 = output_bufs->At(1); MBLOG_INFO MutableData()); } result: 3 1, 122 1, 123 1, 124 123 EmplaceBack 把当前设备的数据转换成Buffer并插入BufferList当中 Status EmplaceBack(void* device_data, size_t data_size, DeleteFunction func = nullptr); Status EmplaceBack(std::shared_ptr device_data, size_t data_size); args: device_data (void*/std::shared_ptr) —— 需要交给BufferList的管理的device的数据指针 data_size (size_t) —— 数据的大小 func (std::function) —— 析构的函数 return: modelbox.Status 插入到BufferList的状态 EmplaceBackFromHost 把主机内存数据转换成Buffer并插入BufferList当中 Status EmplaceBackFromHost(void* host_data, size_t data_size); args: host_data (void*) —— 需要交给BufferList的管理的host的数据指针 data_size (size_t) —— 数据的大小 return: modelbox.Status 插入到BufferList的状态 example: #include Status Process(std::shared_ptr data_ctx) { // 该flowunit为1输入1输出，端口号名字分别为input, output ... auto device_ready_data1 = std::make_shared(123); auto device_ready_data2 = std::make_shared(124); auto host_ready_data1 = std::make_shared(125); auto output_bufs = data_ctx->Output(\"output\"); //析构时不做任何操作，释放的时间交给shared_ptr控制, 数据指针在device侧 output_bufs->EmplaceBack(device_ready_data1.get(), 1, [](void*){}) //智能指针交给buffer管理， 数据指针在device侧 output_bufs->EmplaceBack(device_ready_data2, 1); //数据指针在host侧 output_bufs->EmplaceBackFromHost(host_ready_data1.get(), 1); return STATUS_OK; } result: output_bufs插入了3个Buffer ConstBufferData 获取指定Buffer的数据指针，且只读 const void* ConstBufferData(size_t idx) const; args: idx (size_t*) —— 第idx数据 return: 第idx Buffer的数据常量指针 MutableBufferData 获取指定Buffer的数据指针，且可修改 void* MutableBufferData(size_t idx); args: idx (size_t*) —— 第idx数据 return: 第idx Buffer的数据指针 example: #include Status Process(std::shared_ptr data_ctx) { auto input_bufs = data->Input(\"input\"); for (auto i = 0; i Size(); ++i) { // 通过BufferList访问特定位置的数据指针，data1和data2是同一个指针 void* data2 = input_bufs->ConstBufferData(i); ... } } CopyMeta 把参数的BufferList的Meta信息拷贝给当前BufferList Status CopyMeta(const std::shared_ptr& bufferlist, bool is_override = false); args: bufferlist (modelbox::BufferList) —— Meta来源的BufferList is_override (bool) —— Meta是否需要覆盖 return: modelbox::Status 返回CopyMeta接口Status example: Status Process(std::shared_ptr data_ctx) { // 获取Buffer中的指定Meta值 auto input_bufs = data_ctx->Input(\"input\"); auto output_bufs = data_ctx->Output(\"output\"); output_bufs->Build({1}); output_bufs->CopyMeta(input_bufs); } result: output_bufs具有和原始input_bufs相同的Meta信息 Set 给BufferList当中每一个Buffer都设置Meta值 template void Set(const std::string& key, T&& value) args: key (str) —— Meta的key值 value (template) —— Meta的value值 return: 无 example: Status Process(std::shared_ptr data_ctx) { // 获取Buffer中的指定Meta值 auto input_bufs = data_ctx->Input(\"input\"); auto output_bufs = data_ctx->Output(\"output\"); output_bufs->Build({1}); vector shape{28,28}; output_bufs->Set(\"shape\", shape); } result: output_bufs当中的每一个Buffer都一个Meta值为shape PushBack 往BufferList当中插入一个Buffer void PushBack(const std::shared_ptr& buf) args: buf (modelbox::Buffer) —— 需要插入到BufferList当中的Buffer return: 无 example: Status Process(std::shared_ptr data_ctx) { // 获取Buffer中的指定Meta值 auto input_bufs = data_ctx->Input(\"input\"); auto output_bufs = data_ctx->Output(\"output\"); auto buffer = Buffer(GetBindDevice()); output_bufs->PushBack(buffer); } result: output_bufs当中push了一个Buffer到数组内 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_configuration.html":{"url":"api/c++/modelbox_configuration.html","title":"Configuration","keywords":"","body":"modelbox::Configuration 函数 作用 Get 从configuration对象当中获取值 Size configuration对象中存储键值对的数量 GetKeys configuration对象中获取所有的key值 Contain configuration对象中是否包含某个key值 Get std::string GetString(const std::string &key, const std::string &default_prop = \"\") const; bool GetBool(const std::string &key, bool default_prop = false) const; int8_t GetInt8(const std::string &key, int8_t default_prop = 0) const; uint8_t GetUint8(const std::string &key, uint8_t default_prop = 0) const; int16_t GetInt16(const std::string &key, int16_t default_prop = 0) const; uint16_t GetUint16(const std::string &key, uint16_t default_prop = 0) const; int32_t GetInt32(const std::string &key, int32_t default_prop = 0) const; uint32_t GetUint32(const std::string &key, uint32_t default_prop = 0) const; int64_t GetInt64(const std::string &key, int64_t default_prop = 0) const; uint64_t GetUint64(const std::string &key, uint64_t default_prop = 0) const; float GetFloat(const std::string &key, float default_prop = 0.0f) const; double GetDouble(const std::string &key, double default_prop = 0.0) const; std::vector GetStrings( const std::string &key, const std::vector &default_prop = {}) const; std::vector GetBools(const std::string &key, const std::vector &default_prop = {}) const; std::vector GetInt8s( const std::string &key, const std::vector &default_prop = {}) const; std::vector GetUint8s( const std::string &key, const std::vector &default_prop = {}) const; std::vector GetInt16s( const std::string &key, const std::vector &default_prop = {}) const; std::vector GetUint16s( const std::string &key, const std::vector &default_prop = {}) const; std::vector GetInt32s( const std::string &key, const std::vector &default_prop = {}) const; std::vector GetUint32s( const std::string &key, const std::vector &default_prop = {}) const; std::vector GetInt64s( const std::string &key, const std::vector &default_prop = {}) const; std::vector GetUint64s( const std::string &key, const std::vector &default_prop = {}) const; std::vector GetFloats( const std::string &key, const std::vector &default_prop = {}) const; std::vector GetDoubles( const std::string &key, const std::vector &default_prop = {}) const; args: key (string) —— 需要获取的Meta的key值 default_prop (特定类型) —— 获取key值失败时的默认值 return: 特定类型的值 example: ... Status Open(const std::shared_ptr config): auto str = config->GetString(\"string\", \"default\"); auto strings = config->GetStrings(\"strings\", {\"test\", \"test\"}); auto int32 = config->GetInt32(\"int32\", 1); auto int32s = config->GetInt32s(\"int32s\", {1,2,3}); ... MBLOG_INFO result: default test test1 12 3 Size size_t Size() const; args: 无 return: size_t, configuration item数量 GetKeys set GetKeys() const; args: 无 return: set, configuration 所有的key值 Contain bool Contain(const std::string &key) const; args: key (string) 需要判断的key值 return: bool, 该key值是否存在 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_datacontext.html":{"url":"api/c++/modelbox_datacontext.html","title":"DataContext","keywords":"","body":"modelbox::DataContext 每一个功能单元当中存放数据上下文的所有接口 函数 作用 Input 获取功能单元输入端口数据 Output 获取功能单元输出端口数据 External HasError datacontext当中是否存在error SendEvent 发送event SetPrivate 设置datacontext中的私有Meta值 GetPrivate 获取datacontext中的私有Meta值 GetInputMeta 获取datacontext当中绑定在input端口的值 SetOutputMeta 设置datacontext当中绑定在output端口的值 GetSessionConfig 获取datacontext当中的session config GetSessionContext 获取datacontext当中的session_context GetStatistics 获取statistics对象 Input 获取功能单元输入端口对象 std::shared_ptr Input() const; std::shared_ptr Input(const std::string &port); Output 获取功能单元输出端口数据 std::shared_ptr Output() const; std::shared_ptr Output(const std::string &port); args: port (string) —— 输入端口名 return: modelbox::BufferListMap (map) —— 按照input port name存放BufferList的map modelbox::BufferList (modelbox::BufferList) —— 存放对应input port的BufferList数据 example Status Process(std::shared_ptr data_ctx) { // 1个input，name为“input”; 1个output， name为“output” auto inputs = data_ctx->Input(); auto outputs = data_ctx->Output(); auto input = data_ctx->Input(\"input\"); auto output = data_ctx->Output(\"output\"); } External 获取datacontext绑定的从input端点传入的BufferList std::shared_ptr External(); args: 无 return: modelbox::BufferList example: Status Process(std::shared_ptr data_ctx) { // 1个input，name为“input”; 1个output， name为“output” auto input = data_ctx->External(); } SendEvent 从当前数据上下文发送event给调度器可重新调度当前功能单元 void SendEvent(std::shared_ptr event); args: modelbox::FlowUnitEvent 可以参考FlowUnitEvent的说明和接口 return: 无 example: Status Process(std::shared_ptr data_ctx) { auto event = std::make_shared(); data_ctx->SendEvent(event); } result: 具体接口参见FlowUnitEvent HasError 判断当前datacontext是否存在error bool HasError(); args: 无 return: bool， 是否存在error example: Status Process(std::shared_ptr data_ctx) { auto res = data_ctx->HasError(); } SetPrivate 设置当前flowunit中保存在DataContext中的对象 void SetPrivate(const std::string &key, std::shared_ptr private_content); args: key (string) —— 设置对象的key值 private_content (shared_ptr) —— 设置对象的val的智能指针 return: 无 GetPrivate 获取当前flowunit中保存在DataContext中的对象 std::shared_ptr GetPrivate(const std::string &key); args: key (str) —— 需要获取对象的key值 return: shared_ptr key值对应的value的智能指正 example: Status DataPre(std::shared_ptr data_ctx) { auto index_counter = std::make_shared(0); data_ctx->SetPrivate(\"index\", index_counter); } Status Process(std::shared_ptr data_ctx) { auto index_counter = data_ctx->GetPrivate(\"index\"); MBLOG_INFO result: 0 GetInputMeta flowunit获取input端口的数据 const std::shared_ptr GetInputMeta( const std::string &port); args: port (string) —— input端口的名称 return: modelbox::DataMeta 端口的数据 SetOutputMeta flowunit获取input端口的数据 void SetOutputMeta(const std::string &port, std::shared_ptr data_meta); args: port (string) —— output端口的名称 data_meta (modelbox::DataMeta) —— 端口的数据，具体参见DataMeta return: 无 example: Status Process(std::shared_ptr data_ctx) { auto input_meta = data_ctx->GetInputMeta(\"input\") data_ctx->SetOutputMeta(\"output\", input_meta) } result: 获取input端口的meta和直接将该meta设置给output, modelbox::DataMeta参照data meta的接口 GetSessionConfig 获取当前数据上下文的SessionConfig std::shared_ptr GetSessionConfig(); args: 无 return: modelbox::Configuration GetSessionContext 获取当前数据上下文的SessionContext std::shared_ptr GetSessionContext(); args: 无 return: modelbox::SessionContext GetStatistics 获取当前数据上下文的Statistics std::shared_ptr GetStatistics( DataContextStatsType type = DataContextStatsType::NODE); args: modelbox::DataContextStatsType, statistics的数据级别，具体参见StatisticsItem api example: ... Status Process(std::shared_ptr data_ctx) { auto session_config = data_ctx->GetSessionConfig(); auto session_context = data_ctx->GetSessionContext(); auto statistics = data_ctx->GetStatistics(); ... return STATUS_OK; } result: 具体接口参见session_context、statistics接口 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_datameta.html":{"url":"api/c++/modelbox_datameta.html","title":"DataMeta","keywords":"","body":"modelbox::DataMeta 函数 作用 SetMeta 设置DataMeta对象值 GetMeta 获取DataMeta特定key的数据 SetMeta 设置DataMeta对象值 args: key (string) —— 设置对象的key值 meta (shared_ptr) —— 设置对象的value的智能指针 return: 无 GetMeta 获取DataMeta特定key的数据 args: key (str) —— 需要获取对象的key值 return: shared_ptr 获取当前key值value值 example: ... Status Process(std::shared_ptr data_ctx) { auto data_meta = std::make_shared() data_meta->SetMeta(\"test\", \"test\"); MBLOG_INFO GetMeta(\"test\"); ... return STATUS_OK; } result: \"test\" ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_driverdesc.html":{"url":"api/c++/modelbox_driverdesc.html","title":"DriverDesc","keywords":"","body":"modelbox::DriverDesc 函数 作用 SetName 设置插件名字 SetClass 设置插件类别 SetType 设置插件类型 SetDescription 设置插件描述 SetVersion 设置插件版本 SetFilePath 设置文件路径 SetNodelete 设置插件为不需要删除 SetGlobal 设置插件符号为全局可见 SetDeepBind 设置插件符号解析为优先使用当前插件 SetName 设置Driver名称 void SetName(const std::string &name); args: name (string) —— 插件的名字 return: 无 SetClass 设置Driver类型 void SetClass(const std::string &classname); args: classname (string) —— 插件的类型，例如：DRIVER-FLOWUNIT, DRIVER-DEVICE等等, 具体可以参考其他功能单元的实现 return: 无 SetType 设置Driver设备类型 void SetType(const std::string &type); args: type (string) —— 插件的设备类型，例如：cpu, cuda, ascend return: 无 SetDescription void SetDescription(const std::string &description); args: description (string) —— 插件的详细描述 return: 无 SetVersion Status SetVersion(const std::string &version); args: version (string) —— 插件的版本,为x.y.z的形式 return: modelbox::Status, 设置是否成功, 非必须， 默认为空 SetFilePath void SetFilePath(const std::string &file_path); args: file_path (string) —— 插件的具体位置 return: 无, 非必须 SetNodelete void SetNodelete(const bool &no_delete); args: no_delete (bool) —— 是否为RTLD_NODELETE return: 无, 非必须，默认为false SetGlobal void SetGlobal(const bool &global); args: global (bool) —— 是否为RTLD_GLOBAL return: 无, 非必须，默认为false SetDeepBind void SetDeepBind(const bool &deep_bind); args: deep_bind (bool) —— 是否为RTLD_DEEPBIND return: 无, 非必须，默认为false example: // xxx_flowunit.cc MODELBOX_DRIVER_FLOWUNIT(desc) { desc.Desc.SetName(FLOWUNIT_NAME); desc.Desc.SetClass(modelbox::DRIVER_CLASS_FLOWUNIT); desc.Desc.SetType(FLOWUNIT_TYPE); desc.Desc.SetDescription(FLOWUNIT_DESC); desc.Desc.SetVersion(\"1.0.0\"); } // or flowunit_desc.cc void DriverDescription(modelbox::DriverDesc *desc) { desc->SetName(FLOWUNIT_NAME); desc->SetClass(modelbox::DRIVER_CLASS_FLOWUNIT); // \"DRIVER-FLOWUNIT\" desc->SetType(modelbox::DEVICE_TYPE); // \"cpu\" desc->SetDescription(FLOWUNIT_DESC); desc->SetNodelete(true); desc->SetGlobal(true); return; } result: 设置了Driver属性值 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_externaldatamap.html":{"url":"api/c++/modelbox_externaldatamap.html","title":"ExternalDataMap","keywords":"","body":"modelbox::ExternalDataMap 函数 作用 CreateBufferList 创建BufferList Send 发送BufferList给流程图的输入 Recv 接收当前流程图处理结果的BufferList Close 关闭当前ExternalDataMap对象，等待数据完全处理完成 Shutdown 强制关闭当前ExternalDataMap对象，无论数据是否处理完成 SetOutputMeta 设置当前对象输入端口的DataMeta值 CreateBufferList 创建BufferList std::shared_ptr CreateBufferList() args: 无 return: modelbox::BufferList 创建存储数据的BufferList Send 发送BufferList给流程图的输入 Status Send(const std::string& port_name, std::shared_ptr buffer_list) args: port_name (string) —— 发送数据的目标端口名字 buffer_list (modelbox::BufferList) —— 发送的数据 return: modelbox::Status 发送数据的状态 Recv 接收当前流程图处理结果的BufferList using OutputBufferList = std::unordered_map>; Status Recv(OutputBufferList& map_buffer_list, int32_t timeout = 0) args: map_buffer_list (OutputBufferList) —— 接收数据的数据结构对象 timeout (int32_t) —— 超时的时间 return: modelbox::Status 接收给当前flow对象的BufferList的状态 Close 关闭当前ExternalDataMap对象，等待数据完全处理完成 Status Close() args: 无 return: modelbox::Status 关闭当前ExternalDataMap对象的状态 Shutdown 强制关闭当前ExternalDataMap对象 Status Shutdown() args: 无 return: modelbox::Status 强制关闭当前ExternalDataMap对象的状态 SetOutputMeta 设置当前对象输入端口的DataMeta值 Status SetOutputMeta(const std::string& port_name, std::shared_ptr meta) args: port_name (string) —— 设置meta的目标端口名字 meta (modelbox::DataMeta) —— 设置的数据meta return: modelbox::Status 设置当前对象输入端口的DataMeta的状态 example: #include int main() { auto flow = std::make_shared(); auto external_map = flow->CreateExternalDataMap(); auto buffer_list = external_map->CreateBufferList(); auto data_meta = std::make_shared() data_meta->SetMeta(\"key\", \"value\"); external_map->SetOutputMeta(\"input1\", data_meta); // build buffer ... auto status = external_map->Send(\"input1\", buffer_list); extern_data->Close(); OutputBufferList map_buffer_list; status = extern_data->Recv(map_buffer_list); // or shutdown extern_data->ShutDown(); return 0; } ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_flowunit.html":{"url":"api/c++/modelbox_flowunit.html","title":"FlowUnit","keywords":"","body":"modelbox::FlowUnit 函数 作用 Open 功能单元初始化逻辑 Process 功能单元处理逻辑 Close 功能单元关闭逻辑 DataPre Stream流初始化逻辑 DataPost Stream流结束逻辑 GetBindDevice 获取功能单元绑定的设备 Open 功能单元初始化逻辑 args: config (modelbox::Configuration) —— 流程图当中配给当前功能单元的配置项 return: modelbox::Status 初始化功能单元的返回状态 Process 功能单元处理逻辑 args: data_context (modelbox::DataContext) —— 处理逻辑当中存放数据的上下文 return: modelbox::Status 功能单元处理逻辑的返回状态 Close 功能单元结束逻辑 args: 无 return: modelbox::Status 功能单元结束逻辑的返回状态 DataPre Stream流初始化逻辑 args: data_context (modelbox::DataContext) —— 初始化Stream逻辑当中存放数据的上下文 return: modelbox::Status 初始化Stream时的逻辑的返回状态 DataPost Stream流结束逻辑 args: data_context (modelbox::DataContext) —— 结束Stream逻辑当中存放数据的上下文 return: modelbox::Status 结束Stream逻辑的返回状态 /** * @brief Flowunit open function, called when unit is open for processing data * @param config flowunit configuration * @return open result */ Status Open(const std::shared_ptr &config); /** * @brief Flowunit close function, called when unit is closed. * @return open result */ Status Close(); /** * @brief Flowunit data process. * @param data_ctx data context. * @return open result */ Status Process(std::shared_ptr data_ctx); /** * @brief Flowunit data pre. * @param data_ctx data context. * @return data pre result */ Status DataPre(std::shared_ptr data_ctx); /** * @brief Flowunit data post. * @param data_ctx data context. * @return data post result */ Status DataPost(std::shared_ptr data_ctx); example: ... # 典型flowunit场景 modelbox::Status VideoDecoderFlowUnit::Open( const std::shared_ptr &opts) { out_pix_fmt_str_ = opts->GetString(\"pix_fmt\", \"nv12\"); ... return modelbox::STATUS_OK; } modelbox::Status VideoDecoderFlowUnit::Close() { return modelbox::STATUS_OK; } modelbox::Status VideoDecoderFlowUnit::Process( std::shared_ptr ctx) { auto input_bufs = ctx->Input(\"in_video_packet\"); auto output_bufs = ctx->Output(\"out_video_frame\"); ... return modelbox::STATUS_CONTINUE; } modelbox::Status VideoDecoderFlowUnit::DataPre( std::shared_ptr data_ctx) { MBLOG_INFO GetInputMeta(VIDEO_PACKET_INPUT); ... return modelbox::STATUS_OK; } modelbox::Status VideoDecoderFlowUnit::DataPost( std::shared_ptr data_ctx) { return modelbox::STATUS_OK; } GetBindDevice 获取当前功能单元绑定设备 args: 无 return: modelbox::Device, 当前绑定设备 example: ... modelbox::Status VideoDecoderFlowUnit::Process( std::shared_ptr ctx) { auto device = GetBindDevice(); ... return modelbox::STATUS_OK; } ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_flowunitevent.html":{"url":"api/c++/modelbox_flowunitevent.html","title":"FlowUnitEvent","keywords":"","body":"modelbox::FlowUnitEvent 函数 作用 构造方法 构造flowunitevent SetPrivate 设置flowunitevent中的私有值 GetPrivate 获取flowunitevent中的私有值 构造方法 构造FlowUnitEvent对象 FlowUnitEvent(); example: Status Process(std::shared_ptr data_ctx) { auto event = std::make_shared(); event->SetPrivate(\"index\", index_counter); auto index_counter = event->GetPrivate(\"index\"); MBLOG_INFO SetPrivate 设置当前功能单元中保存在flowunitevent中的对象 void SetPrivate(const std::string &key, std::shared_ptr private_content); args: key (string) —— 设置对象的key值 private_content (shared_ptr) —— 设置对象的val的智能指针 return: 无 GetPrivate 获取当前功能单元中保存在flowunitevent中的对象 std::shared_ptr GetPrivate(const std::string &key); args: key (str) —— 需要获取对象的key值 return: shared_ptr key值对应的value的智能指正 example: Status Process(std::shared_ptr data_ctx) { auto index_counter = std::make_shared(0); auto event = std::make_shared(); event->SetPrivate(\"index\", index_counter); auto index_counter = event->GetPrivate(\"index\"); MBLOG_INFO result: 0 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_flowunitdesc.html":{"url":"api/c++/modelbox_flowunitdesc.html","title":"FlowUnitDesc","keywords":"","body":"modelbox::FlowUnitDesc 描述功能单元描述信息的数据结构 函数 作用 SetFlowUnitName 设置功能单元名字 SetDescription 设置功能单元描述 SetFlowUnitGroupType 设置功能单元组类型 AddFlowUnitInput 给功能单元添加输入 AddFlowUnitOutput 给功能单元添加输出 AddFlowUnitOption 给功能单元添加配置项 SetFlowType 设置功能单元数据处理类型 SetOutputType 设置功能单元输入类型 SetConditionType 设置功能单元条件类型 SetInputContiguous 设置功能单元输入是否连续 SetExceptionVisible 设置功能单元异常是否可见 SetFlowUnitName void SetFlowUnitName(const std::string &flowunit_name); args: flowunit_name (string) —— 功能单元的名字 return: 无 SetDescription void SetDescription(const std::string &description) args: description (string) —— 功能单元的详细描述 return: 无 SetFlowUnitGroupType void SetFlowUnitGroupType(const std::string &group_type) args: group_type (string) —— 功能单元的组类型，限制为智能a-z, A-Z, 0-9, _，只允许存在一个/, 已有的group_type为“Image\"等等 return: 无，非必须， 默认为\"generic\" AddFlowUnitInput Status AddFlowUnitInput(const FlowUnitInput &flowunit_input); args: flowunit_input (modelbox::FlowUnitInput) —— flowunit input对象， 参考flowunit_input return: modelbox::Status, 添加flowunit input返回的status状态 AddFlowUnitOutput Status AddFlowUnitOutput(const FlowUnitOutput &flowunit_output); args: flowunit_output (modelbox::FlowUnitOutput) —— flowunit output对象， 参考flowunit_output return: modelbox::Status, 添加flowunit output返回的status状态 AddFlowUnitOption Status AddFlowUnitOption(const FlowUnitOption &flowunit_option); args: flowunit_option (modelbox::FlowUnitOption) —— flowunit option对象， 参考flowunit_option return: modelbox::Status, 添加flowunit option返回的status状态 SetFlowType void SetFlowType(FlowType flow_type); args: flow_type (FlowType) —— 数据处理的 type， 分为NORMAL和STREAM return: 无, 默认为NORMAL SetOutputType void SetOutputType(FlowOutputType output_type) args: output_type (FlowOutputType) —— 数据输出type， 分为ORIGIN, EXPAND, COLLAPSE return: 无 SetConditionType void SetConditionType(ConditionType condition_type) args: condition_type (ConditionType) —— 功能单元条件type， 分为IF_ELSE, NONE return: 无 note: 上述3个接口共同确认功能单元为五种类型，分别为NORMAL, STREAM, IF_ELSE, EXPAND, COLLAPSE 只需要设置某个特定接口即可 五种flowunit设置样例如下： MODELBOX_FLOWUNIT(HTTPServerAsync, desc) { ... // 1. NORMAL desc.SetFlowType(modelbox::NORMAL); // 2. STREAM desc.SetFlowType(modelbox::STREAM); // 3. IF_ELSE desc.SetConditionType(modelbox::IF_ELSE); // 4. EXPAND desc.SetOutputType(modelbox::EXPAND); // 5. COLLAPSE desc.SetOutputType(modelbox::COLLAPSE); ... } SetInputContiguous void SetInputContiguous(bool is_input_contiguous) args: is_input_contiguous (bool) —— 输入内存是否连续 return: 无, 默认为true SetExceptionVisible void SetExceptionVisible(bool is_exception_visible) args: is_exception_visible (bool) —— 异常是否可见 return: 无, 默认为false example: MODELBOX_FLOWUNIT(HTTPServerAsync, desc) { desc.SetFlowUnitName(FLOWUNIT_NAME); desc.AddFlowUnitOutput({\"out_request_info\"}); desc.SetFlowType(modelbox::NORMAL); desc.SetFlowUnitGroupType(\"Input\"); desc.SetDescription(FLOWUNIT_DESC); desc.SetInputContiguous(false); desc.SetExceptionVisible(true); desc.AddFlowUnitOption(modelbox::FlowUnitOption(\"endpoint\", \"string\", true, \"https://127.0.0.1:8080\", \"http server listen URL.\")); desc.AddFlowUnitOption(modelbox::FlowUnitOption( \"max_requests\", \"integer\", true, \"1000\", \"max http request.\")); desc.AddFlowUnitOption( modelbox::FlowUnitOption(\"keepalive_timeout_sec\", \"integer\", false, \"200\", \"keep-alive timeout time(sec)\")); desc.AddFlowUnitOption( modelbox::FlowUnitOption(\"cert\", \"string\", false, \"\", \"cert file path\")); desc.AddFlowUnitOption( modelbox::FlowUnitOption(\"key\", \"string\", false, \"\", \"key file path\")); desc.AddFlowUnitOption(modelbox::FlowUnitOption( \"passwd\", \"string\", false, \"\", \"encrypted key file password.\")); desc.AddFlowUnitOption(modelbox::FlowUnitOption( \"key_pass\", \"string\", false, \"\", \"key for encrypted password.\")); } result: 设置了插件的属性值 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_flowunitport.html":{"url":"api/c++/modelbox_flowunitport.html","title":"FlowUnitPort","keywords":"","body":"modelbox::FlowUnitPort 函数 作用 FlowUnitInput 功能单元输入端口 FlowUnitOutput 功能单元输出端口 FlowUnitOption 功能单元配置项 modelbox::FlowUnitInput FlowUnitInput(const std::string &name); FlowUnitInput(const std::string &name, const std::string &device_type); FlowUnitInput(const std::string &name, uint32_t device_mem_flags); FlowUnitInput(const std::string &name, const std::string &device_type, uint32_t device_mem_flags); args: name (string) —— 输入端口的名字 device_type (string) —— 输入端口的设备类型 device_mem_flags (uint32_t) —— 输入端口设备内存类型 return: 无 modelbox::FlowUnitOutput FlowUnitOutput(const std::string &name); FlowUnitOutput(const std::string &name, uint32_t device_mem_flags); args: name (string) —— 输入端口的名字 device_mem_flags (uint32_t) —— 输入端口设备内存类型 return: 无 modelbox::FlowUnitOption FlowUnitOption(const std::string &name, const std::string &type) FlowUnitOption(const std::string &name, const std::string &type, bool require) FlowUnitOption(const std::string &name, const std::string &type, bool require, const std::string &default_value, const std::string &desc) FlowUnitOption(const std::string &name, const std::string &type, bool require, const std::string &default_value, const std::string &desc, const std::map &values); args: name (string) —— 配置的名字 type (string) —— 配置的数值类型 require (bool) —— 配置是否必须 default_value (string) —— 配置的默认值 desc (string) —— 配置的描述 values (map) —— 配置可以选择的参数map return: 无 example: std::map kCVResizeMethod = { {\"inter_nearest\", cv::INTER_NEAREST}, {\"inter_linear\", cv::INTER_LINEAR}, {\"inter_cubic\", cv::INTER_CUBIC}, {\"inter_area\", cv::INTER_AREA}, {\"inter_lanczos4\", cv::INTER_LANCZOS4}, {\"inter_max\", cv::INTER_MAX}, {\"warp_fill_outliers\", cv::WARP_FILL_OUTLIERS}, {\"warp_inverse_map\", cv::WARP_INVERSE_MAP}, }; MODELBOX_FLOWUNIT(CVResizeFlowUnit, desc) { desc.SetFlowUnitName(FLOWUNIT_NAME); desc.SetFlowUnitGroupType(\"Image\"); desc.AddFlowUnitInput({\"in_image\"}); desc.AddFlowUnitOutput({\"out_image\"}); desc.SetFlowType(modelbox::NORMAL); desc.SetInputContiguous(false); desc.SetDescription(FLOWUNIT_DESC); desc.AddFlowUnitOption(modelbox::FlowUnitOption(\"image_width\", \"int\", true, \"640\", \"the resize width\")); desc.AddFlowUnitOption(modelbox::FlowUnitOption(\"image_height\", \"int\", true, \"480\", \"the resize height\")); std::map method_list; for (auto &item : kCVResizeMethod) { method_list[item.first] = item.first; } desc.AddFlowUnitOption( modelbox::FlowUnitOption(\"interpolation\", \"list\", true, \"inter_linear\", \"the resize interpolation method\", method_list)); } ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_flow.html":{"url":"api/c++/modelbox_flow.html","title":"Flow","keywords":"","body":"modelbox::Flow 函数 作用 Init 初始化flow Build 构造flow Run 同步运行flow RunAsync 异步运行flow Wait 等待流程图flow运行状态返回 Stop 停止flow CreateExternalDataMap 流程图flow创建输入的字典对象 Init 初始化flow enum Format { FORMAT_AUTO, FORMAT_TOML, FORMAT_JSON, FORMAT_UNKNOWN, }; 支持流程图的类型 Status Init(const std::string& configfile, Format format = FORMAT_AUTO); args: configfile (string) —— 流程图所在的路径位置 format (Format) —— 流程图文件类型 return: modelbox::Status 初始化flow的返回状态 Status Init(const std::string& name, const std::string& graph, Format format = FORMAT_AUTO); args: name (string) —— 流程图所在的名字 graph (string) —— 流程图所在的路径位置 format (Format) —— 流程图文件类型 return: modelbox::Status 初始化flow的返回状态 Status Init(const Solution& solution); args: solution (solution) —— 解决方案对象，可以参考Solution return: modelbox::Status 初始化流程图flow的返回状态 Build 构建flow Status Build(); args: 无 return: modelbox::Status 构造流程图flow的返回状态 Run 同步运行flow，阻塞直至运行结束 Status Run(); args: 无 return: modelbox::Status 同步运行流程图flow的返回状态 RunAsync 异步运行flow Status RunAsync(); args: 无 return: modelbox::Status 异步运行流程图flow的返回状态 Stop 停止flow Status Stop(); args: 无 return: modelbox::Status 停止运行流程图flow的返回状态 Wait 等待flow运行结束 Status Wait(int64_t millisecond = 0, Status* ret_val = nullptr); args: millisecond (int64_t) —— 等待的超时时间，毫秒 ret_val (Status*) —— 流程图的运行结果 return: modelbox::Status 等待流程图flow运行结束的返回状态 example: #include int main() { string conf_file = \"test.toml\"; bool async = true; auto flow = std::make_shared(); auto status = flow->Init(conf_file); // init method 2 { auto flow2 = std::make_shared(); auto status = flow2->Init(\"test_graph\", conf_file); } // init method 3 { auto flow3 = std::make_shared(); Solution solution(\"solution_name\"); auto status = flow3->Init(solution); } status = flow->Build(); if (async) { status = flow->RunAsync(); } else { status = flow->Run(); } Status retval; status = flow->Wait(1000 * 3, &retval); status = flow->Stop(); return 0; } CreateExternalDataMap 创建流程图的输入数据的数据结构 std::shared_ptr CreateExternalDataMap(); args: 无 return: modelbox::ExternalDataMap 存储输入数据的数据结构, 参考ExternalDataMap example: #include int main() { auto flow = std::make_shared(); auto external_map = flow->CreateExternalDataMap(); ... return 0; } ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_sessioncontext.html":{"url":"api/c++/modelbox_sessioncontext.html","title":"SessionContext","keywords":"","body":"modelbox::SessionContext 整个任务全局会话上下文 函数 作用 SetPrivate 设置session_context中的私有值 GetPrivate 获取session_context中的私有值 SetSessionId 设置session_context中的session id GetSessionId 获取session_context中的session id SetPrivate 设置SessionContext私有值 void SetPrivate(const std::string &key, std::shared_ptr private_content); args: key (string) —— 设置对象的key值 private_content (shared_ptr) —— 设置对象的val的智能指针 return: 无 GetPrivate 获取SessionContext私有值 std::shared_ptr GetPrivate(const std::string &key); args: key (str) —— 需要获取对象的key值 return: shared_ptr key值对应的value的智能指针 example: Status Process(std::shared_ptr data_ctx) { auto index_counter = std::make_shared(0); auto session_context = data_ctx->GetSessionContext(); session_context->SetPrivate(\"index\", index_counter); auto index_counter = session_context->GetPrivate(\"index\"); MBLOG_INFO result: 0 SetSessionId 设置session_id args: session_id (string) —— 设置会话的id return: 无 GetSessionId 获取session_id args: 无 return: string 获取当前session的id example: Status Process(std::shared_ptr data_ctx) { auto session_context = data_ctx->GetSessionContext(); session_context->SetSessionId(\"12345\"); auto res = session_context->GetSessionId(\"index\"); MBLOG_INFO result: 12345 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/c++/modelbox_solution.html":{"url":"api/c++/modelbox_solution.html","title":"Solution","keywords":"","body":"modelbox::Solution 函数 作用 构造方法 创建solution SetSolutionDir 设置solution的路径 SetArgs 设置solution的参数 GetSolutionDir 获取solution的路径 GetSolutionName 获取solution的名字 构造方法 创建solution Solution(const std::string &solution_name); args: solution_name (string) —— solution对象的名字 return: modelbox::Solution Solution对象 SetSolutionDir 设置solution的路径 void SetSolutionDir(const std::string &dir); args: dir (string) —— 设置solution对象的路径 return: 无 SetArgs 设置solution的参数 Solution &SetArgs(const std::string &key, const std::string &value); args: key (string) —— 设置solution参数的key值 value (string) —— 设置solution参数的value值 return: modelbox::Solution GetSolutionDir 获取solution对象的路径 const std::string GetSolutionDir() const; args: 无 return: string solution对象的路径 GetSolutionName 获取solution对象的名字 const std::string GetSolutionName() const; args: 无 return: string solution对象的名字 example: #include int main() { string path = \"test/solution/path\"; strint name = \"test_solution\"; Solution solution(name); solution.SetSolutionDir(path); auto get_name = solution.GetSolutionName(); auto get_path = solution.GetSolutionDir(); solution.SetArgs(\"key1\", \"value1\").SetArgs(\"key2\", \"value2\"); MBLOG_INFO result: test_solution, test/solution/path ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python.html":{"url":"api/python.html","title":"Python","keywords":"","body":"Python API Python API如下表所示： 组件名 功能 modelbox.Buffer ModelBox存储基本数据的数据结构 modelbox.BufferList ModelBox存储基本数据的数据结构 modelbox.Configuration ModelBox存放配置项的数据结构 modelbox.DataContext ModelBox在功能单元执行时存放数据的上下文 modelbox.DataMeta ModelBox挂在端口上面存放数据元信息的数据结构 modelbox.Device ModelBox映射设备的数据结构 modelbox.ExtOutputBufferList ModelBox获取入口信息的数据结构 modelbox.ExternalData ModelBox功能单元中自驱动产生数据的数据结构 modelbox.ExternalDataMap ModelBox当中流程图对象产生数据的数据结构 modelbox.Flow ModelBox中启动任务的组件 modelbox.FlowUnit ModelBox当中构建业务的基本节点的数据结构 modelbox.FlowUnitError ModelBox中描述error的数据结构 modelbox.FlowUnitEvent ModelBox中描述event的数据结构 modelbox.Log ModelBox的日志组件 modelbox.SessionContext ModelBox描述上下文的数据结构 modelbox.Status ModelBox描述状态的数据结构 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_buffer.html":{"url":"api/python/modelbox_buffer.html","title":"Buffer","keywords":"","body":"modelbox.Buffer 函数 作用 构造方法 Buffer的构造方法 as_object 将ModelBox Buffer对象转换成Python对象 copy_meta 拷贝参数自带的所有Meta信息给当前Buffer has_error 判断当前Buffer是否存在处理异常 get_error 获取当前Buffer的处理异常 get_bytes 获取当前Buffer的字节数 get 获取当前Buffer的某个Meta值 set 设置当前Buffer的某个Meta值 构造方法 构造Buffer对象。 modelbox.Buffer(device, data) args: device (modelbox.Device) —— 构造当前Buffer所在的modelbox.Device对象 data (numpy.array) —— 当前Buffer包含的numpy数据 modelbox.Buffer(device, string) args: device (modelbox.Device) —— 构造当前Buffer所在的modelbox.Device对象 string (str) —— 当前Buffer包含的string数据 modelbox.Buffer(device, list_item) args: device (modelbox.Device) —— 构造当前Buffer所在的modelbox.Device对象 list_item (str) —— 当前Buffer包含的list数据，其中每一个元素必须同一类型 return: modelbox.Buffer example: import numpy as np ... def process(self, data_ctx): infer_data = np.ones((5,5)) numpy_buffer = modelbox.Buffer(self.get_bind_device(), infer_data) str_buffer = modelbox.Buffer(self.get_bind_device(), \"test\") list_buffer = modelbox.Buffer(self.get_bind_device(), [3.1, 3.2, 3.3]) ... return modelbox.Status() modelbox.Buffer.as_object 将ModelBox Buffer对象自动转换成Python原始对象，如Buffer是由numpy类型转为而来，则调用as_object后返回numpy类型对象。目前支持基础类型、numpy.array、str类型。 args: 无 return: 基础类型、str 或者 numpy.array对象 example: ... def process(self, data_ctx): buf_list = data_ctx.input(\"input\") for buf in buf_list: data = buf.as_object() print(data, type(data)) ... return modelbox.Status() modelbox.Buffer.has_error 判断当前Buffer是否存在处理异常。 args: 无 return: bool, 是否存在处理异常 example: ... def process(self, data_ctx): buf_list = data_ctx.input(\"input\") for buf in buf_list: res = buf.has_error() ... return modelbox.Status() modelbox.Buffer.get_error 获取当前Buffer的第一个异常信息对象。 args: 无 return: modelbox.FlowUnitError example: ... def process(self, data_ctx): buf_list = data_ctx.input(\"input\") for buf in buf_list: error = buf.get_error() ... return modelbox.Status() modelbox.Buffer.get_bytes 获取当前Buffer的字节数 args: 无 return: int64, Buffer的字节数 example: import numpy as np ... def process(self, data_ctx): infer_data = np.ones((5,5)) new_buffer = modelbox.Buffer(self.get_bind_device(), infer_data) bytes = new_buffer.get_bytes() print(bytes) ... return modelbox.Status() result: 字节数为 5*5*8 = 200 modelbox.Buffer.set 设置当前Buffer的某个Meta值 args: key (str) —— Meta的key值 obj (int, str, double, bool, modelbox.ModelBoxDataType, list[str], list[int], list[double], list[bool], list[list], numpy) —— Meta的value值 return: 无 modelbox.Buffer.get 获取当前Buffer的某个Meta值 args: key (str) ——Meta的key值 return: Python object 获取key对应的value值 type: int, double, str, bool, list[int], list[str], list[double], list[bool], list[list], numpy example: ... def process(self, data_ctx): infer_data = np.ones((5,5)) new_buffer = modelbox.Buffer(self.get_bind_device(), infer_data) res = new_buffer.set(\"key\", \"test\") print(res) print(new_buffer.get(\"key\")) ... return modelbox.Status() result: true test modelbox.Buffer.copy_meta 把参数的Buffer的所有Meta信息拷贝给当前Buffer args: buffer (modelbox.Buffer) —— 源Buffer return: modelbox.Status example: import numpy as np ... def process(self, data_ctx): buf_list = data_ctx.input(\"input\") for buf in buf_list: infer_data = np.ones((5,5)) new_buffer = modelbox.Buffer(self.get_bind_device(), infer_data) # new_buffer具有和buf相同的Meta信息 status = new_buffer.copy_meta(buf) ... return modelbox.Status() ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_bufferlist.html":{"url":"api/python/modelbox_bufferlist.html","title":"BufferList","keywords":"","body":"modelbox.BufferList 函数 作用 构造方法 BufferList的构造方法 build BufferList对象申请指定长度大小的内存空间 copy_meta 拷贝参数自带的所有Meta信息给当前BufferList get_bytes 获取当前BufferList的字节大小 push_back 将一个Buffer插入到当前的BufferList当中 set 设置当前BufferList的某个Meta值 size 获取当前BufferList的长度 len 可以被len函数获取当前BufferList的长度 构造方法 构造BufferList对象。 modelbox.BufferList() args: 无 modelbox.BufferList(device) args: device (modelbox.Device) —— 构造当前Buffer所在的modelbox.Device对象 modelbox.BufferList(buffer) args: buffer (modelbox.Buffer) —— 通过Buffer构建BufferList modelbox.BufferList(buffer_list) args: buffer_list (list[modelbox.Buffer]) —— 通过一组Buffer构建BufferList return: modelbox.BufferList example: ... def process(self, data_ctx): inputbuf_list = data_ctx.input(\"input\") outputbuf_list = data_ctx.output(\"output\") ... return modelbox.Status() inputbuf_list和output_buf_list均为构建好的BufferList，一般而言并不需要开发者构建BufferList。 modelbox.BufferList.build BufferList申请Buffer内存空间 args: sizes (list[int]) —— BufferList当中每一个Buffer的大小 return: modelbox.Status 申请结果 example: ... def process(self, data_ctx): buf_list = data_ctx.output(\"output\") res = buf_list.build([20, 20, 20]) print(res, buf_list.size()) for buf in buf_list: print(buf.get_bytes()) ... return modelbox.Status() result: true, 3202020 modelbox.BufferList.copy_meta 把参数的BufferList里面的Meta信息拷贝给当前的BufferList，按照Buffer一对一拷贝。 args: buffer_list (modelbox.BufferList) —— 源BufferList return: modelbox.Status example: ... def process(self, data_ctx): input_bufs = data_ctx.input(\"input\") output_bufs = data_ctx.output(\"output\") input_bytes = [] for buf in input_bufs: input_bytes.append(buf.get_bytes()) output_bufs.build(input_bytes) # output_bufs具有和原始input_bufs相同的Meta信息 res = output_bufs.copy_meta(input_bufs) ... return modelbox.Status() modelbox.BufferList.get_bytes 获取BufferList中的所有的字节数 args: 无 return: uint64 example: ... def process(self, data_ctx): buf_list = data_ctx.output(\"output\") buf_list.build([20,20,20]) print(buf_list.get_bytes()) ... return modelbox.Status() result: 60 modelbox.BufferList.push_back 往BufferList当中插入一个新的Buffer。 args: buffer (modelbox.Buffer) —— 需要插入到BufferList当中的Buffer return: 无 example: import numpy as np ... def process(self, data_ctx): buf_list = data_ctx.output(\"output\") infer_data = np.ones((5,5)) new_buffer = modelbox.Buffer(self.get_bind_device(), infer_data) buf_list.push_back(new_buffer) ... return modelbox.Status() modelbox.BufferList.size 获取当前BufferList的长度 args: 无 return: modelbox.Bufferlist的长度 example: ... def process(self, data_ctx): buf_list = data_ctx.output(\"output\") buf_list.build([20,20,20]) size = buf_list.size() length = len(buf_list) print(size) print(length) for buf in buf_list: print(buf.get_bytes()) ... return modelbox.Status() result: 33202020 modelbox.BufferList.set 设置当前BufferList的某个Meta值 args: key (str) —— Meta的key值 obj (int, str, double, bool, modelbox.ModelBoxDataType, list[str], list[int], list[double], list[bool]) —— Meta的value值 return: 无 example: ... def process(self, data_ctx): buf_list = data_ctx.output(\"output\") for i in range(3): infer_data = np.ones((5,5)) new_buffer = modelbox.Buffer(self.get_bind_device(), infer_data) buf_list.push_back(new_buffer) res = buf_list.set(\"key\", \"test\") print(res) for buf in buf_list: print(buf.get(\"key\")) ... return modelbox.Status() result: truetesttesttest ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_configuration.html":{"url":"api/python/modelbox_configuration.html","title":"Configuration","keywords":"","body":"modelbox.Configuration 函数 作用 get 从configuration对象当中获取值 set 在configuration对象中设置值 modelbox.Configuration.set args: key: str meta的key值 obj: int, str, double, bool, list[str], list[int], list[double], list[bool]meta的value值 return: 无 modelbox.Configuration.get api: 函数名 功能 get_string 获取字符串值 get_int 获取整型值 get_float 获取浮点型值 get_bool 获取布尔值 get_string_list 获取字符串列表值 get_int_list 获取整型列表值 get_float_list 获取浮点型列表值 get_bool_list 获取布尔列表值 args: key (str) —— 需要获取的meta的key值 return: Python object 获取key对应的value值 type: int, double, str, bool, list[int], list[str], list[double], list[bool] example: ... def Open(self, data_ctx, config): config.set(\"int\", 3) config.set(\"bool\", True) config.set(\"string\", \"test\") config.set(\"float\", 3.1) config.set(\"ints\", [3,4]) config.set(\"bools\", [True, False]) config.set(\"strings\", [\"test1\", \"test2\"]) config.set(\"floats\", [3.1, 3.2]) print(config.get_int(\"int\")) print(config.get_int_list(\"ints\")) print(config.get_bool(\"bool\")) print(config.get_bool_list(\"bools\")) print(config.get_string(\"string\")) print(config.get_string_list(\"strings\")) print(config.get_float(\"float\")) print(config.get_float_list(\"floats\")) ... return modelbox.Status() result: 3[3,4]True[True, False]\"test\"[\"test1\", \"test2\"]3.1[3.1, 3.2] ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_datacontext.html":{"url":"api/python/modelbox_datacontext.html","title":"DataContext","keywords":"","body":"modelbox.DataContext 每一个功能单元当中存放数据上下文的所有接口 函数 作用 input 获取功能单元输入端口的数据 output 获取功能单元输出端口的数据 has_error datacontext当中是否存在error get_error 从datacontext中获取error set_private 设置datacontext中的私有值，值可以是Python任意对象 get_private 获取datacontext中的私有值，值可以是Python任意对象 set_private_string 设置datacontext中的私有字符串值 get_private_string 获取datacontext中的私有字符串值 set_private_int 设置datacontext中的私有整型值 get_private_int 获取datacontext中的私有整型值 get_input_meta 获取datacontext当中绑定在input端口的值 set_output_meta 设置datacontext当中绑定在output端口的值 get_session_config 获取datacontext当中的session config get_session_context 获取datacontext当中的session_context send_event 发送event modelbox.DataContext.input 根据端口名称获取功能单元输入端口的数据。 args: key (str) —— input端口的名称 return: modelbox.BufferList 名称为key的input端口的数据的BufferList modelbox.DataContext.output 根据端口名称获取功能单元输出端口的数据。 args: key (str) —— output端口的名称 return: modelbox.BufferList 名称为key的output端口的数据的BufferList example: ... def process(self, data_ctx): input_buf_list = data_ctx.input(\"input\") output_buf_list = data_ctx.output(\"output\") ... return modelbox.Status() modelbox.DataContext.has_error 当前数据上下文中是否存在Buffer处理异常。 args: 无 return: bool 是否存在error modelbox.DataContext.get_error 获取当前数据上下文中存在Buffer处理异常对象。 args: 无 return: modelbox.FlowUnitError example: ... def process(self, data_ctx): if data_ctx.has_error(): error = data_ctx.get_error() print(error.get_description(), type(error)) ... return modelbox.Status() result: \"error message\"modelbox.FlowUnitError modelbox.DataContext.send_event 从当前数据上下文发送事件给Modelbox调框架，可以再次触发调度该功能单元。 args: modelbox.FlowUnitEvent对象 return: 无 example: ... def process(self, data_ctx): event = modelbox.FlowUnitEvent() data_ctx.send_event(event) ... return modelbox.Status() modelbox.DataContext.set_private 设置当前数据上下文任意对象。 args: key (str) —— 设置字符串型值的key value (str) —— 设置任意Python对象 return: 无 modelbox.DataContext.get_private 获取当前数据上下文私有对象。 args: key (str) —— 需要获取的字符串型的key return: py::object 获取当前key值的Python对象 modelbox.DataContext.set_private_string 设置当前数据上下文私有字符串值。 args: key (str) —— 设置字符串型值的key value (str) —— 设置字符串型值的value return: 无 modelbox.DataContext.get_private_string 获取当前数据上下文私有字符串值。 args: key (str) —— 需要获取的字符串型的key return: str 获取当前key值的字符串型value值 modelbox.DataContext.set_private_int 设置当前数据上下文私有整型值。 args: key (str) —— 设置整型值得key value (int) —— 设置整型值的value return: 无 modelbox.DataContext.get_private_int 获取当前数据上下文私有整型值 args: key (str) —— 需要获取的整型的key return: int 获取当前key值的整型value值 example: ... def process(self, data_ctx): data_ctx.set_private_string(\"test\", \"test\") print(data_ctx.get_private_string(\"test\")) data_ctx.set_private_int(\"int\", 33) print(data_ctx.get_private_int(\"int\")) ... return modelbox.Status() result: \"test\"33 modelbox.DataContext.get_input_meta 获取当前数据上下文的input端口上面的Meta值。 args: key (str) —— input端口名 return: modelbox.DataMeta 保存meta值得modelbox数据结构 modelbox.DataContext.set_output_meta 设置当前数据上下文的output端口上面的Meta值 args: key (str) —— output端口名 meta (modelbox.DataMeta) —— 绑定在当前端口的DataMeta数据结构 return: modelbox.Status example: ... def process(self, data_ctx): input_meta = data_ctx.get_input_meta(\"input\") res = data_ctx.set_output_meta(\"output\", input_meta) ... return modelbox.Status() 获取input端口的meta和直接将该meta设置给output, modelbox.DataMeta参照data meta的接口。 modelbox.DataContext.get_session_config 获取当前数据上下文的Session级别配置对象。 args: 无 return: modelbox.Configuration modelbox.DataContext.get_session_context 获取当前数据上下文的SessionContext对象。 args: 无 return: modelbox.SessionContext example: ... def process(self, data_ctx): session_config = data_ctx.get_session_config() session_context = data_ctx.get_session_context() ... return modelbox.Status() ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_datameta.html":{"url":"api/python/modelbox_datameta.html","title":"DataMeta","keywords":"","body":"modelbox.DataMeta 函数 作用 set_private_string 设置DataMeta中的私有字符串值 get_private_string 获取DataMeta中的私有字符串值 set_private_int 设置DataMeta中的私有整型值 get_private_int 获取DataMeta中的私有整型值 modelbox.DataMeta.set_private_string 设置DataMeta私有字符串值。 args: key (str) —— 设置字符串型值得key value (str) —— 设置字符串型值的value return: 无 modelbox.DataMeta.get_private_string 获取DataMeta私有字符串值。 args: key (str) —— 需要获取的字符串型的key return: str 获取当前key值的字符串型value值 modelbox.DataMeta.set_private_int 设置DataMeta私有整型值。 args: key (str) —— 设置整型值得key value (int) 设置整型值的value return: 无 modelbox.DataMeta.get_private_int 获取DataMeta私有整型值。 args: key (str) —— 需要获取的整型的key return: int 获取当前key值的整型value值 example: ... def Process(self, data_ctx): data_meta = modelbox.DataMeta() data_meta.set_private_string(\"test\", \"test\") print(data_meta.get_private_string(\"test\")) data_meta.set_private_int(\"int\", 33) print(data_meta.get_private_int(\"int\")) ... return modelbox.Status() result: \"test\"33 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_device.html":{"url":"api/python/modelbox_device.html","title":"Device","keywords":"","body":"modelbox.Device 函数 作用 get_device_id 获取当前设备编号, e.g. 0, 1, 2 get_type 获取当前设备类型, e.g. cpu, cuda, ascend get_device_desc 获取当前设备描述 modelbox.Device.get_device_id 获取当前设备编号。 args: 无 return: str 设备编号 modelbox.Device.get_type 获取当前设备类型。 args: 无 return: str 设备的类型信息 modelbox.Device.get_device_desc 获取当前设备描述。 args: 无 return: str 设备的描述信息 example: ... def process(self, data_ctx): device = get_bind_device() print(device.get_device_id()) print(device.get_device_type()) print(device.get_device_desc()) ... return modelbox.Status() result: \"0\"\"cpu\"\"this is a cpu device\" ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_externaldata.html":{"url":"api/python/modelbox_externaldata.html","title":"ExternalData","keywords":"","body":"modelbox.ExternalData 函数 作用 create_buffer_list 创建BufferList send 发送要给本功能单元的process单元的BufferList get_session_context 获取session_context对象 get_session_config 获取session_config对象 close 关闭当前external_data的链接 modelbox.ExternalData.create_buffer_list 创建BufferList。 args: 无 return: modelbox.BufferList modelbox.ExternalData.send 发送BufferList给下一个功能单元。使用场景：可以在当前功能单元open中发送数据，在process中处理。 args: bufferlist (modelbox.BufferList) —— 需要发送的BufferList return: modelbox.Status modelbox.ExternalData.close 关闭external对象。 args: 无 return: modelbox.Status example: ... def open(self, config): extern_data = create_external_data() buffer_list = extern_data.create_buffer_list() buffer = np.ones((5,5)) buffer_list.push_back(buffer) extern_data.send(buffer_list) extern_data.close() ... return modelbox.Status() 本功能单元的process既可以接受到当前BufferList modelbox.ExternalData.get_session_context 获取SessionContext对象。 args: 无 return: modelbox.SessionContext modelbox.ExternalData.get_session_config 获取Session级别配置对象。 args: 无 return: modelbox.Configuration example: ... def Open(self, config): extern_data = create_external_data() session_config = extern_data.get_session_config() session_context = extern_data.get_session_context() ... return modelbox.Status() ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_externaldatamap.html":{"url":"api/python/modelbox_externaldatamap.html","title":"ExternalDataMap","keywords":"","body":"modelbox.ExternalDataMap 函数 作用 create_buffer_list 创建BufferList send 发送BufferList给当前流程图 recv 接收给当前流程图处理结果 close 关闭当前external_data_map的输入 shutdown 强制关闭当前external_data_map的连接 set_output_meta 设置external_data_map中的输出端口的meta值 modelbox.ExternalDataMap.create_buffer_list 创建BufferList。 args: 无 return: modelbox.BufferList modelbox.ExternalDataMap.send 发送BufferList给流程图中的输入端口。 args: port_name (str) —— 需要发送的数据的端口名 bufferlist (modelbox.BufferList) —— 需要发送的BufferList return: modelbox.Status modelbox.ExternalDataMap.recv 接收给当前流程图处理结果。 args: bufferlist (modelbox.ExtOutputBufferList) —— 用来接收数据的BufferList return: modelbox.Status modelbox.ExternalDataMap.close 关闭当前external_data_map的输入，表示数据输入结束 args: 无 return: modelbox.Status modelbox.ExternalDataMap.shutdown 强制关闭当前ExternalDataMap对象的状态 args: 无 return: modelbox.Status example: ... import modelbox import numpy as np flow = modelbox.Flow() img_np = np.ones((5,5)) extern_data_map = flow.create_external_data_map() buffer_list = extern_data_map.create_buffer_list() buffer_list.push_back(img_np) # input1 为流程图中定义的input类型端口的端口名称 ret = extern_data_map.send(\"input1\", buffer_list) ret = extern_data_map.close() buffer_list_map = modelbox.ExtOutputBufferList() ret = extern_data_map.recv(buffer_list_map) ret = extern_data_map.shutdown() digraph demo { input1[type=input] python_buffer[type=flowunit, flowunit=python_buffer, device=cpu, deviceid=0] output1[type=output] input1 -> python_buffer:buffer_in python_buffer:buffer_out -> output1 } modelbox.ExternalDataMap.set_output_meta 设置外部数据输出端口的Meta值。 args: port_name (str) —— 输出的端口名字 meta (modelbox.DataMeta) —— 存放Meta的数据结构 return: modelbox.Status ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_extoutputbufferlist.html":{"url":"api/python/modelbox_extoutputbufferlist.html","title":"ExtOutputBufferList","keywords":"","body":"modelbox.ExtOutputBufferList 函数 作用 get_buffer_list 获取数据中的BufferList modelbox.ExtOutputBufferList.get_buffer_list 获取指定端口的BufferList数据, 一般和externalDataMap一起使用。 args: 无 return: modelbox.BufferList example: ... import numpy as np img_np = np.ones((5,5)) flow = modelbox.Flow() extern_data_map = flow.create_external_data_map() buffer_list = extern_data_map.create_buffer_list() buffer_list.push_back(img_np) extern_data_map.send(\"input1\", buffer_list) extern_data_map.shutdown() buffer_list_map = modelbox.ExtOutputBufferList() ret = extern_data_map.recv(buffer_list_map) self.assertTrue(ret) # output1 为流程图中定义的output类型端口的端口名称 result_buffer_list = buffer_list_map.get_buffer_list(\"output1\") ... ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_flow.html":{"url":"api/python/modelbox_flow.html","title":"Flow","keywords":"","body":"modelbox.Flow 函数 作用 init 初始化flow对象 build 构建flow对象，当中主要通过配置文件构建图 run 同步运行flow run_async 异步运行flow wait flow对象等待返回 stop 停止flow create_external_data_map 创建external_data_map modelbox.Flow.init 初始化flow对象。 modelbox.Flow.init(conf_file, format) args: conf_file (str) —— 构建流程图的toml文件 format (modelbox.Flow.Format) —— 流程图的format格式，默认可不填 modelbox.Flow.init(name, graph, format) args: name (str) —— 构建图的名称 graph (str) —— 流程图 format (modelbox.Flow.Format) —— 流程图的format格式，默认可不填 modelbox.Flow.init(config) args: config (modelbox.Configuration) —— 构建图的配置 return: modelbox.Status modelbox.Flow.build 构建流程图对象，当中主要通过配置文件构建图。 args: 无 return: modelbox.Status modelbox.Flow.run 同步运行流程图。 args: 无 return: modelbox.Status modelbox.Flow.run_async 异步运行flow args: 无 return: modelbox.Status modelbox.Flow.wait 等待流程图运行结束。 args: 无 return: modelbox.Status modelbox.Flow.stop 停止运行flow。 args: 无 return: modelbox.Status example: ... conf_file = \"test.toml\" ret = flow.init(conf_file) ret = flow.build() async = True if async == True: ret = flow.run_async() else: ret = flow.run() retval = modelbox.Status() ret = flow.wait(0, retval) ret = flow.stop() modelbox.Flow.create_external_data_map 创建流程图的外部输入对象。 args: 无 return: modelbox.ExternalDataMap, 创建好的external_data_map example: flow = modelbox.Flow() extern_data_map = flow.create_external_data_map() ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_flowunit.html":{"url":"api/python/modelbox_flowunit.html","title":"Flowunit","keywords":"","body":"modelbox.FlowUnit 函数 作用 open 功能单元初始化逻辑 process 功能单元处理逻辑 close 功能单元关闭逻辑 data_pre Stream流初始化时逻辑 data_post Stream流结束时逻辑 create_external_data 创建external_data_map get_bind_device 获取绑定设备 create_buffer 创建Buffer modelbox.FlowUnit.open 功能单元初始化逻辑。 args: config (modelbox.Configuration) —— 流程图当中配给当前flowunit的配置项 return: modelbox.Status 初始化flowunit的返回状态 modelbox.FlowUnit.process 功能单元处理逻辑。 args: data_context (modelbox.DataContext) —— 处理逻辑当中存放数据的上下文 return: modelbox.Status flowunit处理逻辑的返回状态 modelbox.FlowUnit.close 功能单元结束逻辑。 args: 无 return: modelbox.Status flowunit 结束逻辑的返回状态 modelbox.FlowUnit.data_pre Stream流初始化时逻辑。 args: data_context (modelbox.DataContext) —— 初始化stream逻辑当中存放数据的上下文 return: modelbox.Status 初始化stream时的逻辑的返回状态 modelbox.FlowUnit.data_post Stream流结束时逻辑。 args: data_context (modelbox.DataContext) —— 结束stream逻辑当中存放数据的上下文 return: modelbox.Status 结束stream逻辑的返回状态 example: ... class ExampleFlowUnit(modelbox.FlowUnit): # Derived from modelbox.FlowUnit def __init__(self): super().__init__() def open(self, config): # Open the flowunit to obtain configuration information return modelbox.Status.StatusCode.STATUS_SUCCESS def process(self, data_context): # Process the data in_data = data_context.input(\"in_1\") out_data = data_context.output(\"out_1\") # Example process code. # Remove the following code and add your own code here. for buffer in in_data: response = \"Hello World \" + buffer.as_object() result = response.encode('utf-8').strip() add_buffer = modelbox.Buffer(self.get_bind_device(), result) out_data.push_back(add_buffer) return modelbox.Status.StatusCode.STATUS_SUCCESS def close(self): # Close the flowunit return modelbox.Status() def data_pre(self, data_context): # Before streaming data starts return modelbox.Status() def data_post(self, data_context): # After streaming data ends return modelbox.Status() [base] name = \"Example\" # The FlowUnit name device = \"cpu\" # The device the flowunit runs on，cpu，cuda，ascend。 version = \"1.0.0\" # The version of the flowunit description = \"description\" # The description of the flowunit entry = \"example@ExampleFlowUnit\" # Python flowunit entry function type = \"python\" # Fixed value # Flowunit Type stream = false # Whether the flowunit is a stream flowunit condition = false # Whether the flowunit is a condition flowunit collapse = false # Whether the flowunit is a collapse flowunit collapse_all = false # Whether the flowunit will collapse all the data expand = false # Whether the flowunit is a expand flowunit # The default Flowunit config [config] item = \"value\" # Input ports description [input] [input.input1] # Input port number, the format is input.input[N] name = \"in_1\" # Input port name type = \"string\" # Input port type # Output ports description [output] [output.output1] # Output port number, the format is output.output[N] name = \"out_1\" # Output port name type = \"string\" # Output port type modelbox.FlowUnit.create_external_data 创建external_data args: 无 return: modelbox.ExternalData, 创建好的external_data example: ... def open(self, config): extern_data = self.create_external_data() ... return modelbox.Status() result: 可以参考external data的接口 modelbox.FlowUnit.get_bind_device 获取当前flowunit绑定设备 args: 无 return: modelbox.Device example: ... def process(self, data_context): device = self.get_bind_device() print(type(device)) ... return modelbox.Status() modelbox.FlowUnit.create_buffer 创建Buffer args: 无 return: modelbox.Buffer 创建出来的Buffer example: ... def process(self, data_context): e_array = np.array([]) e_buffer = self.create_buffer(e_array) ... return modelbox.Status() ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_flowuniterror.html":{"url":"api/python/modelbox_flowuniterror.html","title":"FlowunitError","keywords":"","body":"modelbox.FlowUnitError 函数 作用 构造方法 构造modelbox.FlowUnitError get_description 获取异常的描述信息 构造方法 构造FlowUnitError对象。 args: error (str) —— 具体的error描述 return: modelbox.FlowUnitError modelbox.FlowUnitError.get_description 获取异常的描述信息 args: 无 return: str, 当前error的描述信息 example: ... error = modelbox.FlowUnitError(\"this is error\") err_desc = error.get_description() print(error) result: \"this is error\" ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_flowunitevent.html":{"url":"api/python/modelbox_flowunitevent.html","title":"FlowunitEvent","keywords":"","body":"modelbox.FlowUnitEvent 函数 作用 set_private_string 设置FlowUnitEvent事件中的私有字符串值 get_private_string 获取FlowUnitEvent事件中的私有字符串值 set_private_int 设置FlowUnitEvent事件中的私有整型值 get_private_int 获取FlowUnitEvent事件中的私有整型值 modelbox.FlowUnitEvent.set_private_string 设置FlowUnitEvent私有字符串值 args: key (str) —— 设置字符串型值得key value (str) —— 设置字符串型值的value return: 无 modelbox.FlowUnitEvent.get_private_string 获取FlowUnitEvent私有字符串值 args: key (str) —— 设置字符串型值得key return: str 获取当前key值的字符串型value值 modelbox.FlowUnitEvent.set_private_int 设置FlowUnitEvent私有整型值 args: key (str) —— 设置整型值得key value (int) —— 设置整型值的value return: 无 modelbox.FlowUnitEvent.get_private_int 获取FlowUnitEvent私有整型值 args: key (str) —— 设置整型值得key return: int 获取当前key值的整型value值 example: ... def process(self, data_ctx): event = modelbox.FlowUnitEvent() event.set_private_string(\"test\", \"test\") print(event.get_private_string(\"test\")) event.set_private_int(\"int\", 33) print(event.get_private_int(\"int\")) ... return modelbox.Status() result: \"test\"33 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_log.html":{"url":"api/python/modelbox_log.html","title":"Log","keywords":"","body":"modelbox.Log 函数 作用 reg 注册log的回调函数 set_log_level 设置log的日志级别 print 打印日志 print_ext 打印带有额外信息的日志 modelbox.Log.reg 注册一个log的回调函数。 args: Callable (python function) —— 注册的Python回调函数Callable[[Level level, str file, int lineno, str func, str msg], None] return: 无 modelbox.Log.set_log_level 设置log组件的日志级别。 args: level (modelbox.Log.Level) —— 日志级别 return: 无 modelbox.Log.print 打印日志。 args: level (modelbox.Log.Level) —— 日志级别 msg (str) —— 打印的日志 return: 无 modelbox.Log.printExt 打印带有额外信息的日志 args: level (modelbox.Log.Level) —— 日志级别 file (str) —— 文件的路径 lineno (str) —— 行号 func (python function) —— 日志的回调函数 msg (str) —— 日志信息 return: 无 example: import modelbox import inspect import os import datetime def LogCallback(level, file, lineno, func, msg): print(\"[{time}][{level}][{file}:{lineno}] {msg}\".format( time=datetime.datetime.now(), level=level, file=file, lineno=lineno, msg=msg )) def RegLog(log): log.reg(LogCallback) log.set_log_level(modelbox.Log.Level.INFO) if __name__ == \"__main__\": log = modelbox.Log() RegLog(log) log.print(modelbox.Log.Level.INFO, \"test print\") frame = inspect.currentframe() info = inspect.getframeinfo(frame) log.print_ext(modelbox.Log.Level.INFO, os.path.basename(info.filename), info.lineno + 2, info.function, \"test print_ext\") # 推荐用法 modelbox.info(\"test_info\") modelbox.warn(\"test_warn\") modelbox.error(\"test_error\") result: [2021-12-13 17:45:47.614834][Level.INFO][test.py:20] test print[2021-12-13 17:45:47.615028][Level.INFO][test.py:25] test print_ext[2021-12-13 19:39:54.994327][Level.INFO][test.py:28] test_info[2021-12-13 19:39:54.994454][Level.WARN][test.py:29] test_warn[2021-12-13 19:39:54.994601][Level.ERROR][test.py:30] test_error modelbox.Log.Level log level分为以下几种级别 modelbox.Log.Level.DEBUGmodelbox.Log.Level.INFOmodelbox.Log.Level.ERRORmodelbox.Log.Level.WARN ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_sessioncontext.html":{"url":"api/python/modelbox_sessioncontext.html","title":"SessionContext","keywords":"","body":"modelbox.SessionContext 函数 作用 set_private 设置SessionContext中的私有值 get_private 获取SessionContext中的私有值 set_private_string 设置SessionContext中的私有字符串值 get_private_string 获取SessionContext中的私有字符串值 set_private_int 设置SessionContext中的私有整型值 get_private_int 获取SessionContext中的私有整型值 get_session_config 获取SessionContext中的配置对象 get_session_id 获取SessionContext的ID modelbox.SessionContext.set_private 设置SessionContext私有值。 args: key (str) —— 设置字符串型值的key value (str) —— 设置任意Python对象 return: 无 modelbox.SessionContext.get_private 获取SessionContext私有值。 args: key (str) —— 设置字符串型值得key return: py::object 获取当前key值的Python对象 modelbox.SessionContext.set_private_string 设置SessionContext私有字符串值。 args: key (str) —— 设置字符串型值得key value (str) —— 设置字符串型值的value return: 无 modelbox.SessionContext.get_private_string 获取SessionContext私有字符串值。 args: key (str) —— 设置字符串型值得key return: str 获取当前key值的字符串型value值 modelbox.SessionContext.set_private_int 设置SessionContext私有整型值。 args: key (str) —— 设置整型值得key value (int) —— 设置整型值的value return: 无 modelbox.SessionContext.get_private_int 获取SessionContext私有整型值。 args: key (str) —— 设置整型值得key return: int 获取当前key值的整型value值 example: ... def process(self, data_ctx): session_ctx = data_ctx.get_session_context() session_ctx.set_private_string(\"test\", \"test\") print(session_ctx.get_private_string(\"test\")) session_ctx.set_private_int(\"int\", 33) print(session_ctx.get_private_int(\"int\")) ... return modelbox.Status() result: \"test\"33 modelbox.SessionContext.get_session_config 获取Session级别配置对象。 args: 无 return: modelbox.Configuration modelbox.SessionContext.get_session_id 获取Session ID。 args: 无 return: str 获取当前session的id example: ... def process(self, data_ctx): session_ctx = data_ctx.get_session_context() id = session_ctx.get_session_id() config = session_ctx.get_session_config() ... return modelbox.Status() ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"api/python/modelbox_status.html":{"url":"api/python/modelbox_status.html","title":"Status","keywords":"","body":"modelbox.Status 函数 作用 构造方法 构造status对象 code status的code码 str_code status的code码的描述 StatusCode status的状态码 set_errormsg status设置错误信息 errormsg status获取错误信息 wrap_errormsgs status获取链式的错误信息 unwrap status解链式status对象 构造方法 构造modelbox.Status对象。 modelbox.Status(status_code) args: code (modelbox.Status.StatusCode) —— 从StatusCode创建Status modelbox.Status(success) args: success (bool) —— 从bool创建Status modelbox.Status(code, errmsg) args: code (modelbox.Status.StatusCode) —— modelbox的状态码 errmsg (str) —— 错误的信息 modelbox.Status(status, errmsg) status (modelbox.Status) —— modelbox的状态 errmsg (str) —— 错误的信息 return: modelbox.Status **str:** \"code: \" + StrCode() + \", errmsg: \" + errmsg_ **bool:** bool(status) example: import modelbox status = modelbox.Status() print(status) status1 = modelbox.Status(modelbox.Status.StatusCode.STATUS_CONTINUE) print(status1) status2 = modelbox.Status(False) print(status2) status3 = modelbox.Status(modelbox.Status.StatusCode.STATUS_CONTINUE, \"continue\") print(status3) status4 = modelbox.Status(status2, \"continue\") print(status4) print(bool(status)) result: SuccessContinue operationFaultcode: Continue operation, errmsg: continuecode: Fault, errmsg: continueTrue modelbox.Status.code 获取status的状态信息。 args: 无 return: modelbox.StatusCode example: import modelbox status = modelbox.Status() print(status.code()) result: StatusCode.STATUS_SUCCESS modelbox.Status.str_code 获取status的状态信息字符串。 args: 无 return: str example: import modelbox status = modelbox.Status() print(status.str_code()) result: Success modelbox.Status.StatusCode 状态码： code str_code modelbox.Status.StatusCode.STATUS_SUCCESS Success modelbox.Status.StatusCode.STATUS_FAULT Fault modelbox.Status.StatusCode.STATUS_AGAIN Try again modelbox.Status.StatusCode.STATUS_NOSPACE No space left modelbox.Status.StatusCode.STATUS_ALREADY Operation already in progress modelbox.Status.StatusCode.STATUS_NOSTREAM Out of streams resources modelbox.Status.StatusCode.STATUS_BADCONF Bad config modelbox.Status.StatusCode.STATUS_NOTFOUND Not found modelbox.Status.StatusCode.STATUS_BUSY Device or resource busy modelbox.Status.StatusCode.STATUS_NOTSUPPORT Not supported modelbox.Status.StatusCode.STATUS_CONTINUE Continue operation modelbox.Status.StatusCode.STATUS_OVERFLOW Value too large for defined data type modelbox.Status.StatusCode.STATUS_EDQUOT Quota exceeded modelbox.Status.StatusCode.STATUS_PERMIT Operation not permitted modelbox.Status.StatusCode.STATUS_EOF End of file modelbox.Status.StatusCode.STATUS_RANGE Out of range modelbox.Status.StatusCode.STATUS_EXIST Already exists modelbox.Status.StatusCode.STATUS_RESET Request reset modelbox.Status.StatusCode.STATUS_SHUTDOWN Shutdown operation modelbox.Status.StatusCode.STATUS_INPROGRESS Operation now in progress modelbox.Status.StatusCode.STATUS_STOP Stop operation modelbox.Status.StatusCode.STATUS_INTERNAL Internal error modelbox.Status.StatusCode.STATUS_INVALID Invalid argument modelbox.Status.StatusCode.STATUS_TIMEDOUT Operation timed out modelbox.Status.StatusCode.STATUS_NOBUFS No buffer space available modelbox.Status.StatusCode.STATUS_NODATA No data available modelbox.Status.StatusCode.STATUS_NOMEM Out of memory modelbox.Status.StatusCode.STATUS_NOENT No such file or directory modelbox.Status.set_errormsg 设置错误信息。 args: errmsg: str 错误信息 return: 无 modelbox.Status.errormsg 获取错误信息。 args: 无 return: str 当前status的errmsg modelbox.Status.wrap_errormsgs 获取status的所有子层次的错误信息。 args: 无 return: str 当前status的子层次错误信息 modelbox.Status.unwrap 返回wrap的status。 args: 无 return: modelbox.Status wrap的status example: import modelbox status = modelbox.Status(False) status.set_errormsg(\"test failed\") print(status.errormsg()) status1 = modelbox.Status(status, \"test failed outside\") print(status1.wrap_errormsgs()) status2 = status1.unwrap() print(status == status2) result: test failedFault, test failed outside -> test failedTrue ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "},"faq/faq.html":{"url":"faq/faq.html","title":"FAQ","keywords":"","body":"FAQ ModelBox 什么是ModelBox，ModelBox有什么功能？ ModelBox是一个AI应用的推理框架，ModelBox通过编排和插件化的形式支持AI应用的开发，支持的数据有视频，音频，语音，文本，通用数据的编排处理，ModelBox的主要功能列表可参考这里 相比直接调用底层API开发AI业务，ModelBox有什么优势？ ModelBox主要聚焦解决AI应用开发的问题，相比直接调用底层API，开发者需要关注每个底层的API使用方法，关注并发，关注GPU，NPU设备编程接口，关注TensorRT，TensorRT等推理框架的编程API，与云计算结合的接口，和分布式通信，日志等琐碎而复杂的周边代码。 ModelBox解决的就是业务开发的周边问题，将周边问题交由ModelBox处理，ModelBox通过对内存，CPU，GPU，周边组件的精细化管理，使AI推理业务开发更高效，性能也更高，质量也更好。 ModelBox目前支持哪些框架训练的模型（TensorFlow、Caffe、LibTorch、MindSpore等） ModelBox框架里面包含了支持TensorFlow, Caffe, LibTorch、MindSpore模型运行所需的功能单元Flowunit，我们称为推理功能单元(Inference Flowunit)，这些推理功能单元可以直接加载对应的模型文件，而不需要编写代码，只需提供一个简单的配置文件，即可将模型引入到ModelBox的流程中。目前支持的模型有TensorFlow, TensorRT, Ascend ACL模型。 ModelBox组件 ModelBox程序包含哪些部分 ModelBox目前包含如下组件 微服务ModelBox Server 运行库ModelBox Library 维护工具ModelBox Tool CPU相关的功能单元 Huawei Ascend相关的功能单元 CUDA相关的功能单元 可视化编辑工具 ModelBox支持服务式吗？ ModelBox有专门的微服务程序，ModelBox Server，ModelBox Server内置了通用的管理插件，和基本功能，开发者只需要配置ModelBox Server即可启动微服务。 如何调试ModelBox程序 ModelBox本身为C++代码编写，开发者可以通过如下方式调试ModelBox程序和相关的功能单元： GDB，IDE等工具调试 ModelBox运行日志。 ModelBox Profiling性能统计工具。 具体操作方法，可参考调试定位章节内容 模型问题 TensorRT在解析模型出错 当TensorRT在解析模型出错时，如果报错 \" expecting compute x.x got compute 7.5, please rebuild\"，说明模型和推理引擎不配套，需要转换模型到配套的硬件, 并在与当前环境配置相同的环境上重新编译模型。 查看当前的镜像对应的欧拉系统的版本 cat /etc/EulerLinux.conf 开发常见问题 功能单元找不到 常见日志报错：code: Not Found, errmsg: create flowunit 'xxx' failed., 通常有如下几个原因，可一一进行排查： 流程图toml配置的driver路径不包含所需功能单元so或文件夹； 功能单元编译异常，找不到符号。通常会在日志前面扫描到so时提示错误，可通过ldd、c++filt命令进行具体定位； 端口未连接 常见日志报错：code: Bad config, errmsg: flowunit 'xxxx' config error, port not connect correctly, 可能错误原因：代码中功能单元定义的端口，未在toml图中使用。需检查定义输入输出端口名称、或driver路径是否错误。 创建图失败 常见日志报错：code: Invalid argument, errmsg: check stream fail., build flow failed等, 通常有以下几种可能错误： toml流程图中，node定义节点在后面边连接中未使用； 端口冲突，如多个输出连接到同一输入端口（if-else终点除外）； if-else功能单元之后的各分支，最终需要合并到一个端口上； if-else分支中不能使用stream类型功能单元； 多个if-else功能单元不能共用同一个终点； 功能单元端口连接不能跨越if-else，拆分合并功能单元； 具体约束原因详见stream流章节。 ©2022 ModelBox Team all right reserved，powered by Gitbook文件修订时间： 2022-10-26 03:00:02 "}}