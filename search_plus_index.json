{"./":{"url":"./","title":"介绍","keywords":"","body":"ModelBox简介 ModelBox是一个专门为开发者，研究人，学生提供的一个易于使用，高效，高扩展的AI推理开发框架，可以帮助开发者快速完成AI推理应用的开发和上线工作。 ModelBox特点 易于开发，AI推理业务可视化编排开发，功能模块化，丰富组件库；c++，python，java多语言支持。 易于集成，集成云上对接的组件，云上对接更容易。 高性能，高可靠，pipeline并发运行，数据计算智能调度，资源管理调度精细化，业务运行更高效。 软硬件异构， CPU，ARM，GPU，NPU多异构硬件支持，资源利用更便捷高效。 全场景，视频，语音，文本，NLP全场景，专为服务化定制，云上集成更容易，端边云数据无缝交换。 易于维护，服务运行状态可视化，应用，组件性能实时监控，优化更容易。 ModelBox主要功能 功能 说明 主要业务场景 快速完成AI推理业务的开发工作 多种数据处理 视频，音频，图片，通用数据，其他 庞大用户群 软件开发者，研究人员，学生 跨平台 服务器， 边侧设备，嵌入式设备 图形化编排 支持模型的串联，支持视频流，音频流，图片等推理 API列表 C++SDK, PYTHON SDK, JAVA SDK(暂未支持) 支持OS Linux, andriod(暂未支持),iOS（暂未支持) 支持硬件 X86, ARM, GPU, NPU, ... 图可视化 编排开发可视化图，子图 性能调测 性能跟踪。 图能力 支持条件分支，循环分支，splice，reduce等图能力 分布式 支持分布式图处理，分布式动态调整业务执行 一次开发，多处运行 PYTHON功能单元，C++功能单元，java功能单元 完善的功能单元 包含了大部分高性能的基础功能单元，包括http，视频，音频，图像，云相关的功能单元 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"qs-modelbox/ModelBox使用简介.html":{"url":"qs-modelbox/ModelBox使用简介.html","title":"快速入门","keywords":"","body":"ModelBox使用简介 ModelBox是一个专门为开发者，研究人，学生提供的一个易于使用，高效，高扩展性的AI推理开发框架，可以帮助开发者快速完成AI推理应用的开发和上线工作。 ModelBox的目标就是解决AI开发者在开发AI应用时的编程复杂度，降低AI应用的开发难度，将复杂的数据处理、并发互斥、多设备协同、组件复用、数据通信交由ModelBox处理。开发者主要聚焦业务逻辑本身，而不是AI 软件推理实现细节。 在提高AI应用开发效率的同时，保证软件应用推理的性能、可靠性、跨平台、易扩展等属性。 常用概念 使用ModelBox前，请提前了解ModelBox相关概念和介绍。 图 1 ModelBox核心概念 流程图 ModelBox中用流程图(Graph)来表达应用逻辑。采用有向图的方式，将应用的执行逻辑表达为顶点和边，其中顶点表示了应用的某个数据处理逻辑单元，边则表示了逻辑单元之间的数据传递关系。在ModelBox中，针对流程图的开发，既可以使用文本方式直接编辑，也可以使用可视化的编辑器进行编辑。对于流程图的表述，ModelBox默认采用Graphviz进行解释，即图的表述需要满足Graphviz的语法要求。 功能单元 ModelBox将流程图中的顶点称为功能单元。功能单元是应用的基本组成部分，也是ModelBox的执行单元。在ModelBox中，内置了大量的基础功能单元，开发者可以将这些功能单元直接集成到应用流程图中，这也是基于流程图开发的一大好处。除内置功能单元外，ModelBox支持功能单元的自定义开发，支持的功能单元形式多样，如C/C++动态库、Python脚本、模型+Toml配置文件等。 接收数据处理请求 应用流程图构建完毕后，需要数据处理请求才能触发应用运行。ModelBox提供两种数据处理请求接收的方式：在功能单元中，通过在加载时调用API产生数据处理的请求，因为产生的请求是固定的，所以一般用于调试场景；标准使用方式是使用ModelBox提供的服务插件机制，在插件中接收外部请求，并调用任务相关的API来完成数据处理的请求。ModelBox提供了默认的服务插件可用于参考。数据处理请求的创建请详见数据流。 ModelBox 在应用构建完成后，结合ModelBox的框架才能形成完整可运行的应用。ModelBox作为应用入口，首先进行功能单元的扫描加载、应用流程图读取构建，然后接收数据处理请求，数据触发ModelBox中的执行引擎对功能单元进行调度，最终完成请求的数据处理任务。 使用流程 本文档以一个简单的应用为例，帮助您快速熟悉端到端使用ModelBox开发应用的流程。本文档开发的应用即打开一个mp4视频文件，推送到RTSP服务器，然后在PC端使用PotPlayer播放该mp4视频文件。 步骤一：启动ModelBox开发镜像 步骤二：远程连接ModelBox 步骤三：开发ModelBox应用 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"qs-modelbox/步骤一-启动ModelBox开发镜像.html":{"url":"qs-modelbox/步骤一-启动ModelBox开发镜像.html","title":"2 步骤一-启动ModelBox开发镜像","keywords":"","body":"步骤一：启动ModelBox开发镜像 准备工作 已下载SSH登录工具，如PuTTY。 操作步骤 运行PuTTY，单击“Session“，在“Host Name(or IP address)“的输入框中输入服务器IP，在“Port“输入框中输入端口号，如图1所示。 图 1 使用PuTTY登录服务器 单击“open“。 登录端侧设备。 输入服务器账户密码，登录服务器。 根据服务器端硬件规格，在服务器选择合适的文件夹下载ModelBox开发镜像。 X86+GPU服务器，执行如下命令下载镜像 docker pull modelbox/modelbox_cuda101_develop ARM+D310服务器（如Atlas500），执行如下命令下载镜像 docker pull modelbox/modelbox_ascend_aarch64_develop 在服务器中选择合适的文件夹创建如下docker启动脚本，或在本地将如下脚本按需修改后，粘贴到ssh终端中执行。 说明： 脚本中注明[modify]的地方都可以根据自己的需要修改。 X86+GPU服务器可使用如下脚本。 #! /bin/bash # ssh map port [modify] SSH_MAP_PORT=50011 # editor map port [modify] EDITOR_MAP_PORT=1104 # http server port [modify] HTTP_SERVER_PORT=7788 # container name [modify] CONTAINER_NAME=\"modelbox_instance_`date +%s`_xxx\" # xxx可自定义。 HTTP_DOCKER_PORT_COMMAND=\"-p $HTTP_SERVER_PORT:$HTTP_SERVER_PORT\" sudo docker run -itd --gpus all \\ -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,video \\ --tmpfs /tmp \\ --tmpfs /run \\ -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ --name $CONTAINER_NAME \\ -v /opt/modelbox:/opt/modelbox \\ -v /home:/home \\ -p $SSH_MAP_PORT:22 \\ -p $EDITOR_MAP_PORT: 1104 $HTTP_DOCKER_PORT_COMMAND \\ registry-cbu.huawei.com/modelbox/euler/modelbox_cuda101_develop:v1.0.8 /usr/sbin/init ARM+D310服务器可使用如下脚本。 #!/bin/bash # ssh map port, [modify] SSH_MAP_PORT=50011 # editor map port [modify] EDITOR_MAP_PORT=1144 # http server port [modify] HTTP_SERVER_PORT=7788 # container name [modify] CONTAINER_NAME=\"modelbox_instance_arm64_`date +%s`_xxx\" # xxx可自定义。 HTTP_DOCKER_PORT_COMMAND=\"-p $HTTP_SERVER_PORT:$HTTP_SERVER_PORT\" sudo docker run -itd --privileged --cap-add=SYS_PTRACE \\ --tmpfs /tmp --tmpfs /run -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ --device=/dev/davinci0 --device=/dev/davinci_manager --device=/dev/hisi_hdc --device=/dev/devmm_svm \\ --name $CONTAINER_NAME -v /opt/modelbox:/opt/modelbox -v /home:/home \\ -p $SSH_MAP_PORT:22 -p $EDITOR_MAP_PORT:1144 $HTTP_DOCKER_PORT_COMMAND \\ registry-cbu.huawei.com/modelbox/euler/modelbox_ascend_aarch64_develop:v1.0.8 /usr/sbin/init 执行如下命令查看本服务器已启动的所有镜像，保存新启动的镜像ID。 docker ps –a|grep modelbox 镜像ID在第一列，如图2所示。 图 2 查看镜像ID 执行如下命令进入ModelBox。 docker exec -it $docker_id bash # 其中$docker_id 指新启动的镜像ID。 执行如下命令修改root用户密码。密码要求至少三类字符组合，如数字、字母、特殊符号。 passwd root 退出容器，安装RTSP服务器，用于推送实时的RTSP视频流。 X86+GPU服务器选择合适的文件夹下载EasyDarwin，下载后解压文件，执行start.sh启动RTSP服务器。 图 3 下载EasyDarwin ARM+D310服务器选择合适的文件夹下载rtsp-simple-server，下载后解压得到可执行文件和配置文件。 图 4 下载rtsp-simple-server 打开rtsp-simple-server.yml可修改RTSP服务相关的配置参数，如端口号默认为8554，可以根据需要修改。 图 5 rtsp-simple-server.yml 修改后，执行如下命令后台启动RTSP服务器。 nohup ./rtsp-simple-server & 说明： 除了使用本地服务器启动ModelBox开发镜像，也可以选择使用ModelArts在线方式启动ModelBox开发镜像，详情可参考ModelArts创建Notebook启动镜像。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"qs-modelbox/步骤二-远程连接ModelBox.html":{"url":"qs-modelbox/步骤二-远程连接ModelBox.html","title":"3 步骤二-远程连接ModelBox","keywords":"","body":"步骤二：远程连接ModelBox 准备工作 已下载并安装Visual Studio Code。 说明： 请下载1.57.1及以下版本的Visual Studio Code。 操作步骤 打开Visual Studio Code，单击左侧图标，搜索并安装Remote-SSH、Remote-Containers、Docker等插件。 图 1 安装插件 单击左侧图标，鼠标移至“SSH TARGETS“右侧的，在右侧搜索框中选择“config“。 使用Remote-SSH添加连接。 图 2 配置ModelBox 配置ModelBox容器的远程连接，如图2所示。 Host：用户可自定义本次远程连接的名称。 HostName：服务器IP。 port：与步骤一：启动ModelBox开发镜像创建docker时“SSH_MAP_PORT“的值一致。 User：填写“root“。 配置完，单击“SSH TARGETS“下方出现的图标。 新弹出一个窗口。 图 3 ModelBox远程连接 在新窗口，按提示输入ModelBox容器root帐号的密码，即步骤一：Docker开发镜像中修改后的新密码。 等待一会，即可连接上ModelBox容器。 连接成功后，单击“Open folder“可直接打开ModelBox容器中的文件。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"qs-modelbox/步骤三-开发ModelBox应用.html":{"url":"qs-modelbox/步骤三-开发ModelBox应用.html","title":"4 步骤三-开发ModelBox应用","keywords":"","body":"步骤三：开发ModelBox应用 开发流程图 开发功能单元 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"qs-modelbox/开发流程图.html":{"url":"qs-modelbox/开发流程图.html","title":"4.1 开发流程图","keywords":"","body":"开发流程图 本章节以开发一个简单的应用为例，介绍如何在Visual Studio Code开发ModelBox应用，该应用即打开一个mp4视频文件，推送到RTSP服务器，然后在PC端使用PotPlayer播放该mp4视频文件。 准备工作 在PC端安装PotPlayer播放器，用于播放RTSP视频流。 操作步骤 在步骤一：启动ModelBox开发镜像已下载ModelBox的服务器中创建“toml“格式文件，用来描述流程图。 ModelBox会根据流程图构建应用处理逻辑。 [driver] dir = [\"/usr/local/\"] skip-default = false [profile] profile=false trace=false dir=\"/tmp/\" [log] level=\"DEBUG\" [graph] format = \"graphviz\" graphconf = \"\"\"digraph test { node [shape=Mrecord] queue_size = 16 video_input[type=flowunit, flowunit=video_input, device=cpu, deviceid=0, source_url=\"XXX/XXX/XXX.mp4\"] videodemuxer[type=flowunit, flowunit=videodemuxer, device=cpu, deviceid=0] videodecoder[type=flowunit, flowunit=videodecoder, device=cpu, deviceid=0, pix_fmt=rgb] videoencoder[type=flowunit, flowunit=videoencoder, device=cpu, deviceid=0, default_dest_url=\"rtsp://IP:PORT/stream\", format=rtsp] video_input:stream_meta -> videodemuxer:stream_meta videodemuxer:video_packet -> videodecoder:video_packet videodecoder:frame_info -> videoencoder: frame_info }\"\"\" [flow] desc = \"test for video streams\" 其中，“source_url“和“default_dest_url“需要根据实际情况自己配置。 “XXX/XXX/XXX.mp4“：实际mp4视频文件存放路径。 “IP“：服务器IP。 “PORT“：步骤一：Docker开发镜像中安装的RTSP服务器配置端口号。 例如source_url=\"home/test.mp4\"、default_dest_url=\"rtsp://10.10.10.100:8554/stream\"。 说明： 如果启动ModelBox开发镜像使用的是ModelArts创建Notebook启动镜像，“default_dest_url“中的“IP:PORT“配置成localhost即可，例如default_dest_url=\"rtsp://localhost/stream\"。 ModelBox使用graphviz格式描述流程图，将流程图定义内容拷贝到graphhiz工具中进行查看。 图 1 应用流程图-0 在Visual Studio Code执行如下命令，执行流程图。 modelbox-tool -verbose -log-level INFO flow -run xxx/xxx.toml 其中，“xxx/xxx.toml“为“toml“格式文件的实际存放路径。 打开浏览器，输入“toml“格式文件中配置的“default_dest_url“地址，选择对话框中“打开PotPlayer专用播放“。 弹出的PotPlayer将会播放“toml“格式文件中配置的mp4视频文件。 图 2 视频播放-0 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"qs-modelbox/开发功能单元.html":{"url":"qs-modelbox/开发功能单元.html","title":"4.2 开发功能单元","keywords":"","body":"开发功能单元 开发流程图所开发的应用使用的是ModelBox自带的功能单元，本章节在此基础上使用Python语言开发一个简单的功能单元，嵌入到应用中，即在视频页面左上方写上“Hello World“。 在Visual Studio Code执行如下命令，在指定路径下基于Python功能单元模板生成“HelloWorld“功能单元。 modelbox-tool create -t python -n HelloWorld -d ./examples/flowunits 其中，“./examples/flowunits“为存放功能单元的实际路径。 生成的功能单元包括“.py“文件和“.toml“配置文件。 图 1 功能单元 “.py“文件描述了功能单元的处理逻辑，在“.py“文件中增加OpenCV与numpy包的引用，修改其中的process函数，具体如下。 import cv2 import numpy as np … def process(self, data_context): # Process the data in_data = data_context.input(\"Input\") out_data = data_context.output(\"Output\") for buffer in in_data: width = buffer_img.get('width') height = buffer_img.get('height') channel = buffer_img.get('channel') img_data = np.array(buffer_img.as_object(), copy=False) img_data = img_data.reshape((height, width, channel)) cv2.putText(img_data, 'Hello World', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2) out_buffer = self.create_buffer(img_data) out_buffer.copy_meta(buffer_img) out_data.push_back(out_buffer) return modelbox.Status.StatusCode.STATUS_SUCCESS “.toml“配置文件配置该功能单元的名称、类别、输入输出端口等信息，当前不用修改。 修改在开发功能单元创建的流程图“toml“格式文件，具体内容如下。 [driver] dir = [\"/usr/local/\" , \"path-to-HelloWorld-flowunits\"] skip-default = false [profile] profile=false trace=false dir=\"/tmp/\" [log] level=\"DEBUG\" [graph] format = \"graphviz\" graphconf = \"\"\"digraph test { node [shape=Mrecord] queue_size = 16 video_input[type=flowunit, flowunit=video_input, device=cpu, deviceid=0, source_url=\"xxx/xxx.mp4\"] videodemuxer[type=flowunit, flowunit=videodemuxer, device=cpu, deviceid=0] videodecoder[type=flowunit, flowunit=videodecoder, device=cpu, deviceid=0, pix_fmt=rgb] HelloWorld[type=flowunit, flowunit=HelloWorld, device=cpu, deviceid=0] videoencoder[type=flowunit, flowunit=videoencoder, device=cpu, deviceid=0, default_dest_url=\"rtsp://IP:PORT/stream\", format=rtsp] video_input:stream_meta -> videodemuxer:stream_meta videodemuxer:video_packet -> videodecoder:video_packet videodecoder:frame_info -> HelloWorld: Input HelloWorld: Output -> videoencoder: frame_info }\"\"\" [flow] desc = \"test for video streams\" 其中， [driver]配置项的“dir“中添加了HelloWorld功能单元的路径“path-to-HelloWorld-flowunits“，根据实际路径配置。 “source_url“和“default_dest_url“需要根据实际情况自己配置，和开发功能单元创建的“toml“格式文件内容一致。 ModelBox使用graphviz格式描述流程图，将流程图定义内容拷贝到graphhiz工具中进行查看。 图 2 应用流程图-1 在Visual Studio Code执行如下命令，执行流程图。 modelbox-tool -verbose -log-level INFO flow -run xxx/xxx.toml 其中，“xxx/xxx.toml“为“toml“格式文件的实际存放路径。 打开浏览器，输入“toml“格式文件中配置的“default_dest_url“地址，选择对话框中“打开PotPlayer专用播放“。 弹出的PotPlayer将会播放“toml“格式文件中配置的mp4视频文件。 图 3 视频播放-1 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"qs-modelbox/ModelArts创建Notebook启动镜像.html":{"url":"qs-modelbox/ModelArts创建Notebook启动镜像.html","title":"A ModelArts创建Notebook启动镜像","keywords":"","body":"ModelArts创建Notebook启动镜像 用户可在ModelArts开发环境中一键创建ModelBox实例并通过VsCode远程连接实例，方便地利用云上的GPU/D310资源进行推理应用开发和调试。当前在ModelArts控制台“上海一“区域以白名单形式开放。 准备工作 登录华为云ModelArts“上海一“区域。如果访问不了可以提工单申请白名单。 在华为云创建访问秘钥（AK和SK），用于对请求加密签名，确保请求的机密性、完整性和请求双方身份的正确性。 访问秘钥在第一次创建时会自动下载，此后使用相同的秘钥时不会再有下载页面，请保存好。 创建Notebook实例 登录ModelArts管理控制台，在左侧导航栏中选择“开发环境 > Notebook“，进入“Notebook“新版管理页面。 图 1 进入新版Notebook 单击“前往新版“，进入Notebook2.0界面，如所示。 图 2 进入新版Notebook 单击“创建“，进入“创建Notebook“页面，请参见如下说明填写参数。 填写Notebook基本信息，包含名称、描述、是否自动停止，详细参数请参见表1。 图 3 Notebook基本信息 表 1 基本信息的参数描述 参数名称 说明 “名称” Notebook的名称。只能包含数字、大小写字母、下划线和中划线，长度不能超过20位且不能为空。 “描述” 对Notebook的简要描述。 “自动停止” 默认开启，且默认值为“1小时后”，表示该Notebook实例将在运行1小时之后自动停止，即1小时后停止计费。 开启自动停止功能后，可选择“1小时后”、“2小时后”、“4小时后”、“6小时后”或“自定义”几种模式。选择“自定义”模式时，可指定1~24小时范围内任意整数。 填写Notebook详细参数，如工作环境、资源规格等，详细参数请参见ModelArts创建Notebook启动镜像。 图 4 Notebook实例的详细参数 图 5 Notebook实例的详细参数 表 2 Notebook实例的详细参数说明 参数名称 说明 “镜像” 支持公共镜像和自定义镜像。 公共镜像：即预置在ModelArts内部的AI框架。自定义镜像：可以将基于公共镜像创建的实例保存下来，作为自定义镜像使用。 一个镜像对应支持一种AI引擎，不可以在同一个Notebook实例中切换AI引擎。 由于开发应用使用推理框架ModelBox，因此创建Notebook实例时选择ModelBox镜像。 “资源池” “公共资源池”无需单独购买，即开即用，按需付费，即按您的Notebook实例运行时长进行收费。 “公共资源池”本次版本暂不支持，将在下个版本开放。 “专属资源池”需要单独购买并创建。 “类型” 芯片类型包括CPU、GPU和Ascend类型。 不同的镜像支持的芯片类型不同，根据实际需要选择。 GPU性能更佳，但是相对CPU而言，费用更高。 “规格” 根据选择的芯片类型不同，可选资源规格也不同。 CPU规格“2核8GB”：Intel CPU通用规格，用于快速数据探索和实验 “8核32GB”：Intel CPU算力增强型，适用于密集计算场景下运算 GPU规格“GPU: 1*V100(32GB)|CPU: 8 核 64GB”：NVIDIA V100 GPU单卡规格，32GB显存，适合深度学习场景下的算法训练和调测 Ascend规格“Ascend: 1*Ascend 910|CPU: 24 核 96GB”：昇腾910(32GB显存)单卡规格，配搭ARM处理器，适合深度学习场景下的模型训练和调测 镜像选择ModelBox镜像时，可选规格为“GPU: 1*V100(32GB)|CPU: 8 核 64GB”。 “资源池规格” 按实际情况选择已创建的专属资源池。 “规格” 支持Ascend规格：“Ascend: 1*Ascend 910|CPU: 24 核 96GB” 昇腾910(32GB显存)单卡规格，配搭ARM处理器，适合深度学习场景下的模型训练和调测 “存储配置” 存储配置可选“默认存储”和“云硬盘EVS”。 选择“默认存储”作为存储位置。公共资源池模式下支持，专属资源池模式下不支持。 选择此模式，平台免费为每一个Notebook提供50GB的默认存储。 选择“云硬盘EVS”作为存储位置根据实际使用量设置磁盘规格。磁盘规格默认5GB。磁盘规格的取值范围为5GB～4096GB。 从Notebook实例创建成功开始，直至实例删除成功，磁盘每GB按照规定费用收费。 “默认存储”和“云硬盘EVS”的存储路径挂载在/home/ma-user/work目录下。用户在Notebook实例中的所有文件读写操作都是针对该存储目录下的的内容操作，与OBS无关。 停止或重启Notebook实例时，内容会被保留，不丢失。 删除Notebook实例时，内容不保留。 “SSH远程开发” 开启此功能后，用户可以在本地开发环境中远程接入Notebook实例的开发环境。 开发应用时支持本地SSH远程连接，开启“SSH远程开发”功能。 “密钥对” 开启“SSH远程开发”功能后，需要设置此参数。 可以选择已有密钥对。 也可以单击密钥对右侧的“立即创建”，跳转到数据加密控制台，在“密钥对管理 > 私有密钥对”页面，单击“创建密钥对”。 注意： 创建好的密钥对，请下载并妥善保存，使用本地IDE远程连接云上Notebook开发环境时，需要用到密钥对进行鉴权认证。 “远程访问白名单” 开启“SSH远程开发”功能后，需要设置此参数。 远程访问白名单设置为将要访问这个Notebook的IP地址，例如本地PC的IP地址或者访问机器的外网地址。 访问机器的外网地址可以在主流搜索引擎中搜索“IP地址查询”获取，如图6所示。 白名单IP地址如果配置错误将无法连接Notebook开发环境。 创建完Notebook后，可以在Notebook详情页中修改白名单IP地址。 图 6 IP地址查询 参数填写完成后，单击“下一步“进行规格确认。 参数确认无误后，单击“提交“，完成Notebook的创建操作。 进入Notebook列表，正在创建中的Notebook状态为“创建中“，创建过程需要几分钟，请耐心等待。当Notebook状态变为“运行中“时，表示Notebook已创建并启动完成。 在Notebook列表，单击实例名称，进入实例详情页，查看Notebook实例配置信息。 包括Notebook实例名称、规格、状态、镜像类型、用户ID、存储路径、存储容量、Notebook地址和端口号、允许远程访问的白名单IP地址、认证密钥文件名。 图 7 查看Notebook实例详情 在白名单右侧单击修改，可以修改允许远程访问的白名单IP地址。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"get-start/get-start.html":{"url":"get-start/get-start.html","title":"开始","keywords":"","body":"开始 本章将从概念、编译、第一个应用开发来对ModelBox进行初步介绍，让开发者对框架快速建立一个初步的认识。旨在帮助开发者在之后的开发中，快速掌握基于ModelBox的应用开发。 基于ModelBox的开发流程 首先总体介绍基于ModelBox的应用开发流程，之后的章节将会对开发中的每个步骤进行详细的讲解。 如上图所示，这是一个典型的基于ModelBox的应用开发流程，这个流程看起来与其他应用开发流程类似。这里我们简单介绍一下每个开发的步骤： 首先需要准备相关的开发环境，比如下载ModelBox的代码，在系统中进行编译安装，当然如果使用基于镜像的开发环境，这一步是可以省去的。 开发部分，主要关注点在于如何使用ModelBox的能力完成应用的功能，这里的图开发的概念在之后的章节会有详细的介绍。组件开发主要是基于ModelBox提供的sdk和接口约束进行应用功能的开发。服务集成开发则涉及到了ModelBox应用的运行模式。 在基本的代码开发完毕后，就是对应用的功能、性能测试。ModelBox提供了一些调试辅助工具协助开发者完成这项工作。 应用开发完毕后，一般可以作为服务对外提供功能，需要关注一些应用的配置以及应用的启动。 接下来我们将介绍ModelBox的基本概念。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"get-start/conception.html":{"url":"get-start/conception.html","title":"基本概念","keywords":"","body":"基本概念 本文将对ModelBox的基本概念进行介绍，希望在完成阅读后，对开发者ModelBox的整体认识有所帮助。 ModelBox解决的问题 目前AI应用开发时，需要将训练完成的模型和应用逻辑一起组成AI应用，并上线发布成为服务。在整个过程中，需要面临复杂的应用编程问题： 周边复杂的API使用 GPU，NPU等复杂的API 多线程并发互斥 服务化上线复杂 多种开发语言的配合 应用性能，质量不满足要求 ModelBox的目标就是解决AI开发者在开发AI应用时的编程复杂度，降低AI应用的开发难度，将复杂的数据处理，并发互斥，多设备协同，组件复用，数据通信，交由ModelBox处理。开发者主要聚焦业务逻辑本身，而不是软件细节。 在提高AI推理开发的效率同时，保证软件的性能，可靠性，安全性等属性。 ModelBox的核心概念 如图所示，开发者在使用ModelBox前，需要关注的基本核心概念包括：功能单元、流程图、和接收数据处理请求的部分（REST API、Service）。 流程图ModelBox中用流程图(Graph)来表达应用逻辑。采用有向图的方式，将应用的执行逻辑表达为顶点和边，其中顶点表示了应用的某个数据处理逻辑单元，边则表示了逻辑单元之间的数据传递关系。在ModelBox中，针对流程图的开发，既可以使用文本方式直接编辑，也可以使用可视化的编辑器进行编辑。对于流程图的表述，ModelBox默认采用Graphviz进行解释，即图的表述需要满足Graphviz的语法要求。 功能单元 ModelBox将流程图中的顶点称为功能单元(FlowUnit)。功能单元是应用的基本组成部分，也是ModelBox的执行单元。在ModelBox中，内置了大量的基础功能单元，开发者可以将这些功能单元直接集成到应用流程图中，这也是基于流程图开发的一大好处。除内置功能单元外，ModelBox支持功能单元的自定义开发，支持的功能单元形式多样，如C/C++动态库、Python脚本、模型+Toml配置文件等。 接收数据处理请求应用流程图构建完毕后，需要数据处理请求才能触发应用运行。ModelBox提供两种数据处理请求接收的方式：在flowunit中，通过在加载时调用API产生数据处理的请求，因为产生的请求是固定的，所以一般用于调试场景；标准使用方式是使用ModelBox提供的服务插件机制，在插件中接收外部请求，并调用任务相关的API来完成数据处理的请求。ModelBox提供了默认的服务插件可用于参考。数据处理请求的创建请详见数据流。 ModelBox在应用构建完成后，结合ModelBox的框架才能形成完整可运行的应用。ModelBox作为应用入口，首先进行功能单元的扫描加载、应用流程图读取构建，然后接收数据处理请求，数据触发ModelBox中的执行引擎对功能单元进行调度，最终完成请求的数据处理任务。 更多概念更详细的概念可以阅读框架概念章节的内容。 ModelBox的运行模式 ModelBox主要分为如下三部分组件： libmodelboxModelBox的核心部分，负责图加载、功能单元加载、设备加载、图的执行调度等功能，可以通过使用其提供的API单独集成此组件到已有系统中。 driversModelBox的设备插件及预置功能单元插件集合。 ModelBox包含ModelBox的启动、服务插件加载、预置的服务插件。这里的插件就是前文提到的服务插件，主要解决应用对接收数据处理请求的定制化实现需求。这部分组件包含了ModelBox的执行入口，启动后，会先加载服务插件，然后服务插件中使用任务接口创建出图，之后再由服务插件接收请求并在图中创建数据处理的请求。同时ModelBox组件中还包含了名为modelbox-tool的工具，此工具提供了许多调试的能力。 在理解了ModelBox的组成后，再来看一下ModelBox的运行模式： 携带服务插件运行：一般的启动方式，这种启动方式需要通过命令执行方式，并指定服务的配置文件，ModelBox会通过服务的配置文件，加载指定的服务插件，并通过服务插件与外部系统的交互来完成对ModelBox需要处理的数据请求管理。 常驻服务ModelBox可以通过注册系统服务的方式启动，与上一种方式的区别是ModelBox作为服务由系统拉起，其他部分没有区别。 不携带服务插件运行调试流程图时，往往不需要加载服务插件，只是希望验证流程图本身的正确性，此时可以借助到调试工具modelbox-tool来加载指定的流程图，进行启动验证，具体使用可以参见modelbox-tool章节。当然，此方法也适用于不依赖服务插件进行数据处理请求响应的应用，例如图中存在处理外部请求的功能单元的场景下，就可以直接使用modelbox-tool启动应用。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"get-start/compile.html":{"url":"get-start/compile.html","title":"编译安装","keywords":"","body":"编译安装 ModelBox采用C++语言编写，工程编译软件是CMake，在编译ModelBox之前，需要满足如下要求。 编译依赖 类别 依赖 依赖说明 最低版本 推荐版本 是否必须 相关组件 编译器 gcc gcc编译器 4.8 7.x 是 所有 编译器 g++ g++编译器 4.8 7.x 是 所有 编译器 Cmake Cmake工具 2.9 3.5 是 所有 OS Linux Linux操作系统 ubuntu16.04, centOS 7.2 ubuntu 18.04 是 所有 运行时 nodejs 前端编译 10.x V12.x 否 前端Editor 运行时 python python编译 3.x 3.8 否 python支持 开发库 cuda cuda支持 10.0 10.1 否 cuda支持 开发库 Ascend Ascend支持 否 Ascend支持 开发库 ffmpeg 视频解码编码支持 否 视频相关功能 开发库 tensorrt tensorrt模型推理 否 tensorrt相关的模型推理功能 开发库 tensorflow tensorflow推理支持 否 tensorflow相关的模型推理功能 开发库 cpprest http服务支持 是 modelbox-server以及http相关的功能组件 上述依赖可按需求选择，其中是否必须为“是”的依赖，必须要安装到编译环境中才能正常编译代码。如果使用基于镜像的开发环境，可以省去这一步。 Docker开发镜像 ModelBox项目提供了docker镜像，里面包含了ModelBox编译运行所需的组件及预先安装的ModelBox，可以优先选择docker镜像进行应用的开发编译。 安装启动docker后，执行下列命令下载内置了cuda10.1版本的docker镜像： docker pull modelbox/modelbox_cuda101_develop:latest 如果想要下载cuda10.2版本的镜像，可以选择使用以下命令 docker pull modelbox/modelbox_cuda102_develop:latest 下载镜像之后，执行下列命令启动镜像 如果docker版本大于等于19.03，那么可以使用以下命令： docker run -itd --gpus all -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,video \\ --tmpfs /tmp --tmpfs /run -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ modelbox/modelbox_cuda101_develop:latest 如果docker版本小于19.03, 则可以使用： docker run -itd --runtime=nvidia -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,video \\ --tmpfs /tmp --tmpfs /run -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ modelbox/modelbox_cuda101_develop:latest docker版本可通过执行docker version来查询。 如果需要在容器中进行gdb调试，需要在启动容器时添加如下选项： --​privileged 如果需要通过ssh连接至容器，需要在启动容器时添加如下选项： --p 22:[host port] 启动镜像之后，执行下列命令进入镜像即完成ModelBox开发镜像环境准备 docker exec -it [container id] bash 如有疑问，可参考FAQ中的docker相关内容 基于当前操作系统安装 如果不想下载开发镜像，那么也可按上述依赖列表，自行基于当前操作系统进行安装。 ubuntu操作系统 apt update apt install build-essential unzip ffmpeg cmake apt install python3-setuptools python3-wheel python3-numpy python3-opencv python3-pip apt install libssl-dev libcpprest-dev python3-dev libswscale-dev libavformat-dev graphviz-dev centos操作系统 yum update yum install ffmpeg cmake libcpprest 编译安装ModelBox 准备 编译ModelBox之前，需要准备好开发环境。或在镜像中进行编译，或按上述依赖列表，安装相应的依赖组件。 下载ModelBox代码 git clone http://code-cbu.huawei.com/ModelArts/Infer/BaseImages/modelbox.git cd modelbox 执行Cmake创建编译工作区 mkdir build cd build cmake -DLOCAL_PACKAGE_PATH=/opt/thirdparty/source .. 在编译过程中，还需要下载第三方依赖，请保持网络能正常连接第三方服务器。 如需编译release版本，可以执行如下cmake命令cmake -DCMAKE_BUILD_TYPE=Release .. 如需进行断点调试，则应编译debug版本，可以执行如下cmake命令cmake -DCMAKE_BUILD_TYPE=Debug .. -DLOCAL_PACKAGE_PATH：若本地已经有依赖的第三方软件包，则可以使用此参数指定本地依赖包路径，若使用ModelBox编译镜像时，编译镜像的/opt/thirdparty/source已经有相关依赖包，可直接指定本地路径使用，若需要从公共源码仓下载，则无需指定此参数，但需要确保网络通畅。 编译安装包 make package -j16 编译完成后，将在release目录下生成对应的安装包。 安装 ModelBox编译完成后，将生成配套OS安装的安装包，如deb、rpm包和tar.gz包，路径为编译目录的release子目录。可根据使用需求进行安装，下表是软件包的用途对照表。 安装包功能对照表 类型 名称 说明 运行库 modelbox-x.x.x-Linux-libmodelbox.[deb|rpm] modelbox核心运行库。 运行库 modelbox-x.x.x-Linux-graph-graphviz.[deb|rpm] 图解析组件。 服务组件 modelbox-x.x.x-Linux-server.[deb|rpm] modelbox server服务组件。 运行库 modelbox-x.x.x-Linux-ascend-device-flowunit.[deb|rpm] Ascend设备SDK以及配套基础功能单元组件。 运行库 modelbox-x.x.x-Linux-cpu-device-flowunit.[deb|rpm] Cuda设备SDK以及配套基础功能单元组件。 运行库 modelbox-x.x.x-Linux-cuda-device-flowunit.[deb|rpm] CPU设备SDK以及配套基础功能单元组件。 开发库 modelbox-x.x.x-Linux-libmodelbox-devel.[deb|rpm] modelbox开发库。 开发库 modelbox-x.x.x-Linux-server-devel.[deb|rpm] modelbox server服务插件开发库。 开发库 modelbox-x.x.x-Linux-ascend-device-flowunit-devel.[deb|rpm] Ascend设备开发库。 开发库 modelbox-x.x.x-Linux-cpu-device-flowunit-devel.[deb|rpm] CPU开发包。 开发库 modelbox-x.x.x-Linux-cuda-device-flowunit-devel.[deb|rpm] Cuda设备开发库。 运行库 modelbox-x.x.x-py3-none-any.whl python wheel包。 全量包 modelbox-x.x.x-Linux.tar.gz 全量安装包，包括上述所有组件。 安装包说明 modelbox运行库，cpu运行库，graphviz图解析组件必须安装。 ascend设备组件，cuda设备组件，可根据硬件配置情况合理安装。 modelbox-server服务组件，推荐安装。 modelbox-x.x.x-py3-none-any.whl在需要python运行时安装。 modelbox-x.x.x-Linux.tar.gz可解压直接使用。推荐使用安装包，方便管理。 安装命令说明 debian安装包 sudo dpkg -i *.deb rpm安装包 sudo rpm -i *.rpm python wheel包 pip install *.whl tar.gz包的使用（可选，如果已经安装了deb|rpm包，则可不用安装tar.gz包） tar xf modelbox-x.x.x-Linux.tar.gz 将解压后的目录，复制到/目录，此步骤可选。 cd modelbox cp * / -avf 若未选择上述步骤，未复制modelbox文件到/目录，则需要将/lib/systemd/system/modelbox.service中文件修改为对应的解压目录 ExecStart=[path/to]/modelbox -p /var/run/modelbox.pid -c [path/to]/modelbox.conf 将上述路径[[path/to]修改为对应解压后的路径。然后执行如下命令 cp modelbox.service /lib/systemd/system/modelbox.service systemctl daemon-reload 启动服务 如安装了modelbox-x.x.x-Linux-server，可以使用下述命令启动服务。 systemctl enable modelbox systemctl start modelbox 关于ailfow server服务的配置，请查阅运行服务章节。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"get-start/first-app.html":{"url":"get-start/first-app.html","title":"第一个应用","keywords":"","body":"第一个应用 环境准备 Docker开发镜像 安装启动docker后，执行下列命令下载docker镜像 docker pull modelbox/modelbox_cuda101_develop:latest 如需要下载其他cuda版本的镜像，可参考FAQ中的其他版本的cuda相关内容 在系统中创建如下docker启动脚本，或将如下脚本按需修改后，粘贴到ssh终端中执行： #!/bin/bash # ssh map port, [modify] SSH_MAP_PORT=50022 # editor map port [modify] EDITOR_MAP_PORT=1104 # http server port [modify] HTTP_SERVER_PORT=7778 # container name [modify] CONTAINER_NAME=\"modelbox_instance_`date +%s` \" HTTP_DOCKER_PORT_COMMAND=\"-p $HTTP_SERVER_PORT:$HTTP_SERVER_PORT\" docker run -itd --gpus all -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,video \\ --tmpfs /tmp --tmpfs /run -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ --name $CONTAINER_NAME -v /opt/modelbox:/opt/modelbox -v /home:/home \\ -p $SSH_MAP_PORT:22 -p $EDITOR_MAP_PORT:1104 $HTTP_DOCKER_PORT_COMMAND \\ modelbox/modelbox_cuda101_develop:latest 如果docker版本低于19.03，则需要修改脚本 docker run -itd --runtime=nvidia -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,video \\ --tmpfs /tmp --tmpfs /run -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ --name $CONTAINER_NAME -v /opt/modelbox:/opt/modelbox -v /home:/home \\ -p $SSH_MAP_PORT:22 -p $EDITOR_MAP_PORT:1104 $HTTP_DOCKER_PORT_COMMAND \\ modelbox/modelbox_cuda101_develop:latest 注意事项： 可使用vim start_docker.sh创建文件后，i进入编辑模式后，粘贴上述代码，编辑修改后，wx保存。 SSH_MAP_PORT: 为容器ssh映射端口号。 EDITOR_MAP_PORT: 为可视化开发界面链接端口号。 HTTP_SERVER_PORT: 为http flowunit默认服务端口号。 docker启动脚本中，请注意启动的镜像版本是否与自己所需的镜像版本一致。 如果启动镜像之后，端口未被占用却仍旧无法访问，需要检查防火墙。 如有疑问，可参考FAQ中的docker相关内容。 进入容器并且切换至modelbox开发者模式 docker exec -it [container id] bash modelbox-tool develop -e 然后按照提示，访问editor界面。 如果访问被拒绝，可参考运行编排服务中的ACL访问控制列表相关内容。 第一个应用开发 开发环境准备好了之后进入应用开发环节，这里以MNIST为例介绍整个应用开发过程。首先介绍MNIST应用实现的功能，然后介绍流程图编排、功能单元编写、运行与调试3个开发步骤。 MNIST案例是使用MNIST数据集，训练的一个手写数字识别tensorflow模型，搭建的一个简易的http请求服务。模型训练可参考tensorflow教程 。 功能 监听端口接收http请求，然后从请求体中的base64解析出图片，接着用训练出的MNIST模型进行推理，最后将识别出的数字返回给用户。 图片base64编码： import base64 img_path = \"path_to_test_image\" with open(img_path, 'rb') as fp: base64_data = base64.b64encode(fp.read()) img_base64_str = str(base64_data, encoding='utf8') 请求样例： { \"image_base64\": \"xxxxx\" } 响应样例： { \"predict_reuslt\": \"x\" } 流程图编排 流程图是编排整个应用的过程，可根据应用的逻辑进行编排，具体可参考流程图开发章节。有两种方式可编排流程图，第一种是使用UI进行可视化编排，第二种是直接编写图文件。这里采用第二种方式。 从上到下共有5个功能单元，分别为接收http请求，MNIST预处理，MNIST模型推理，MNIST响应构造，发送http响应。图定义如下。 [graph] format = \"graphviz\" graphconf = '''digraph mnist_sample { queue_size = 32 batch_size = 1 httpserver_sync_receive[type=flowunit, flowunit=httpserver_sync_receive, device=cpu, deviceid=0, label=\"\", request_url=\"http://localhost:7778\", max_requests=100, time_out=10] mnist_preprocess[type=flowunit, flowunit=mnist_preprocess, device=cpu, deviceid = 0, label=\" | \"] mnist_infer[type=flowunit, flowunit=mnist_infer, device=cuda, deviceid=0, label=\" | \"] mnist_response[type=flowunit, flowunit=mnist_response, device=cpu, deviceid=0, label=\" | \"] httpserver_sync_reply[type=flowunit, flowunit=httpserver_sync_reply, device=cpu, deviceid=0, label=\"\"] httpserver_sync_receive:Out_1 -> mnist_preprocess:In_1 mnist_preprocess:Out_1 -> mnist_infer: Input mnist_infer: Output -> mnist_response: In_1 mnist_response: Out_1 -> httpserver_sync_reply: In_1 }''' 再加上下面实现功能单元的路径即可完成流程图的编写。 开发镜像中已集成该图配置，可参考/usr/local/share/modelbox/solution/graphs/mnist_detection/mnist.toml。 功能单元编写 ModelBox提供基础功能单元，除此之外还需补充流程图中缺失的功能单元，具体开发可参考功能单元开发章节。 这里接收http请求、发送http响应两个功能单元ModelBox已提供，我们只需实现MNIST的预处理，推理，响应构造三个功能单元即可。 MNIST预处理功能单元 预处理需要做：解析出图片，对图片进行reshape，构建功能单元输出buffer。 in_data = data_context.input(\"In_1\") out_data = data_context.output(\"Out_1\") for buffer in in_data: # get image from request body request_body = json.loads(buffer.as_object().strip(chr(0))) img_base64 = request_body[\"image_base64\"] img_file = base64.b64decode(img_base64) # reshape img img = cv2.imdecode(np.fromstring(img_file, np.uint8), cv2. IMREAD_GRAYSCALE) img = cv2.resize(img, (28, 28)) infer_data = np.array([255 - img], dtype=np.float32) infer_data = np.reshape(infer_data, (784,)) / 255. # build buffer add_buffer = modelbox.Buffer(self.get_bind_device(), infer_data) out_data.push_back(add_buffer) 开发镜像中已集成该功能单元，可参考/usr/local/share/modelbox/solution/flowunit/mnist/mnist_preprocess。 MNIST推理功能单元 推理功能单元只需准备好模型和对应的配置文件即可。 配置文件如下： [base] name = \"mnist_infer\" device = \"cuda\" version = \"0.0.1\" description = \"Recognition handwritten digits recognition. The sample mnist_model.pb requires tensorflow1.13 + cuda10.0.\" entry = \"path_to_mnist_model.pb\" type = \"inference\" virtual_type = \"tensorflow\" [input] [input.input1] name = \"Input\" type = \"float\" [output] [output.output1] name = \"Output\" type = \"float\" 开发镜像中已集成该功能单元，可参考/usr/local/share/modelbox/solution/flowunit/mnist/mnist_infer。 MNIST响应功能单元 得到推理的结果之后，需要构造响应： in_data = data_context.input(\"In_1\") out_data = data_context.output(\"Out_1\") for buffer in in_data: # get result max_index = np.argmax(buffer.as_object()) # build response result = { \"predict_reuslt\": str(max_index) } result_str = (json.dumps(result) + chr(0)).encode('utf-8').strip() add_buffer = modelbox.Buffer(self.get_bind_device(), result_str) out_data.push_back(add_buffer) 开发镜像中已集成该功能单元，可参考/usr/local/share/modelbox/solution/flowunit/mnist/mnist_response。 运行与测试 首先需要把http服务运行起来，然后再模拟请求测试。 运行流程图 执行如下命令即可启动MNIST识别http服务： modelbox-tool -log-level info flow -run path_to_mnist.toml 由于开发镜像已集成样例，可开发镜像中直接运行modelbox-tool -log-level info flow -run /usr/local/share/modelbox/solution/graphs/mnist_detection/mnist.toml。 测试 这里已经准备好测试脚本/usr/local/share/modelbox/solution/graphs/mnist_detection/test_mnist.py，测试图片是mnist测试集中的0数字。 直接运行python3 test_mnist.py得到结果为： { \"predict_reuslt\": \"0\" } © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"server/server.html":{"url":"server/server.html","title":"运行服务","keywords":"","body":"Server服务 ModelBox Server是最基本也是最重要的服务，提供了服务化运行流程图和图形化进行UI编排的能力。 用户只需将flow流程图配置文件放到指定的目录下，即可实现flow作为服务的功能。 Server服务使用流程 Server服务是预编译好的可执行文件，在使用时，按照正常的服务流程使用，其流程为： 安装服务。 启动服务。 修改服务配置文件。 添加流程图。 管理扩展插件。 启动管理服务 ModelBox Server服务使用标准的systemd unit管理，启动管理服务，使用systemd命令管理。 通过如下命令对ModelBox服务进行操作： sudo systemctl status modelbox.service：查看ModelBox服务的状态。 sudo systemctl stop modelbox.service：停止ModelBox服务。 sudo systemctl start modelbox.service：启动ModelBox服务。 sudo systemctl restart modelbox.service：重启ModelBox服务。 ModelBox Server服务配置 ModelBox Serverf服务配置文件中包含主服务配置、插件、服务启动参数、编排服务配置和访问控制列表。 相关配置文件和配置功能说明如下： 配置类别 配置文件 说明 主服务配置 /usr/local/etc/modelbox/modelbox.conf 包含基本的配置信息，如插件路径，日志级别。 插件配置 /usr/local/etc/modelbox/modelbox.conf 和具体插件相关。 编排服务配置 /usr/local/etc/modelbox/modelbox.conf 包括编排服务的配置信息，详情可见运行服务中的运行编排服务 访问控制列表 /usr/local/etc/modelbox/modelbox.conf 可访问modelbox后端服务的白名单列表，详情可见运行服务中的ACL 服务启动参数配置 /usr/local/etc/modelbox/modelbox-opts 支持配置ModelBox Server服务的启动参数。 主服务配置项 主服务配置主要配置插件列表，日志级别信息，具体配置项如下： 配置项 配置功能 plugin.files Modelbox Server插件列表，顺序加载。 log.level ModelBox服务日志级别，默认为INFO，支持DEBUG, INFO, NOTICE, WARN, ERROR, FATAL, OFF，如果指定OFF，将关闭日志打印。 log.num ModelBox服务日志归档文件个数最大值，默认为32，当归档日志超过该阈值时，最旧归档日志文件将删除。 log.path ModelBox服务日志文件路径，默认为/var/log/modelbox/modelbox.log。如果修改该配置项，需要保证日志目录存在且具有可读写权限。 插件服务配置 ModelBox启动后，会按照plugin.files配置的插件，顺序加载插件，各插件的配置，参考插件配置参数，默认Modelbox plugin插件的配置，可参考运行流程图。 ModelBox服务中实现HTTP服务的可插拔模块，详见ModelBox服务插件。 ModelBox服务启动参数配置 ModelBox Server服务启动参数配置项目如下： 配置项 配置功能 MODELBOX_OPTS ModelBox服务启动时会加载该变量的内容作为启动参数。如果用户需要重新指定其他的ModelBox服务运行配置时，可修改该变量的值实现。 ModelBox Server预置功能列表 ModelBox Server可以通过自定义插件的形式扩展其基本功能，默认情况下，ModelBox Server集成了任务管理REST API服务，以及流程图的执行能力, ModelBox集成的插件列表。 插件 功能 说明 使用指导 modelbox-plugin 默认流程图执行插件 默认的流程图执行插件，支持REST API管理流程图，和其执行结果。 指导 ModelBox Server文件目录 ModelBox Server安装完成后，对应的安装目录如下 文件路径 说明 /usr/local/bin/modelbox modelbox独立服务器主进程。 /usr/local/etc/modelbox modelbox配置目录。 /usr/local/etc/modelbox/modelbox.conf modelbox主程序配置文件。 /usr/local/etc/modelbox/modelbox-opts modelbox主程序启动参数配置文件。 /usr/local/etc/modelbox/graph modelbox执行程序图存储目录。 /lib/systemd/modelbox.systemd modelbox服务启动systemd unit。 /usr/local/lib/libmodelbox-*.so libmodelbox，以及相关插件目录。 ModelBox Server运行日志 ModelBox Server运行时的日志会记录到/var/log/modelbox/modelbox.log文件中。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"server/editor.html":{"url":"server/editor.html","title":"运行编排服务","keywords":"","body":"运行编排服务 ModelBox提供了在线可视化编排的工具——Editor，在开发时，可使用此工具，提升开发效率。 编排服务是什么 编排服务是用来在Editor可视化界面上，编排流程图并自动生成相对应的图代码的快速开发工具。 编排服务开发使用流程 安装ModelBox server服务。 配置Modelbox Server。 配置启用编排服务。 浏览器访问Editor界面。 业务进行编排操作。 下发编排任务。 编排服务集成在ModelBox Server中，默认情况下，编排服务未启用。可以参考下方《编排服务配置》章节来启用编排服务并加载Editor界面。 编排服务配置 ModelBox Server安装完成后，编排服务会通过插件的形式由ModelBox Server加载，并在网页浏览器上提供在线可视化编排服务。 对应插件路径为\"/usr/local/lib/modelbox-plugin-editor.so\"。 编排服务插件的配置文件路径为/usr/local/etc/modelbox/modelbox.conf，其配置项目如下： 配置项目 配置说明 editor.enable 是否启用Editor工具 editor.ip Editor工具监听IP，默认为127.0.0.1。不指定的情况下，和server.ip一致 editor.port Editor工具监听端口，默认为1104，不指定情况下，和server.port一致 editor.root Editor前端UI路径，默认为/usr/local/share/modelbox/www editor.solution_graphs Editor solution_graphs路径，默认为/usr/local/share/modelbox/solution/graphs 下面分别介绍两种启用Editor的方法。 命令行启用Editor 通过如下命令，可开启基于Web的可视化编辑工具——Editor。 modelbox-tool develop -e 命令执行后，将开启http服务，可使用对应主机的IP地址，和开启的端口号（默认端口号为1104），访问Editor界面。 配置启用Editor 若需要定制化编排服务启动参数，可以修改配置文件，具体修改流程如下： 打开/usr/local/etc/modelbox/modelbox.conf，修改其中的配置项： [server] # 允许访问服务 ip = \"0.0.0.0\" port = \"1104\" flow_path = \"/usr/local/etc/modelbox/graph\" [plugin] # 确保Editor组件加载。 files = [ \"/usr/local/lib/modelbox-plugin-editor.so\" ] [editor] # 启用Editor enable = true # 设置绑定IP和端口。 ip = \"0.0.0.0\" port = \"1104\" # 指定前端UI路径，默认情况无需修改。 root = \"/usr/local/share/modelbox/www\" solution_graphs = \"/usr/local/share/modelbox/solution/graphs\" 重启ModelBox Server服务使配置生效。 systemctl restart modelbox 访问编排服务 服务启动成功后，可使用浏览器访问服务，输入对应的网址即可，如：http://[host]:1104/editor/，成功后，将显示如下界面： UI界面分为7个功能区域，其对应的功能如下： 区域1，功能页面选择。 区域2，基本编排操作区域，包含对6号区域的放大，缩小，重置大小，居中显示等操作。 区域3，基础组件列表区域，安装不同的组件分类，可从此面板选择对应编排的组件。组件数量受图的FlowUnit路径和服务器中功能单元插件个数的影响。 区域4，帮助和API页面的链接。 区域5，图操作功能区，包含新建，保持，图属性，选择图表，和解决方案功能。 区域6，图形化编排界面，使用鼠标可以控制组件链接和移动。Ctrl+鼠标左键可以拖动画布。 区域7，对应文本化编排界面，可使用标准的DOT语法进行文本编辑。 快捷键说明： 放大缩小：鼠标滚轮，或键盘，-，=按键。 全选：ctrl+a。 撤销：ctrl+z。 重做：ctrl+u。 取消选择：escape。 注意事项： 对应网址的端口号以docker启动脚本中的 EDITOR_MAP_PORT 为准，默认端口号为1104。 区域3中若无显示任务组件，请确保图设置界面中，选中了使用系统功能单元，和正确指定功能单元路径。 编排完成后，需要点击另存为保存图，然后才能在任务管理界面下发任务。 执行编排任务 编排完成，并保存完编排图后，可在编排管理界面下发编排任务，对应的编排任务管理界面如下： 任务界面分为4个功能区域，其对应的功能如下： 区域1，新建任务按钮，用于新建编排页面创建的图。 区域2，服务器端执行的任务列表。 区域3，任务执行状态。 区域4，任务执行出错情况下的错误信息。 ACL访问控制列表 访问控制列表ACL（Access Control List）是由一条或多条规则组成的集合，里面配置了允许访问Editor的IP地址。 可以通过修改配置文件，来修改ACL列表，具体流程如下： 打开/usr/local/etc/modelbox/modelbox.conf，修改其中的配置项： 假设打开编排UI的机器的IP地址为10.11.12.13。 [acl] allow = [ \"10.11.12.13\", ] [plugin] files = [ \"/usr/local/lib/modelbox-plugin.so\", \"/usr/local/lib/modelbox-plugin-editor.so\" ] 如果没有配置任何访问白名单，则允许所有人皆可访问。 # [acl] # allow = [ # \"10.11.12.13\", # ] 重启ModelBox Server服务使配置生效。 systemctl restart modelbox 注意：1. 确保[editor]下enable = true。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"server/run-flow.html":{"url":"server/run-flow.html","title":"运行流程图","keywords":"","body":"运行流程图 ModelBox Plugin插件 ModelBox Plugin插件用于对外提供服务，管理并运行流程图。此插件内置在Modelbox Server中。 默认情况下，可以直接使用此插件执行相关的流程图功能。例如，新建任务后，将使用插件运行流程图，并将结果对外输出。 ModelBox Plugin功能说明： ModelBox Plugin主要提供两个功能。 添加配置文件管理流程图。 调用REST API执行流程图。 ModelBox Plugin插件配置 ModelBox Plugin插件配置文件和ModelBox Server主配置文件相同，即为/usr/local/etc/modelbox/modelbox.conf, ModelBox Plugin插件的配置项目如下： 配置项目 配置说明 server.ip ModelBox Plugin绑定的管理IP地址，默认为127.0.0.1 server.port ModelBox Plugin绑定的管理端口，默认为1104 server.flow_path ModelBox Plugin加载flow配置文件的扫描路径。默认为/usr/local/etc/modelbox/graph 为确保ModelBox Plugin插件生效，请确保插件在/usr/local/etc/modelbox/modelbox.conf配置文件的plugin.files配置项中配置此插件，并在配置完成后，重启ModelBox服务。 添加配置文件管理流程图 ModelBox Plugin支持通过添加流程图配置文件的形式自动执行流程图，默认情况下的流程图配置文件路径为/usr/local/etc/modelbox/graph, 配置文件的存放目录为： /usr/local/etc/modelbox/graph |-some-flow1.toml |-some-flow2.toml . . . 配置文件复制到图存储目录后，可执行ModelBox Server服务重启命令systemctl restart modelbox生效。 注意： ModelBox服务加载该目录下的所有文件作为flow作业，如果加载失败将跳过该flow，文件名将作为flow的作业名。 文件名不要包含特殊符号，并且后缀名为.toml。 如果修改该配置项，需要保证指定的目录存在并具有读权限，否则将加载失败。 路径可通过server.flow_path参数修改。 图形化运行流程图 请参考运行编排服务。 REST API管理执行流程图 ModelBox Server启动后之后，ModelBox Plugin就开始对外提供服务，服务的endpoint为http://server.ip:server.port，其中server.ip和server.port为ModelBox服务运行配置中的配置项，默认为http://127.0.0.1:1104，服务的path为/v1/modelbox/job，业务可通过发送REST请求到插件管理流程图。 ModelBox服务当前提供动态增加flow作业，动态删除flow作业，查询所有flow作业列表，查询flow作业状态 增加flow作业 REST API URI: http://server.ip:server.port/v1/modelbox/job/ METHOD: POST REST API BODY { \"job_id\": \"flow2\", \"job_graph\": \"xxxxx\" } job_id： flow名字，用户自定义，建议不要包含特殊字符。 job_graph：toml格式的图信息，graph的配置详见图。 例子 命令：curl -X PUT --data @flow-example http://127.0.0.1:1104/v1/modelbox/job flow-example文件内容： { \"job_id\": \"flow-example\", \"job_graph\": { \"graph\": { \"format\":\"graphviz\", \"graphconf\": [ \" digraph demo { \", \" httpserver_sync_receive[type=flowunit, flowunit=httpserver_sync_receive, device=cpu, deviceid=0, label=\\\"\\\", request_url=\\\"http://localhost:54321/example\\\", max_requests=10, time_out=5]\", \" httpserver_sync_reply[type=flowunit, flowunit=httpserver_sync_reply, device=cpu, deviceid=0, label=\\\"\\\"]\", \" httpserver_sync_receive:out_request_info -> httpserver_sync_reply:in_reply_info, \" }\" ] }, \"driver\": { \"dir\": \"/usr/local/lib/\" } } } 返回值 正常返回HTTP code 201。 异常返回错误json： { \"error_code\": \"[some error code]\", \"error_msg\" : \"[some error message]\" } error_code：错误码，参考错误码。 error_msg：错误码对应的消息。 查询flow作业状态 REST API URI: http://server.ip:server.port/v1/modelbox/job/[flow-name] REST API RESPONSE 正常返回HTTP code 200，如下json： { \"job_status\": \"RUNNING\", \"job_id\": \"[flow-name]\" } job_status： flow的状态代码。 job_id：flow名称。 例子 命令：curl http://127.0.0.1:1104/v1/modelbox/job/flow-example 删除flow作业 REST API URI: http://server.ip:server.port/v1/modelbox/job/[flow-name] 返回值 正常返回HTTP code 204。 异常返回错误json。 例子 命令： curl -X DELETE http://127.0.0.1:1104/v1/modelbox/job/flow-example 查询所有flow作业列表 REST API URI: http://server.ip:server.port/v1/modelbox/job/list/all 例子 命令: curl http://127.0.0.1:1104/v1/modelbox/job/list/all REST API BODY { \"job_list\": [ { \"job_status\": \"RUNNING\", \"job_id\": \"[flow-name1]\" }, { \"job_status\": \"RUNNING\", \"job_id\": \"[flow-name2]\" } ] } job_list: flow列表 flow作业状态码 状态码 状态码说明 CREATEING 正在创建任务 RUNNING 任务正在执行 SUCCEEDED 任务执行成功 FAILED 任务执行失败 PENDING 等待执行 DELETEING 正在删除任务 UNKNOWN 未知状态 NOTEXIST 任务不存在 错误码 当前支持的错误码： 错误码 错误码说明 MODELBOX_001 server internal error MODELBOX_002 request invalid, no such job MODELBOX_003 request invalid, can not get jobId MODELBOX_004 request invalid, can not get graph MODELBOX_005 request invalid, job already exist MODELBOX_006 request invalid, invalid command © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/develop.html":{"url":"develop/develop.html","title":"开发","keywords":"","body":"开发 本章节开始，将开始介绍如何使用modelbox进行应用的开发。modelbox在设计中以易扩展为目标，在框架多个位置增加了扩展的能力，提供多种编程的模式，以及多语言的支持，以更加灵活的满足应用的实现。 modelbox的扩展能力如下： modelbox服务插件扩展。 modelbox组件可任意集成。 推理加速设备支持扩展。 功能单元的扩展及复用。 c++，Python，Java等多语言的开发支持。 这些扩展能力将满足应用开发中的大部分需求。对于基本应用开发的场景下，开发者一般只需要关注应用图和功能单元的开发即可；在需要与外部系统交互以响应外部数据处理请求的场景下，则需要对modelbox服务插件进行自定义扩展；当应用需要支持的运行环境含有modelbox未支持的硬件设备时，则需要扩展开发推理加速设备支持模块；当AI应用需要作为组件被集成到其他系统中时，可以直接使用libmodelbox组件提供的API，完成应用的集成；并且modelbox支持多种语言的开发，可以选择擅长使用的语言快速完成应用。 Modelbox开发套件 modelbox的开发视图如下，图中绿色的部分是开发者可以自定义扩展的部分。 modelbox开发套件包含如下部分： 流程图 Flow 控制ModelBox执行的过程，默认情况，采用Graphviz DOT语言进行流程图设计，开发。 modelbox server 为简化服务化应用开发者的工作量，Modelbox集成了服务功能，modelbox server有完善的服务集成功能，REST-API功能，任务管理功能，并集成了modelbox library，对应用开发者提供了插件扩展的接口，应用开发者只需编写业务需要的插件，即可驱动modelbox和业务控制服务对接。 自定义应用 与modelbox server对应，如果业务需要更多的定制能力，业务可以直接使用ModelBox SDK提供的接口来驱动ModelBox，支持的语言有c++，python。 modelbox sdk ModelBox应用提供的开发API，包含了C++, python等多种语言的支持。开发者可以使用自己熟悉的语言进行集成或扩展。 modelbox tool ModelBox运行、开发支撑工具，可用于检查图的正确性，调试单个图，查询可用的流单元Flowunit，模型加密等功能。 modelbox library modelbox核心库，包含了modelbox运行需要的功能，包含内存管理，调度管理，图管理等功能。此核心组件不可定制，开发者直接通过API使用modelbox核心库功能。 flowunit 功能单元，ModelBox的关键组成，处理数据关键组件，开发者主要开发的组件。 device 设备支持组件，用于支持特定的硬件，如GPU，NPU等，MODELBOX已经内置了主流的GPU，NPU开发支持。开发者只需要开发相应的功能即可。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/flow/flow.html":{"url":"develop/flow/flow.html","title":"流程图开发及运行","keywords":"","body":"流程图开发及运行 流程图(Graph)是应用逻辑的表述，modelbox将根据流程图构建应用的处理逻辑。 因此在应用开发中，流程图的开发是首要进行的，流程图开发完毕后，才能明确需要开发的功能单元。 流程图开发模型 流程图开发之前需要搞清几个概念： drivers， flowunit， node 以及 flow。 driver:算子的实现单元， 实现的具体的算子是作为driver加载在modelbox中的， modelbox服务在启动的时候会加载指定目录下的所有的算子库作为driver并管理。 flowunit: driver是功能的抽象，那么flowunit就是功能的具体，当扫描完所有的driver之后，modelbox会读取toml文件中的配置，通过flowunit_name以及配置创建driver抽象的的实例，这个实例就称之为flowunit， 根据不同的配置创建了不同的配置参数实现了不同的功能。 当然除了纯粹的创建实例之外还增加了一些内存管理， 端口管理， 设备管理等功能，具体的请前往flowunit和device的页面查看。 node:node作为实际的数据处理单元，集成了flowunit， device管理， 内存管理，端口管理等为一体， 一个node作为数据流中一个节点。 flow: 一个图构建完成后就是一个flow， 一个flow由多个node相连接构成。 流程图的开发步骤分为四步： graph LR A(图的创建) --> B(图的加载) --> C(图的构建) --> D(图的运行) 图的创建： 也叫图的定义，是开发者根据实际的业务需求，按照流程图的开发规范创建的流程图， 流程图中标识了功能单元的名称、配置以及数据流向。 图的加载： 用户通过调用modelbox的函数将图文件或者存储图的内存块加载到modelbox中， modelbox会根据配置解析出代码可识别的模型，如果图的配置有问题也会在此时发现并通过返回值获取。此时modelbox会根据配置中的flowunit以及device去查询当前已加载的驱动库是否有匹配的driver，只有当所有的driver都能正确查询到时，才能正确加载图。 图的构建： 当用户调用对应的modelbox函数接口时， modelbox会将解析完毕的图模型转换为各个Node对象，并且创建好数据流通道。 此时所有的node都已经准备好，等有数据到来时既可以直接处理数据。 图的运行： 当用户调用函数接口时， modelbox会从用户配置的数据源读取数据并按照图构建的路径处理数据，并输出到用户指定的路径中。此时node从前面的节点中获取数据并调用flowunit的处理函数处理数据，并将处理后的数据输出到下一个node中，此时所有的node节点都已经运行起来了，直到数据结束或者用户手动终结流程。 流程图开发 流程图配置 一个流程图使用一份TOML格式的配置表示，配置文件内容如下： [log] level=\"DEBUG\" [driver] dir=\"/usr/local/lib\" skip-default=false [graph] graphconf = '''digraph demo { input[type=input] output[type=output] process[flowunit=process] input->process->output }''' graph.graphconffilepath = \"/path/to/graphviz/flow.conf\" format = \"graphviz\" 配置文件项目说明： [driver]：用于说明驱动加载路径。 dir: 指定功能单元等驱动加载路径，可以指定多个，逗号分隔。 skip-default：是否跳过默认的/usr/local/lib路径。 [graph]：用于指定图的内容。 format指定流程图的格式，目前仅支持graphviz。 graphconf为内联graphviz流程图。 graph.graphconffilepath为外部流程图文件。 [log]: 指定图的日志级别。 level: 指定级别，可以是DEBUG, INFO, NOTICE, WARN, ERROR, FATAL, OFF 注意：修改此级别，将全局影响日志级别，建议仅在调试时使用。 流程图定义 ModelBox默认情况，采用Graphviz DOT语法表达图，关于DOT语法，可以查看Graphviz DOT的指导。 假设有一个简单的业务例子如下图： * ModelBox启动http server监听80端口, * 当有请求时，调用PROCESS功能处理数据， * 数据处理完成后，再将结果回应到客户端。 ![graphviz](../assets/images/figure/framework-conception/graphviz.png) Graphviz的表达： digraph G { node[shape=Mrecord] // 定义点属性 HTTP_REQUEST[flowunit=http, listen=\"0.0.0.0:80\", label=\"{% raw %}{HTTP REQUEST|{OUT}}{% endraw %}\"] PROCESS[flowunit=json, label=\"{% raw %}{{IN}|PROCESS|{OUT}}{% endraw %}\"] HTTP_RESPONSE[flowunit=http, label=\"{% raw %}{{IN}|HTTP RESPONSE}{% endraw %}\"] // 定义点关系 HTTP_REQUEST:OUT->PROCESS:IN PROCESS:OUT->HTTP_RESPONSE:IN } 完成上述图构成后，即可将上述图，组成ModelBox可识别的配置文件。ModelBox可识别的配置文件采用TOML配置格式。生成TOML文件后，即可将配置文件加载到ModelBox中执行。 [graph] graphconf = ''' digraph G { node[shape=Mrecord] // 定义点属性 HTTP_REQUEST[flowunit=http, listen=\"0.0.0.0:80\", label=\"{% raw %}{HTTP REQUEST|{OUT}}{% endraw %}\"] PROCESS[flowunit=json, label=\"{% raw %}{{IN}|PROCESS|{OUT}}{% endraw %}\"] HTTP_RESPONSE[flowunit=http, label=\"{% raw %}{{IN}|HTTP RESPONSE}{% endraw %}\"] // 定义点关系 HTTP_REQUEST:OUT->PROCESS:IN PROCESS:OUT->HTTP_RESPONSE:IN } ''' format = \"graphviz\" 关键字说明 下面图的配置，包含三部分。 // 1. 图 digraph G { node[shape=Mrecord] // 2. 定义点属性 HTTP_REQUEST[flowunit=http, listen=\"0.0.0.0:80\", label=\"{% raw %}{HTTP REQUEST|{OUT}}{% endraw %}\"] PROCESS[flowunit=json, label=\"{% raw %}{{IN}|PROCESS|{OUT}}{% endraw %}\"] HTTP_RESPONSE[flowunit=http, label=\"{% raw %}{{IN}|HTTP RESPONSE}{% endraw %}\"] // 3. 定义点关系 HTTP_REQUEST:OUT->PROCESS:IN PROCESS:OUT->HTTP_RESPONSE:IN } 第一部分是图 格式 digraph [name] 说明 digraph开头，[name]可以是字符串。 第二部分是点Node的定义 格式 name[key=value] 说明 name为点的名称，key为node的配置属性，每个节点不同，value为key的配置值。 type参数指定点node的类型，可以是input, output, flowunit 当未指定type参数时，node缺省为flowunit。 flowunit表示此点为功能单元功能模块，配合flowunit=xx指定，功能单元的执行实体。 node[type=\"flowunit\", flowunit=httpserver] 上述配置表示，点的名称为node，类型为flowunit，其执行实体为httpserver。 支持的Flowunit可以使用modelbox-tool工具查询。 input：表示此点的类型为输入端口，为整个图的配置，表示图的数据输入端口。 graphinput[type=input] 上述配置表示，图输入点的名称为graphinput，在使用SDK形式调用ModelBox时可以使用此名称发送数据给图。 output: 表示此点的类型为输出端口，为整个图的配置，表示图的数据输出端口。 graphoutput[type=output] 上述配置表示，图输出点的名称为graphoutput，在使用SDK形式调用ModelBox时可以使用此名称接收图处理后的数据。 第三部分是点的关系定义 格式 name:outport -> name:inport 说明 name为点的名称，outport为输出端口名称，inport为输入端口名称。 流程图开发方式 流程图开发时，可采用如下形式进行开发 方式 说明 推荐度 连接 Modelbox编排服务 使用ModelBox编排服务进行流程图的开发。 ⭐️⭐️⭐️ 指导 手工编写 手工编写toml格式的流程图文件，并添加到ModelBox Server插件中运行 ⭐️ 指导 流程图的运行 流程图完成后，可以采用下列形式运行流程图 方式 说明 特点 推荐度 连接 modelbox-server 使用modelbox加载运行流程图 基本无需编程，只需要通过配置即可完成图的运行 ⭐️⭐️⭐️ 指导 modelbox-tool modelbox-tool调试 调试图时使用的工具，方便，快速检查结果是否正确 ⭐️⭐️⭐️ 指导 Python SDK Python SDK形式 Python接口形式，方便开发者与当前python服务集成 ⭐️⭐️ 指导 C++ SDK C++ SDK形式 c++SDK形式，方便开发者与当前c/c++程序集成 ⭐️⭐️ 指导 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/flow/c++.html":{"url":"develop/flow/c++.html","title":"C++","keywords":"","body":"C++开发流程图 当前方式适合哪些场景下使用 此方式主要适用于C++开发者开发流程图。 C++的API接口 flow的运行流程可参考flow章节。 从flow章节中我们知晓了流程图运行的流程，在C++中有对应的函数接口用于处理对应不同的阶段。下面是C++中使用的API列表： API接口 参数说明 函数说明 Flow::Init configfile: 指定config文件的路径format： 指定图文件的格式，可选项为 FORMAT_AUTO,FORMAT_TOML，FORMAT_JSON 初始化modelbox服务，主要包含功能如下：1. 读取driver参数，获取driver的扫描路径2. 扫描指定路径下的driver文件，并创建driver实例3. 加载流程图并转换为modelbox可识别的模型4. 初始化设备信息，性能跟踪和数据统计单元 Flow::Init name: 指定的图的名称graph: 存储图的字符串format：指定图的格式 与上面Init的区别是，上面通过读取文件的方式，而此函数通过读取字符串的方式，其他功能相同 Flow::Init is: 图的输入流istreamfname: 输入的图名称 功能与上面Init相同， 区别在于输入的是流保存的图信息 Flow::Init config: Configuration指针，存储图信息 功能同上 Flow::Build / 用于构建图，将图模型转为可以运行的Node节点并且建立好数据通道 Flow::Run / 图的运行： 同步方式，图运行完成后返回 Flow::RunAsync / 图的运行： 异步运行， 调用后直接返回， 通过调用Wait()函数判断运行是否结束 Flow::Wait millisecond: 超时时间， 以毫秒为单位ret_val: 图运行的结果 等待图运行结束，当图的运行时间超过millisecond表示的时间时，则强制停止图的运行，并返回TIMEOUT Flow::Stop() / 强制停止运行中的图 Flow::CreateExternalDataMap / 当图中的第一个节点为input节点时， 使用此函数可以创建一个输入的ExternalDataMap， 用户可以通过向ExternalDataMap数据中赋值并传递数据给Input节点。具体使用方法可参考章节 C++ SDK API调用说明 C++开发调用流程图时，需要先安装C++的运行包，然后再编写C++函数，调用Flow的API来执行流程图。 Flow流程图接口调用过程如下图所示。 安装C++ SDK包。 开发流程图，配置基础部分和图部分。 调用Flow::init接口，输入流程图文件。 调用Flow::build初始化流程图。 调用Flow::run_async，异步执行流程图。 调用Flow::wait等待结果。 TOML流程图配置 [driver] dir=\"/usr/local/lib\" [graph] graphconf = '''digraph demo { input[type=input] output[type=output] process[flowunit=process] input->process->output }''' format = \"graphviz\" 导入modelbox包 编写时，需要引入头文件。 #include 基本接口 int RunFlow(const std::string &file) { // 创建Flow执行对象 auto flow = std::make_shared(); // 输入流程图配置文件 MBLOG_INFO Init(file); if (!ret) { MBLOG_ERROR Build(); if (!ret) { MBLOG_ERROR RunAsync(); // 等待执行结果 ret = flow->Wait(); if (!ret) { MBLOG_ERROR Stop(); MBLOG_INFO 流程执行流程 使用flow-example.toml文件中配置的流程图初始化flow， auto flow = std::make_shared()， 如何配置流程图详见流程图开发流程 flow->Init(file) 根据配置文件初始化flow对象。 flow->Build() 开始构建flow对象 flow->RunAsync() 开始异步运行flow flow->Wait() 等待flow结束，参数为超时时间，超时时间为0表示无限等待。 flow->Stop() 停止流程图。 外部数据交互 配置图，图中增加input, output端口名称。 digraph demo { input[type=input] output[type=output] process[flowunit=process] input->process->output } 初始化图的数据处理对象。 std::shared_ptr ExternDataInit(std::shared_ptr flow) { auto ext_data = flow->CreateExternalDataMap(); return ext_data; } 代码发送数据，到input端口。 modelbox::Status SendExternalData(std::shared_ptr ext_data, void *data, int len) { // 申请外部数据对象 auto output_buf = ext_data->CreateBufferList(); // 申请内存，并设置内容 output_buf->Build({len}); auto buff = (int*)output_buf->MutableData(); memcpy(buff, data, len); // 将数据发送到input端口 auto status = ext_data->Send(\"input\", output_buf); if (!status) { return {status, \"send data to input failed.\"}; } // 关闭输入 status = ext_data->Shutdown(); if (!status) { return {status, \"shutdown failed.\"}; } return modelbox::STATUS_OK; } 代码从图中output端口接收数据。 modelbox::Status RecvExternalData(std::shared_ptr ext_data) { OutputBufferList map_buffer_list; // 接收数据 while (true) { auto status = ext_data->Recv(map_buffer_list); if (status != STATUS_SUCCESS) { if (status == STATUS_EOF) { // 数据处理结束 break; } // 处理出错，关闭输出。 auto error = ext_data->GetLastError(); ext_data->Close(); MBLOG_ERROR GetDesc(); break; } // 处理结果数据 auto buffer_list = map_buffer_list[\"output\"]; ProcessData(buffer_list); } return modelbox::STATUS_OK; } C++日志 默认情况，ModelBox的SDK输出日志到console，业务需要注册相关的日志处理函数，注册方法可参考日志章节。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/flow/python.html":{"url":"develop/flow/python.html","title":"Python","keywords":"","body":"Python开发流程图 当前方式适合哪些场景下使用 此方式适用于python开发者开发流程图。 Python开发调用流程图时，需要安装python的运行包，然后再编写python函数，调用Flow执行API执行流程图。 Python的API接口 从flow章节中我们知晓了流程图运行的流程，在Python中有对应的函数接口用于处理对应不同的阶段。下面是Python中使用的API列表： API接口 参数说明 函数说明 | Flow::init | configfile: 指定config文件的路径format： 指定图文件的格式，可选项为 FORMAT_AUTO,FORMAT_TOML，FORMAT_JSON | 初始化modelbox服务，主要包含功能如下：1. 读取driver参数，获取driver的扫描路径2. 扫描指定路径下的driver文件，并创建driver实例3. 加载流程图并转换为modelbox可识别的模型4. 初始化设备信息，性能跟踪和数据统计单元 | | Flow::init | name: 指定的图的名称graph: 存储图的字符串format：指定图的格式 | 与上面init的区别是，上面通过读取文件的方式，而此函数通过读取字符串的方式，其他功能相同 | | Flow::init | config: Configuration指针，存储图信息 | 功能同上 | | Flow::build() | / | 用于构建图，将图模型转为可以运行的Node节点并且建立好数据通道 | | Flow::run() | / | 图的运行： 同步方式，图运行完成后返回 | | Flow::run_async | / | 图的运行： 异步运行， 调用后直接返回， wait()函数判断运行是否结束 | | Flow::wait | millisecond: 超时时间， 以毫秒为单位ret_val: 图运行的结果 | 等待图运行结束，当图的运行时间超过millisecond表示的时间时，则强制停止图的运行，并返回TIMEOUT | | Flow::stop | / | 强制停止运行中的图 | | Flow::create_external_data_map | / | 当图中的第一个节点为input节点时， 使用此函数可以创建一个输入的ExternalDataMap， 用户可以通过向ExternalDataMap数据中赋值并传递数据给Input节点。具体使用方法可参考外部数据交互章节 | Python SDK API调用说明 Flow流程图接口调用过程如下图所示。 安装python SDK包 开发流程图，配置基础部分和图部分。 调用Flow::init接口，输入流程图文件。 调用Flow::build初始化流程图。 调用Flow::run_async，异步执行流程图。 调用Flow::wait等待结果。 TOML流程图配置 [driver] dir=\"/usr/local/lib\" [graph] graphconf = '''digraph demo { input[type=input] output[type=output] process[flowunit=process] input->process->output }''' format = \"graphviz\" 导入modelbox包 编写时，需要导入ModelBox的开发包。 import modelbox 基本接口 def RunFlow(): # 指定图文件路径 flow_file = \"/path/to/graph/flow-example.toml\" flow = modelbox.Flow() # 初始化Flow接口 ret = flow.init(flow_file) if ret == False: modelbox.error(flow_file + \" flow init failed\") # 创建流程图 ret = flow.build() if ret == False: modelbox.error(flow_file + \" flow build failed\") # 异步执行流程图 ret = flow.run_async() if ret == False: modelbox.error(flow_file + \" flow run async failed\") # 等待结果 ret = flow.wait(0) if ret != modelbox.Status.StatusCode.STATUS_STOP: modelbox.error(flow_file + \" flow run failed\") 流程执行流程 使用flow-example.toml文件中配置的流程图初始化flow， flow = modelbox.Flow() 返回一个flow对象， 如何配置流程图详见流程图开发流程 flow.init(flow_file) 根据配置文件初始化flow对象。 flow.build() 开始构建flow对象 flow.run_async() 开始异步运行flow flow.wait(0) 等待flow结束，超时时间为0表示无限等待。 外部数据交互 配置图，图中增加input, output端口名称。 digraph demo { input[type=input] output[type=output] process[flowunit=process] input->process->output } 创建external data对象 # extern_data 对象 def init_external_dat(): extern_data = flow.create_external_data_map() return extern_data 代码发送数据，到input端口。 # 发送数据到图 def send_external_data(extern_data): # 申请内存。 buffer_list = extern_data.create_buffer_list() im_array = np.asarray(img_rgb[:,:]) buffer_list.push_back(im_array) # 将数据发送到\"input\"。 extern_data.send(\"input\", buffer_list) # 结束输入。 extern_data.shutdown() 代码从图中output端口接收数据 # 从图中接收数据 def recv_flow_data(extern_data): out_buffer = extern_data.create_buffer_list() # 使用创建的external对象从output接收数据 while True: ret = extern_data.recv(out_buffer) if ret != modelbox.Status.StatusCode.STATUS_SUCCESS: if ret == modelbox.Status.StatusCode.STATUS_EOF: break extern_data.close() print(\"recv data failed\", ret) break result_buffer_list = out_buffer.get_buffer_list(\"output\") # 循环处理数据 for i in range(result_buffer_list.size()): aa = result_buffer_list[i] np_image = np.array(aa, copy= False) image = Image.fromarray(np_image) # .... Python日志 默认情况，ModelBox的SDK输出日志到console，业务需要注册相关的日志处理函数，注册方法可参考日志章节 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/flowunit/flowunit.html":{"url":"develop/flowunit/flowunit.html","title":"流单元开发","keywords":"","body":"功能单元开发 在完成了流程图编排之后，还需通过功能单元(FlowUnit)来实现应用的实际功能。ModelBox加载功能单元后，根据图结构实例化为图中的各个节点。功能单元需提供特定的接口，根据接口协议在数据处理的各个阶段对接口进行调用。 有关功能单元的详细介绍，请先阅读框架概念章节，以及后续的功能单元、数据流章节内容。 本章节内容主要介绍功能单元的开发过程。 功能单元开发流程 先从Example中复制样例代码。 确定功能单元类型。 修改Example代码的编译，TOML，源代码名称，和设置功能单元的输入，输出，参数，以及运行设备信息。 根据需要实现FlowUnit的Open，DataPre，DataPost，Process，DataGroupPre，DataGroupPost，Close接口。 编译连接外部组件，调用外部功能组件接口。 编译安装包。 流程图中配置使用功能单元。 功能单元类型 在开发功能单元时，应该先明确开发功能单元处理数据的类型，业务的场景。再根据上述信息选择合适的功能单元类型。 具体功能单元类型，请查看功能单元中的分类。在确认功能单元类型后，需要对功能单元进行如下参数的配置。 功能单元参数说明： 配置参数 是否必须设置 参数类型 功能描述 功能单元名称 是 String 功能单元名称 工作模式 是 FlowType 功能单元工作模式 添加功能单元输入 是 FlowUnitInput 设置输入端口和内存位置 添加功能单元输出 是 FlowUnitOutput 设置输出端口和内存位置 设置功能单元参数 是 FlowUnitOption 设置功能单元参数，包括参数名，类型，描述等信息 功能单元组类型 否 GroupType 设置Driver描述 否 bool 条件类型 否 ConditionType 是否为条件功能单元 输出类型 否 FlowOutputType 设置是否为扩张，或者合并功能单元 额外参数 否 String 功能单元假名 否 String 设置输入输出buffer数量是否相同 否 bool 仅流类型可设置，标识是否允许输入buffer和输出buffer数量不一致 设置输入是否连续 否 bool 内存是否连续 否 bool 是否组装为连续内存 设置是否需要收齐buffer 否 bool 是否收集所有扩张的buffer。仅当收缩功能单元需设置，默认false 设置异常可见 否 bool 设置虚拟类型 否 bool 不同功能单元的配置如下： 功能单元类型 配置参数 说明 |通用功能单元| 工作模式设置为NORMAL | | |条件功能单元| 工作模式设置为NORMAL，并且条件类型设置为IF_ELSE | | |流数据功能单元| 工作模式设置为STEAM | | |流数据拆分功能单元| 工作模式设置为STREAM，并且输出类型设置为扩张 | | |流数据合并功能单元| 工作模式设置为STREAM，并且输出类型设置为合并 | | |推理功能单元| 只需准备好模型和配置toml文件即可 | | | 功能单元接口说明 功能单元接口，包含功能单元插件初始化接口，功能单元初始化接口和数据处理接口。 API接口类型实现对照关系 功能单元提供了FlowUnit::相关的接口，其接口按不同类型的功能单元而不同，开发者应根据FlowUnit处理数据的类型，选择实现相关的接口，对应的关系表如下： 接口 接口类型 接口功能 调用实时机 是否必须 通用功能单元 条件功能单元 数据流功能单元 数据流拆分功能单元 数据流合并功能单元 DriverInit 插件初始化接口 模块初始化 插件加载时调用一次 否 ✔️ ✔️ ✔️ ✔️ ✔️ DriverFini 插件初关闭接口 模块退出 插件结束时调用一次 否 ✔️ ✔️ ✔️ ✔️ ✔️ FlowUnit::Open 功能单元初始化接口 功能单元初始化 图加载功能单元时调用 否 ✔️ ✔️ ✔️ ✔️ ✔️ FlowUnit::Close 功能单元关闭接口 功能单元关闭 图结束时调用 否 ✔️ ✔️ ✔️ ✔️ ✔️ FlowUnit::Process 数据处理接口 数据处理 有数据产生时调用 是 ✔️ ✔️ ✔️ ✔️ ✔️ FlowUnit::DataGroupPre 数据处理接口 流数据合并开始 流数据合并时，流开始点触发 否 ✔️ FlowUnit::DataGroupPost 数据处理接口 流数据合并结束 流数据合并时，流结束点触发 否 ✔️ FlowUnit::DataPre 数据处理接口 流数据开始 流数据开始时触发 否 ✔️ FlowUnit::DataPost 数据处理接口 流数据结束 流数据结束时触发 否 ✔️ 接口实现关系说明 大部分情况下，业务都属于通用功能单元，仅需要处理单个数据，开发只需要实现FlowUnit::Process的功能即可。 若功能单元需要处理流数据，或需要记录状态，对数据进行跟踪处理，则需要实现FlowUnit:DataPre, FlowUnit::Process, FlowUnit::DataPost接口的功能。 若需要多数据合并，汇总结果，则需要实现FlowUnit::DataGroupPre, FlowUnit::DataPre, FlowUnit::Process, FlowUnit::DataPost, FlowUnit::DataGroupPost接口。 DriverInit, DriverFini, FlowUnit::Open, FlowUnit::Close在不同的时机触发，业务可根据需要实现相关的功能。比如初始化某些会话，句柄等资源。 功能单元初始化、关闭 对应需实现的接口为FlowUnit::Open、FlowUnit::Close，此接口可按需求实现。例如，使用::Open接口获取用户在图中的配置参数，使用::Close接口释放功能单元的一些公共资源。 数据处理 对应需实现的接口为FlowUnit::Process, Process为FlowUnit的核心函数。输入数据的处理、输出数据的构造都在此函数中实现。Process接口处理流程大致如下： 从DataContext中获取Input输入BufferList，Output输出BufferList对象，参数为Port名称。 使用Output->Build申请输出内存，内存和设备相关，设备在DriverDesc的时候设置。如是CPU则是CP内存，如是GPU则是GPU显存。 循环处理每一个Input Buffer数据。 对每一个Input Buffer数据可使用Get获取元数据信息。 业务处理，根据需求对输入数据进行处理。 构造output_buffer，并使用output_buffer->Build申请输出内存，内存和设备相关，设备DriverDesc的时候设置。如是CPU则是CPU内存，如是GPU则是GPU显存。 对每一个Output Buffer数据可使用Set设置元数据信息。 返回成功后，ModelBox框架将数据发送到后续的功能单元。 Stream流数据处理 对应需实现的接口为FlowUnit::DataPre、FlowUnit::DataPost，此接口Stream模式可按需实现。例如，处理一个视频流时，在视频流开始时会调用DataPre，视频流结束时会调用DataPost。FlowUnit可以在DataPre阶段初始化解码器，在DataPost阶段关闭解码器，解码器的相关句柄可以设置到DataContext上下文中。DataPre、DataPost接口处理流程大致如下： Stream流数据开始时，在DataPre中获取数据流元数据信息，并初始化相关的上下文，存储DataContext->SetPrivate中。 处理Stream流数据时，在Process中，使用DataContext->GetPrivate获取到上下文对象，并从Inpu中获取输入，处理后，输出到Output中。 Stream流数据结束时，在DataPost中释放相关的上下文信息。 拆分合并处理 对应需实现的接口为FlowUnit::DataGroupPre、FlowUnit::DataGroupPost，当数据需要拆分合并时需要实现。 在业务处理过程中对数据进行拆分，然后在后续功能单元中处理，当数据处理完成后需要对数据进行合并得到最终结果。 对应的处理代码和DataPre，DataPost类似，如下图 如要将输入Stream1，Stream2，...合并为一个Stream。则接口调用过程为 DataGroupPre DataPre, Stream1 DataPost, Stream1 DataPre, Stream2 DataPost, Stream2 ... DataGroupPost 在编写代码时，其过程和DataPre类似，差别在于合并时对一组数据的归并动作：GroupPre中获取数据，并在Post中打开output的数据流上下文， 每个DataPre，DataPost中处理每个数据，最后在GroupPost中结束数据的合并。 上下文 功能单元上下文包含，会话上下文|SessionContext和数据上下文|DataContext SessionContext 会话上下文 SessionContext主要供调用图的业务使用，业务处理数据时，设置状态对象。 生命周期 绑定ExternalData，从数据进入Flow，贯穿整个图，一直到数据处理完成结束。 使用场景 例如http服务同步响应场景，首先接收到http请求后转化成buffer数据，然后通过ExternalData->GetSessionContext接口获取到SessionContext，接着调用SessionContext->SetPrivate设置响应的回调函数，之后通过ExternalData->Send接口把buffer数据发送到flow中；经过中间的业务处理功能单元；最后http响应功能单元中在业务数据处理完成后，再调用SessionContext->GetPrivate获取响应回调函数，发送http响应。至此SessionContext也结束。 DataContext 数据上下文 DataContext是提供给当前功能单元处理数据时的临时获取BufferList 功能单元处理一次Stream流数据，或一组数据的上下文，当数据生命周期不再属于当前功能单元时，DataContext生命周期也随之结束。 生命周期 绑定BufferList，从数据进入FlowUnit到处理完成。 使用场景 通过DataContext->Input接口获取输入端口BufferList，通过DataContext->Output接口获取输出端口BufferList对象,通过DataContext->SetPrivate接口设置临时对象，DataContext->GetPrivate接口获取临时对象。 多种语言开发功能单元 功能单元的开发可以使用多种语言，开发者可以使用合适的语言进行开发。 方式 说明 适合类型 复杂度 连接 C++ C++ SDK形式 适合有高性能要求的功能开发，开发复杂度稍高。 ⭐️⭐️⭐️ 指导 Python Python SDK形式 适合对性能要求不高的功能开发，可快速上线运行。 ⭐️⭐️ 指导 模型 提供模型的形式 适合模型推理类功能的开发，直接提供模型即可运行，方便快捷。 ⭐️ 指导 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/flowunit/inference.html":{"url":"develop/flowunit/inference.html","title":"推理","keywords":"","body":"推理功能单元 ModelBox内置了主流的推理引擎，如TensorFlow，TensorRT，Ascend ACL。在开发推理功能单元时，只需要通过配置toml文件，即可完成推理功能单元的开发。 样例工程可从源代码目录的example/inference/flowunit/中获取。开发之前，可以从功能单元概念章节了解流单的执行过程。 推理功能单元目录结构 推理功能单元只需要提供独立的toml配置文件，指定推理功能单元的基本属性即可，其目录结构为： [some-flowunit] |---[some-flowunit].toml |---[model].pb |---[plugin].so ModelBox框架在初始化时，会扫描/path/to/flowunit/[some-flowunit]目录中的toml后缀的文件，并读取相关的推理功能单元信息，具体可通过modelbox-tool工具查询是否配置正确。[plugin].so是推理所需插件，可按需实现。 TOML配置 # 基础配置 [base] name = \"FlowUnit-Name\" # 功能单元名称 device = \"Device\" # 功能单元运行的设备类型，cpu，cuda，ascend等。 version = \"x.x.x\" # 功能单元组件版本号 description = \"description\" # 功能单元功能描述信息 entry = \"model.pb\" # 模型文件路径 type = \"inference\" #推理功能单元时，此处为固定值 virtual_type = \"tensorrt\" # 指定推理引擎, 可以是tensorflow, tensorrt, atc plugin = \"yolo\" # 推理引擎插件 # 输入端口描述 [input] [input.input1] # 输入端口编号，格式为input.input[N] name = \"Input\" # 输入端口名称 type = \"datatype\" # 输入端口数据类型 # 输出端口描述 [output] [output.output1] # 输出端口编号，格式为output.output[N] name = \"Output\" # 输出端口名称 type = \"datatype\" # 输出端口数据类型 编写完成toml文件后，将对应的路径加入ModelBox的搜索路径即可使用开发后的推理功能单元。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/flowunit/python.html":{"url":"develop/flowunit/python.html","title":"Python","keywords":"","body":"Python开发功能单元 Python开发功能单元时，需要预先安装ModelBox的运行包，在release目录中，使用pip命令安装，然后再基于example改造，修改为相应的功能单元组件。 样例工程可从源代码目录的example/python/flowunit/中获取，在开发之前，可以从功能单元概念章节了解流单的执行过程。 Python API调用说明 Python FlowUnit接口调用过程如下图所示。 FlowUnit开发分为两部分，一部分是TOML配置, 一部分是FlowUnit，这两部分的需要实现的功能如下： 组件 函数 功能 是否必须 实现功能 TOML配置 Basic 设置Python插件基本属性 是 填写Python插件相关的描述信息，包括，插件名称，插件版本号，插件运行的设备类型，查询的细节描述信息，以及插件的Python入口信息。 Driver InputOutput 输入，输出属性 是 用于描述插件的输入，输出端口个数，名称，类型 FlowUnit FlowUnit::OpenFlowUnit::Close FlowUnit初始化 否 FlowUnit初始化、关闭，创建、释放相关的资源 FlowUnit FlowUnit::Process FlowUnit数据处理 是 FlowUnit数据处理函数，读取数据数据，并处理后，输出数据 FlowUnit FlowUnit::DataPreFlowUnit::DataPost Stream流数据开始，结束通知 部分 Stream流数据开始时调用DataPre函数初始化状态数据，Stream流数据结束时释放状态数据，比如解码器上下文。 FlowUnit FlowUnit::DataGroupPreFlowUnit::DataGroupPost 数据组归并开始，结束通知 部分 数据组归并，结束通知函数，当数据需要合并时，对一组数据进行上下文相关的操作。 Python功能单元目录结构 python功能单元需要提供独立的toml配置文件，指定python功能单元的基本属性。一般情况，目录结构为： [some-flowunit] |---[some-flowunit].toml |---[python-module].py |---xxx.py 创建模板工程 ModelBox提供了模板创建工具，可以通过modelbox-tool工具产生python功能单元的模板，具体的命令为 modelbox-tool create -t python -n FlowUnitName -d /path/to/flowunit ModelBox框架在初始化时，会扫描/path/to/flowunit/[some-flowunit]目录中的toml后缀的文件，并读取相关的信息，具体可通过modelbox-tool工具查询。 modelbox-tool driver -info -path /usr/local/lib64,/path/to/flowunit/ TOML配置 # 基础配置 [base] name = \"FlowUnit-Name\" # 功能单元名称 device = \"Device\" # 功能单元运行的设备类型，cpu，cuda，ascend等。 version = \"x.x.x\" # 功能单元组件版本号 description = \"description\" # 功能单元功能描述信息 entry = \"python-module@SomeFlowunit\" # python 功能单元入口函数 type = \"python\" # Python功能单元时，此处为固定值 # 工作模式 stream = false # 是否数据功能单元 condition = false # 是否条件功能单元 collapse = false # 是否合并功能单元 collapse_all = false # 是否合并所有数据 expand = false # 是否拆分功能单元 # 默认配置值 [config] item = value # 输入端口描述 [input] [input.input1] # 输入端口编号，格式为input.input[N] name = \"Input\" # 输入端口名称 type = \"datatype\" # 输入端口数据类型 # 输出端口描述 [output] [output.output1] # 输出端口编号，格式为output.output[N] name = \"Output\" # 输出端口名称 type = \"datatype\" # 输出端口数据类型 头文件 编写时，需要先确认设备的类型，确认完成设备类型后，导入对应设备的头文件，例如 import _flowunit as modelbox 基本接口 import _flowunit as modelbox from PIL import Image class SomeFlowunit(modelbox.FlowUnit): # 派生自modelbox.FlowUnit def __init__(self): super().__init__() def open(self, config): # 打开功能单元，获取配置信息 return modelbox.Status.StatusCode.STATUS_SUCCESS def process(self, data_context): # 数据处理 return modelbox.Status.StatusCode.STATUS_SUCCESS def close(self): # 关闭功能单元 return modelbox.Status() def data_pre(self, data_context): # stream流数据开始 return modelbox.Status() def data_post(self, data_context): # stream流数据结束 return modelbox.Status() def data_group_pre(self, data_context): # 数据组开始 return modelbox.Status() def data_group_post(self, data_context): # 数据组结束 return modelbox.Status() FlowUnit接口说明 功能单元的数据处理的基本单元。如果功能单元的工作模式是stream = false时，功能单元会调用open、process、close接口；如果功能单元的工作模式是stream = true时，功能单元会调用open、data_group_pre、data_pre、process、data_post、data_group_post、close接口；用户可根据实际需求实现对应接口。 功能单元初始化、关闭接口 对应的需要实现的接口为open, close接口，实现样例如下： def open(self, config): # 打开功能单元，获取配置信息。 # 获取用户配置。 config_item = config.get_float(\"config\", \"default\") # 初始化公共资源。 # 返回初始化结果。 return modelbox.Status.StatusCode.STATUS_SUCCESS def close(self): # 关闭功能单元，返回关闭结果。 return modelbox.Status.StatusCode.STATUS_SUCCESS 返回modelbox.Status.StatusCode.STATUS_SUCCESS，表示初始化成功，否则初始化失败。 数据处理 对应的需要实现的接口为process接口，实现样例如下： def process(self, data_context): # 获取输入，输出控制对象。 # 此处的\"Input\"和\"Output\"必须与toml的端口名称一致 inputs = data_context.input(\"Input\") outputs = data_context.output(\"Output\") # 循环处理每一个input输入 for buffer in inputs: np_in_data = np.array(buffer, copy= False) np_out_data = process_data(np_in_data) out_buffer = self.create_buffer(np_out_data) out_buffer.set(\"brightness\", self.__brightness) outputs.push_back(out_buffer) return modelbox.Status.StatusCode.STATUS_SUCCESS Process接口处理流程大致如下： 从context中获取Input输入，Output输出对象，参数为Port名称。 循环处理每一个inputs数据。 将input数据转换为numpy对象，并编写process_data函数。 将process_data结果返回的output numpy数据调用self.create_buffer，转换为buffer。 设置output buffer的meta信息。 将output放入outputs结果集中。 返回处理结果。 Process的返回值说明 返回值 说明 STATUS_OK 返回成功，将Output中的数据，发送到后续FlowUnit流程。 STATUS_CONTINUE 返回成功，暂缓发送Output中的数据。 STATUS_SHUTDOWN 停止数据处理，终止整个流程图。 其他 停止数据处理，当前数据处理报错。 Stream流数据处理 对应需实现的接口为data_pre、data_post，此接口Stream模式可按需实现。实现样例如下： def data_pre(self, data_ctx): # 获取Stream流元数据信息 stream_meta = data_ctx.get_input_meta(\"Stream-Meta\") # 初始化Stream流数据处理上下文对象。 decoder = self.CreateDecoder(stream_meta) # 保存流数据处理上下文对象。 data_ctx.SetPrivate(\"Decoder\", decoder) return modelbox.Status.StatusCode.STATUS_SUCCESS def process(self, data_ctx): # 获取流数据处理上下文对象。 decoder = data_ctx.GetPrivate(\"Decoder\") inputs = data_ctx.input(\"Input\") outputs = data_ctx.output(\"Output\") # 处理输入数据。 decoder.Decode(inputs, outputs) return modelbox.Status.StatusCode.STATUS_SUCCESS def data_post(self, data_ctx): # 关闭解码器。 decoder = data_context.GetPrivate(\"Decoder\") decoder.DestroyDecoder() return modelbox.Status.StatusCode.STATUS_SUCCESS 拆分合并处理 对应需实现的接口为data_group_pre、data_group_post，假设需要统计视频流中每一帧有几个人脸，和整个视频文件所有人脸数量，实现样例如下： def data_group_pre(self, data_ctx): # 创建整个视频流计数 stream_count = 0 data_ctx.SetPrivate(\"stream_count\", stream_count) return modelbox.Status.StatusCode.STATUS_SUCCESS def data_pre(self, data_ctx): # 创建当前帧的人脸计数 frame_count = 0 data_ctx.SetPrivate(\"frame_count\", frame_count) return modelbox.Status.StatusCode.STATUS_SUCCESS def process(self, data_ctx): # 获取流数据处理上下文对象 inputs = data_ctx.input(\"Input\") outputs = data_ctx.output(\"Output\") stream_count = data_ctx.GetPrivate(\"stream_count\") frame_count = data_ctx.GetPrivate(\"frame_count\") return modelbox.Status.StatusCode.STATUS_SUCCESS def data_post(self, data_ctx): # 打印当前帧人脸计数 frame_count = data_context.GetPrivate(\"frame_count\") print(\"frame face total is \", frame_count) return modelbox.Status.StatusCode.STATUS_SUCCESS def data_group_post(self, data_ctx): # 打印视频流的人脸计数 stream_count = data_context.GetPrivate(\"stream_count\") print(\"stream face total is \", stream_count) return modelbox.Status.StatusCode.STATUS_SUCCESS © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/flowunit/c++.html":{"url":"develop/flowunit/c++.html","title":"C++","keywords":"","body":"c++开发功能单元 c++开发功能单元时，需要预先安装modelbox的开发包，然后再基于example改造，修改为相应的功能单元组件。 样例工程可从源代码目录的example/flowunit/中获取，在开发之前，可以从功能单元概念章节了解功能单元的执行过程。 C++ API调用说明 FlowUnit接口调用过程如下图所示。 FlowUnit开发分为三部分，Driver和Factory，分别设置插件属性和功能单元属性，封装为易于接口，FlowUnit为功能单元处理对象，三部分的需要实现的功能如下： 组件 函数 功能 是否必须 实现功能 Driver MODELBOX_DRIVER_FLOWUNIT 设置插件属性 是 Driver属性设置接口，填写插件相关的描述信息，包括，插件名称，插件版本号，插件运行的设备类型，初始化函数，查询的细节描述信息，插件的配置参数列表。 Factory MODELBOX_FLOWUNIT 功能单元属性设置接口，并注册到ModelBox 是 填写ModelBox相关的输入，输出端口，参数设置等信息 FlowUnit FlowUnit::OpenFlowUnit::Close FlowUnit初始化 否 FlowUnit初始化、关闭，创建、释放相关的资源 FlowUnit FlowUnit::Process FlowUnit数据处理 是 FlowUnit数据处理函数，读取数据数据，并处理后，输出数据 FlowUnit FlowUnit::DataPreFlowUnit::DataPost Stream流数据开始，结束通知 部分 stream流数据开始时调用DataPre函数初始化状态数据，Stream流数据结束时释放状态数据，比如解码器上下文。 FlowUnit FlowUnit::DataGroupPreFlowUnit::DataGroupPost 数据组归并开始，结束通知 部分 数据组归并，结束通知函数，当数据需要合并时，对一组数据进行上下文相关的操作。 头文件 编写时，需要先确认设备的类型，确认完成设备类型后，导入对应设备的头文件，例如 设备类型 头文件 说明 cpu #include CPU类型的功能单元 cuda #include Nvidia GPU类型的功能单元 ascend #include Huawei Ascend类型的功能单元 flowunit #include 功能单元开发接口 创建模板工程 ModelBox提供了模板创建工具，可以通过modelbox-tool工具产生c++功能单元的模板，具体的命令为 modelbox-tool create -t c++ -n FlowUnitName -d /path/to/flowunit Driver接口说明 Driver相关的接口，主要用于描述Drvier相关的属性，如Driver名称，版本号，设备类型，描述信息。FlowUnit功能单元开发，这里只需修改功能单元名称即可。 设置Driver相关属性 #include \"modelbox/flowunit_api_helper.h\" MODELBOX_DRIVER_FLOWUNIT(desc) { // 设置插件相关属性 desc.Desc.SetName(FLOWUNIT_NAME); desc.Desc.SetClass(modelbox::DRIVER_CLASS_FLOWUNIT); desc.Desc.SetType(modelbox::DEVICE_TYPE); desc.Desc.SetVersion(FLOWUNIT_VERSION); desc.Desc.SetDescription(FLOWUNIT_DESC); desc.Init([]() { // driver init function. return modelbox::STATUS_OK; }); desc.Exit([]() { // driver finish function. return modelbox::STATUS_OK; }); return; } 代码从上到下，分别设置Driver命令，类型，设备类型，描述信息，版本号。 如果有需要Driver相关的初始化功能，可以通过通过desc.Init, desc.Exit设置回调函数。desc.Init在插件启用时调用，desc.Exit在插件关闭时调用。 设置FlowUnit相关属性 ModelBox在编写插件时，需要定义FlowUnit对应的处理函数和设置FlowUnit对应的端口，参数等信息。 FlowUnit属性设置 #include \"modelbox/flowunit_api_helper.h\" MODELBOX_FLOWUNIT(SomeFlowUnit, desc) { // 设置FlowUnit属性 desc.SetFlowUnitName(\"some-flowuint\"); desc.SetFlowType(modelbox::NORMAL); desc.AddFlowUnitInput( modelbox::FlowUnitInput(\"input\", modelbox::DEVICE_TYPE)); desc.AddFlowUnitOutput( modelbox::FlowUnitOutput(\"output\", modelbox::DEVICE_TYPE)); } 设置功能单元名称，工作模式，输入及输出的名称、类型。此处可以写多个功能单元功能模块的描述，此描述等效于向框架注册功能单元的信息，注册后框架扫描到so时，才能够正确按照名称对功能单元进行加载。 SomeFlowUnit：对应的插件功能单元派生对象，从FlowUnit派生出来的类。 MODELBOX_FLOWUNIT: 一个Driver内部可以注册多个功能单元，MODELBOX_FLOWUNIT可以设置多个不同的FlowUnit。 FlowUnit接口说明 功能单元的数据处理的基本单元。如果功能单元的工作模式是modelbox::NORMAL时，功能单元会调用::Open、::Process、::Close接口；如果功能单元的工作模式是modelbox::STREAM时，功能单元会调用::Open、::DataGroupPre、::DataPre、::Process、::DataPost、::DataGroupPost、::Close接口；用户可根据实际需求实现对应接口。 FlowUnit数据处理接口 class SomeFlowUnit : public modelbox::FlowUnit { public: modelbox::Status Open(const std::shared_ptr &opts) { // 功能单元打开，读取配置 return modelbox::STATUS_OK; } modelbox::Status Close() { // 功能单元关闭，释放资源 return modelbox::STATUS_OK; } modelbox::Status Process(std::shared_ptr data_ctx) { // 数据处理 return modelbox::STATUS_OK; } modelbox::Status DataPre(std::shared_ptr data_ctx) { // stream流数据处理开始 return modelbox::STATUS_OK; }; modelbox::Status DataPost(std::shared_ptr data_ctx) { // stream流数据处理结束 return modelbox::STATUS_OK; }; modelbox::Status DataGroupPre(std::shared_ptr data_ctx) { // 数据组处理开始 return modelbox::STATUS_OK; }; modelbox::Status DataGroupPost(std::shared_ptr data_ctx) { // stream流数据处理结束 return modelbox::STATUS_OK; }; }; 注意：Cuda编程的接口，与CPU编程的接口以及Ascend编程的接口稍有不同，具体参考下列编程接口 设备 说明 连接 Ascend Huawei Ascend 链接 Cuda Nvidia Cuda 链接 功能单元初始化、关闭接口 对应需实现的接口为FlowUnit::Open、FlowUnit::Close，实现样例如下： modelbox::Status SomeFlowUnit::Open( const std::shared_ptr &opts) { // 获取功能单元配置参数 auto pixel_format = opts->GetString(\"pixel_format\", \"bgr\"); return modelbox::STATUS_OK; } Open函数将在图初始化的时候调用，const std::shared_ptr &opts为功能单元的配置参数，可调用相关的接口获取配置，返回modelbox::STATUS_OK，表示初始化成功，否则初始化失败。 modelbox::Status Close() { return modelbox::STATUS_OK; } Close函数将在图处理结束时调用，可用于释放相关的资源。 数据处理 对应需实现的接口为FlowUnit::Process，实现样例如下： modelbox::Status CVResizeFlowUnit::Process( std::shared_ptr ctx) { // 获取输入，输出Buffer对象，\"input\", \"output\"为对应功能单元Port名称，可以有多个。 // 此处的\"Input\"和\"Output\"必须与toml的端口名称一致。 auto input_bufs = ctx->Input(\"input\"); auto output_bufs = ctx->Output(\"output\"); // 获取绑定设备，设备在DriverDesc的时候设置的输出buffer设备 auto device = GetBindDevice(); // 循环处理每个输入数据，并产生相关的输出结果。 for (auto &input : *input_bufs) { // 获取数据元数据信息 auto meta = input->Get(\"Meta\", \"Default\"); // 获取输入，输出的内存指针。输入为const只读数据，输出为可写入数据。 auto input_data = input->ConstData(); // 根据device类型构造buffer auto output_buffer = std::make_shared(device); // 处理数据，下面给出几个例子，根据需要选择对应转换方式 /* 1. string转成buffer */ std::string test_str = \"test string xxx\"; // 申请内存，单位是字节数 output_buffer->Build(test_str.size()); // 获取对应类型的buffer指针 auto output_data = static_cast(output_buffer->MutableData()); // 拷贝string到buffer中。假设输出为cpu设备，则这里使用cpu内存拷贝 if(memcpy_s(output_data, output_buffer->GetBytes(), test_str.data(), test_str.size()) != 0 ) { MBLOG_ERROR (input_data); output_buffer->Build(input->GetBytes()) // 拷贝string到buffer中。假设输出为cuda设备，则这里需要用cuda显存拷贝 if(cudaMemcpy(output_buffer, in_data, input_buf->GetBytes(), cudaMemcpyHostToDevice) != 0) { MBLOG_ERROR Set(\"Meta\", \"Meta Data\"); // push到输出bufferlist中 output_bufs->PushBack(output_buffer); } return modelbox::STATUS_OK; Process的返回值说明 返回值 说明 STATUS_OK 返回成功，将Output中的数据，发送到后续FlowUnit流程。 STATUS_CONTINUE 返回成功，暂缓发送Output中的数据。 STATUS_SHUTDOWN 停止数据处理，终止整个流程图。 其他 停止数据处理，当前数据处理报错。 Stream流数据处理 对应需实现的接口为FlowUnit::DataPre、FlowUnit::DataPost，此接口Stream模式可按需实现。实现样例如下： modelbox::Status VideoDecoderFlowUnit::DataPre( std::shared_ptr data_ctx) { // 获取Stream流元数据信息 auto stream_meta = data_ctx->GetInputMeta(\"Stream-Meta\"); // 初始化Stream流数据处理上下文对象。 auto decoder = CreateDecoder(stream_meta); // 保存流数据处理上下文对象。 data_ctx->SetPrivate(\"Decoder\", decoder); return modelbox::STATUS_OK; } modelbox::Status CVResizeFlowUnit::Process( std::shared_ptr ctx) { // 获取流数据处理上下文对象。 auto decoder = data_ctx->GetPrivate(\"Decoder\"); auto inputs = ctx->Input(\"input\"); auto outputs = ctx->Output(\"output\"); // 处理输入数据。 decoder->Decode(inputs, outputs); return modelbox::STATUS_OK; } modelbox::Status VideoDecoderFlowUnit::DataPost( std::shared_ptr data_ctx) { // 关闭解码器。 auto decoder = data_ctx->GetPrivate(\"Decoder\"); decoder->DestroyDecoder(); return modelbox::STATUS_OK; } 拆分合并处理 对应需实现的接口为FlowUnit::DataGroupPre、FlowUnit::DataGroupPost，假设需要统计视频流中每一帧有几个人脸，和整个视频文件所有人脸数量，实现样例如下： modelbox::Status VideoDecoderFlowUnit::DataGroupPre( std::shared_ptr data_ctx) { // 创建整个视频流计数 uint64_t stream_count = 0; data_ctx->SetPrivate(\"stream_count\", stream_count); return modelbox::STATUS_OK; } modelbox::Status VideoDecoderFlowUnit::DataPre( std::shared_ptr data_ctx) { // 创建当前帧的人脸计数 uint64_t frame_count = 0; data_ctx->SetPrivate(\"frame_count\", frame_count); return modelbox::STATUS_OK; } modelbox::Status CVResizeFlowUnit::Process( std::shared_ptr ctx) { // 获取流数据处理上下文对象 auto inputs = ctx->Input(\"input\"); auto outputs = ctx->Output(\"output\"); auto stream_count = std::static_pointer_cast(data_ctx->GetPrivate(\"stream_count\")); auto frame_count = std::static_pointer_cast(data_ctx->GetPrivate(\"frame_count\")); ++frame_count; ++stream_count; return modelbox::STATUS_OK; } modelbox::Status VideoDecoderFlowUnit::DataPost( std::shared_ptr data_ctx) { // 打印当前帧人脸计数 auto stream_count = std::static_pointer_cast(data_ctx->GetPrivate(\"frame_count\")); MBLOG_INFO data_ctx) { // 打印视频流的人脸计数 auto stream_count = std::static_pointer_cast(data_ctx->GetPrivate(\"stream_count\")); MBLOG_INFO 编译安装 生成的c++功能单元模板中包含CMakeLists.txt文件，主要流程如下： 设置功能单元名称 链接功能单元所需头文件 链接功能单元所需库 设置编译目标为动态库 指定功能单元安装目录 需要特殊说明的是编译生成的so命名需要libmodelbox-开头，否则modelbox无法扫描到。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/service-plugin/service-plugin.html":{"url":"develop/service-plugin/service-plugin.html","title":"服务插件","keywords":"","body":"Modelbox服务插件 Modelbox服务插件介绍 什么是Modelbox服务插件 服务插件是指基于ModelBox框架对外交互的组件，它可以用来作为AI应用和周边系统对接的桥梁。ModelBox框架提供了服务插件管理和扩展开发能力，用户可以定制化开发属于自己业务的插件来对接第三方平台，ModelBox框架可以将其加载并运行。在服务插件内可以完成流程图的加载和运行、任务的创建和启停，统计数据的收集等。同时，ModelBox框架可以支持多个服务插件的加载。 服务插件使用场景 服务插件在视频场景使用较为普遍，典型使用场景为：视频分析任务需要从外部平台或者组件下发到ModelBox框架进行任务分析时，需要通过服务插件来接受外部的请求并转化为ModelBox框架里的分析任务进行业务分析。同时服务插件也可以实现统计信息的收集并发送给外部运维平台，实现与外部系统的对接。 ModelBox框架提供了预置的服务插件ModelBox Plugin，提供流程图的加载和运行, 见运行流程图章节。在大部分情况下，可以直接使用ModelBox Plugin完成相应的业务功能，当某些场景下，ModelBox Plugin功能无法满足要求时，需要自定义开发服务插件，下面介绍服务插件的具体开发流程。 服务插件开发流程 服务插件开发整体流程如下： 服务插件API ModelBox API按照类型包含： Plugin: 插件创建和启停等重载接口，此接口需要由用户实现 接口 接口功能 说明 CreatePlugin 用户创建服务插件对象，并返回给ModelBox框架 ModelBox框架启动时加载参加时调用 Plugin::Init 用户实现服务插件初始化逻辑，提供系统配置，插件初始化时调用 ModelBox框架启动时，在CreatePlugin成功后插件初始化调用；不能存在阻塞操作 Plugin::Start 用户实现服务插件启动逻辑，插件启动时调用 插件启动时调用 Plugin::Stop 用户实现服务插件停止逻辑，插件停止时调用 ModelBox框架进程退出时插件停止时调用 Job： 任务管理组件 任务管理组件提供任务的添加，删除，查询。ModelBox框架任务管理存在以下几种对象概念： Job：算法服务层面的任务，一个Job加载一个流程图。 JobManager：Job的管理，可以创建Job对象。 Task：处理数据源层面的作业，一个Task即对应一次数据分析，可以是一路视频流的分析，也可以是一个视频文件的分析。Task可以实现数据输入到流程图(需要配合Input节点使用)，也可以实现配置参数传递到功能单元。 TaskManager：Task的管理，可以创建Task对象。 OneShotTask: 继承自Task，一次task，专指只有一次数据输入到流程图的场景，比如输入为一路视频流的地址，只会有一次数据传递给流程图，而后需要等待分析结果。所以OneShotTask还提供了task状态变化的回调注册接口。 Session：会话信息，一个Task对应存在一个Session，Session中的数据可以在不同功能单元共享访问。 具体接口如下表： 接口 接口功能 说明 JobManager::CreateJob 创建Job JobManager::DeleteJob 删除Job JobManager::GetJob 获取某个Job对象 JobManager::GetJobList 获取全部Job对象列表 JobManager::QueryJobStatus 查询某个Job状态 JobManager::GetJobErrorMsg 获取某个异常Job的错误信息 Job::Init 初始化Job对象 Job::Build Job对象资源申请 Job::Run 运行Job对象 Job::Stop 停止Job对象 Job::GetJobStatus 获取某个Job状态 Job::CreateTaskManger 创建TaskManger TaskManager::Start 启动TaskManager TaskManager::Stop 停止TaskManager TaskManager::CreateTask 创建Task TaskManager::DeleteTaskById 删除某个Task TaskManager::GetTaskById 获取某个Task对象 TaskManager::GetTaskCount 获取Task个数 TaskManager::GetAllTasks 获取所有Task对象 TaskManager::SetTaskNumLimit 设置同时并发的Task最大个数 超过设置最大个数时，ModelBox内部会排队处理 Task::Start 启动Task Task::Stop 停止Task Task::GetUUID 获取Task id Task::CreateBufferList 创建输入的buffer数据对象 Task::GetLastError 获取Task错误信息 Task::GetTaskStatus 获取Task状态 Task::GetSessionConfig 获取Session配置对象 获取配置对象后可以通过设置自定义参数，传递给需要的功能单元读取 OneShotTask::FillData 发送数据指流程图 OneShotTask::RegisterStatusCallback 注册任务状态回调函数,任务结束或异常时会调用 Config： 配置对象 配置对象提供从服务配置文件中获取配置信息 Listener：http/https监听组件 Listener监听组件可以注册http服务，监听相关的URI Timer： 定时器组件 定时器组件可以用于启动定时任务 开发例子 插件开发前，请确保： 准备工作 ModelBox Server正确安装并运行。 ModelBox Server Develop安装包正确安装。 编写CMake文件 编写cmake文件，将所有源代码编译为so动态库文件： cmake_minimum_required(VERSION 2.8.2) # 设置插件名称 set(UNIT_NAME \"example-plugin\") project(modelbox-${UNIT_NAME}) # 将当前目录下所有源代码文件加入工程 file(GLOB_RECURSE MODELBOX_UNIT_SOURCE *.cpp *.cc *.c) include_directories(${CMAKE_CURRENT_LIST_DIR}) # 编译插件文件，设置连接的库，生成example-plugin.so set(MODELBOX_SERVER_PLUGIN example-plugin) add_library(${MODELBOX_SERVER_PLUGIN} SHARED ${MODELBOX_UNIT_SOURCE}) target_link_libraries(${MODELBOX_SERVER_PLUGIN} pthread) target_link_libraries(${MODELBOX_SERVER_PLUGIN} rt) # 设置系统安装路径和安装包名称为server-plugin-example install(TARGETS ${MODELBOX_SERVER_PLUGIN} COMPONENT server-plugin-example RUNTIME DESTINATION ${CMAKE_INSTALL_FULL_BINDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_FULL_LIBDIR} ARCHIVE DESTINATION ${CMAKE_INSTALL_FULL_LIBDIR} ) 编写插件入口函数 插件入口流程 #ifndef MODELBOX_MODELBOX_EXAMPLE_PLUGIN_H_ #define MODELBOX_MODELBOX_EXAMPLE_PLUGIN_H_ #include \"modelbox/server/plugin.h\" #include // 插件需要实现的接口 class ModelboxExamplePlugin : public Plugin { public: ModelboxExamplePlugin(){}; ~ModelboxExamplePlugin(){}; // 插件初始化时调用。 bool Init(std::shared_ptr config) override; // 插件开工启动时调用，非阻塞。 bool Start() override; // 插件停止时调用。 bool Stop() override; }; // 插件创建接口 extern \"C\" { std::shared_ptr CreatePlugin() { return std::make_shared(); }; } ModelBox Server先调用插件中的CreatePlugin函数创建插件对象。 插件需要在此函数中，创建插件对象，返回智能指针。 再调用Plugin::Init()初始化插件，入参为TOML文件配置。 插件可在初始化函数中，获取配置，并调用插件自身的初始化功能。 再调用Plugin::Start()启动插件。 插件在Start函数中启动插件服务，申请相关的资源；此函数不能阻塞。 进程退出时，调用Plugin::Stop()停止插件功能。 插件在Stop函数中停止插件的功能，并释放相关的资源。 调用ModelBox Server，ModelBox Library相关接口。 插件调用ModelBox Server，以及ModelBox Library的API进行业务控制和运行。具体参考相关的API。 Job创建流程 使用场景为流程图不依赖于外部给其输入，直接加载即可运行场景。如图片推理服务，数据流可由流程图中的HTTP功能单元产生数据，再比如流程图中读本地文件作为数据源的场景。 Job job_; JobManager job_manager_; bool ModelboxExamplePlugin::Init(std::shared_ptr config) { //创建JobManager job_manager_ = std::make_shared(); //从配置文件获取图配置，config对象为运行时传入的conf配置文件，默认路径为/usr/local/etc/modelbox/modelbox.conf auto graph_path = config->GetString(\"server.flow_path\"); //创建Job job_ = job_manager_->CreateJob(\"my_job\", graph_path); auto ret = job_->Init(); return ret; } bool ModelboxExamplePlugin::Start() { job_->Build(); job_->Run(); return true; } bool ModelboxExamplePlugin::Stop() { auto ret = job_->Stop(); ret = job_manager_->DeleteJob(\"my_job\"); return ret; } Task创建流程 使用场景为流程图运行依赖与外部输入的场景，如分析的视频流信息需要由外部传入服务插件，再有服务插件创建Task，并把相应配置参数数据传递到流程图。 void ModelboxExamplePlugin::ModelBoxTaskStatusCallback(modelbox::OneShotTask *task, modelbox::TaskStatus status) { //实现任务结束或者任务异常时处理逻辑 return; } bool ModelboxExamplePlugin::Start() { job_->Build(); job_->Run(); //创建task manager，携带最大并发数 auto task_manager = job->CreateTaskManger(10); task_manager->start(); //创建task auto task = task_manager->CreateTask(modelbox::TASK_ONESHOT); auto oneshot_task = std::dynamic_pointer_cast(task); //创建buff数据并发送给流程图 auto buff_list = oneshot_task->CreateBufferList(); auto input_cfg = \"{\\\"url\\\"：\\\"xxxx\\\"}\"; //输入需要的配置信息，由流程图中输入节点决定 auto status = buff_list->Build({input_cfg.size()}); if (status != modelbox::STATUS_OK) { return status; } auto buff = buff_list->At(0); auto ret = memcpy_s(buff->MutableData(), buff->GetBytes(), input_cfg.data(), input_cfg.size()); buff->Set(\"source_type\", std::string(\"url\")); //输入需要的配置信息，由流程图中输入节点决定 std::unordered_map> datas; datas.emplace(\"input1\", buff_list);//输入节点名称由流程图决定 status = oneshot_task->FillData(datas); if (status != modelbox::STATUS_OK) { return status; } //填充用户自定义配置 auto config = oneshot_task->GetSessionConfig(); config->SetProperty(\"nodes.{key}\", \"{vaule}\");//config 使用方法见XXX //注册Task状态变更回调函数 oneshot_task->RegisterStatusCallback( [&](OneShotTask *task, TaskStatus status) { t->ModelBoxTaskStatusCallback(task, status); return; }); status = oneshot_task->Start(); if (status != modelbox::STATUS_OK) { return status; } //等待task执行结束 auto task_status = iva_task->GetTaskStatus(); while (task_status != TaskStatus::STOPPED && task_status != TaskStatus::ABNORMAL && task_status != TaskStatus::FINISHED) { sleep(1); } task_manager->stop(); return true; } 服务插件配置使用 插件开发完成后，编译为SO文件。需要将插件加入Modelbox Server配置文件的plugin.files配置项插件配置列表中，即默认路径为/usr/local/etc/modelbox/modelbox.conf的plugin.files配置项。 [plugin] files = [ \"/usr/local/lib/modelbox-plugin.so\", \"/xxx/xxx/example-plugin.so\" ] 配置说明： modelbox-plugin.so为系统预置插件，可根据需要添加 插件按从上到下的顺序加载。 若采用tar.gz包安装的服务，modelbox.conf配置文件在对应的服务目录中。 开发者可扩展增加toml的配置项，在ModelboxExamplePlugin::Init接口的configuration对象中获取即可。 插件加入配置文件后，systemctl restart modelbox重启ModelBox Server生效， 同时服务插件日志将统一收集到ModelBox日志。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/modelbox-tool/modelbox-tool.html":{"url":"develop/modelbox-tool/modelbox-tool.html","title":"modelbox-tool","keywords":"","body":"Modelbox-tool ModelBox Tool是ModelBox套件集成的一个开发、维护工具，提供了开发、维护的常用命令。 在功能上，modelbox-tool包含了如下功能 功能 功能说明 Driver 查看Driver列表及其功能 Flow 快速运行一个流程，快速验证 Key 密码加解密，模型加解密 Server 查看Log，Stack，Slab和Statistics信息 ModelBox为标准的命令行工具，可以使用modelbox-tool -h查看详细的帮助说明 Driver功能 用于查询modelbox driver相关的信息。 此命令组为modelbox-tool driver 查询可用功能单元 在开发过程中，可能需要查询图中需要的插件报列表，这时modelbox-tool可以用于查询当前图的情况。 此命令组为modelbox-tool driver action [options] 查询当前系统中已经安装可用的功能单元： modelbox-tool driver -info 查询指定图使用的功能单元 modelbox-tool driver -info -conf [path/to/graph.conf] Flow功能 流程图相关的功能，用于测试，验证图是否配置正确。 此命令组为modelbox-tool flow 运行调测流程图 在开发过程中，可能需要临时调试图以及对应的功能单元，这时，可以使用modelbox-tool flow命令组的命令。 执行图 modelbox-tool flow -run [path/to/graph.conf] 工具执行后的运行日志，存储在/var/log/modelbox/modelbox-tool.log中。如果需要修改日志级别，或将日志输出到屏幕上，可参考后续章节的内容。 Key功能 key功能包括了模型加解密，密码加密等功能。 此命令组为modelbox-tool key 密码加密 某些情况，需要对存储在本地文件，或图中的密码等敏感信息，加密。 键盘输入密码： modelbox-tool key -pass 标准输入输入密码： modelbox-tool key -pass modelbox-tool key -pass 环境变量输入密码加密 MODELBOX_PASSWORD=\"pass\" modelbox-tool key -pass 注意 默认情况下，加密的密码和设备绑定，若需要和设备无关，则需要增加-n参数。 密码安全性上，键盘输入最可靠，其次是标准输入，环境变量形式不推荐。 编程接口，可以使用popen执行命令，然后write密码到管道中。 Server功能 查看Log，Stack，Slab和Statistics信息。 此命令组为modelbox-tool server Log 查看Control线程的日志 此命令组为modelbox-tool server log Stack 查看Control线程的栈信息 此命令组为modelbox-tool server stack Slab 查看Control线程的内存碎片 此命令组为modelbox-tool server slab Statistics 查看Control线程的统计信息 此命令组为modelbox-tool server stat ModelBox Tool日志设置 modelbox-tool可以支持修改日志级别，输出形式，和日志文件路径，在执行命令时，可以通过如下参数修改 modelbox tool main options: -verbose output log to screen. -log-level log level: DEBUG, INFO, NOTICE, WARN, ERROR, FATAL. -log-path log file: default : /var/log/modelbox/modelbox-tool.log 注意，使用时，上述参数需要紧接modelbox-tool命令后，不能放到子命令组阶段，如 modelbox-tool -verbose flow -run [/path/to/graph.conf] 参数说明 参数 功能说明 -verbose 是否将日志输出到屏幕 -log-level 输出日志级别，可以为debug, info, notice, warn, error, fatal -log-path 输出日志文件，默认为/var/log/modelbox/modelbox-tool.log © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/debug/debug.html":{"url":"develop/debug/debug.html","title":"调试定位","keywords":"","body":"调试 ModelBox提供了多种调试方法，包含了业务运行，性能，和代码的调试。 各个组件调试方法 各个组件的调试方法参考下表： 语言 组件 调试方法 c++ ModelBox套件 编译debug版本，安装并配置GDB，日志。 c++ 自开发服务 编译debug版本，安装并配置GDB，日志。 c++ 流单元 编译debug版本，安装并配置GDB，日志，Profiling。 python 功能单元 PDB，日志，Profiling。 上述表格中，使用GDB、PDB调试的，可以配合IDE完成。 调试指导 调试方法 说明 连接 代码调试 代码级别的调试方法，主要使用现有的调试工具，IDE进行调试。 指导 运行调试 使用运行日志，业务代码使用log类函数打印相关的日志。 指导 性能调试 对图的执行进行数据打点，并输出甘特图供性能分析调试。 指导 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/debug/code-debug.html":{"url":"develop/debug/code-debug.html","title":"代码调试","keywords":"","body":"debug 代码调试使用对应语言的调试方法即可，c++使用gdb，python使用pdb。 GDB调试方法 C++调试使用GDB即可，在调试前，需要将对应的软件编译为DEBUG版本。 编译debug版本 使用CMake的编译参数，编译为DEBUG版本。 mkdir build cd build cmake -DCMAKE_BUILD_TYPE=Debug .. make -j32 配置调试流程图 配置构造一个简单的flow流程图，确保被调测组件能被调用。 启动调试 若是调试功能单元，可以使用modelbox-tool辅助调试。 GDB命令 gdb modelbox-tool set args -verbose -log-level info flow -run [path/to/graph.conf] b [some-func] r 上述命令意思为： 使用gdb启动modelbox-tool 设置运行参数-verbose -log-level info表示显示日志，及设置日志级别 flow -run [path/to/graph.conf]表示运行的调测流程图。 b [some-func]对指定的函数进行断点。 r 运行命令 vscode vscode调试，可以先下载GDB插件，再配置调试文件.vscode/launch.json，设置program和args两个配置项如下。 \"program\": \"modelbox-tool\", \"args\": [ \"-verbose\", \"-log-level\", \"info\", \"flow\", \"-run\", \"[path/to/graph.toml]\" ], 设置完成后，使用vscode的F5功能键进行调试 Python调试方法 Python调试时，则需要先设置环境变量MODELBOX_DEBUG_PYTHON=yes后，直接使用IDE调试，其调试方法和标准的python脚本类似。 环境变量可通过python脚本，或启动进程前的shell命令设置。 python中设置启用 import os # 设置环境变量 os.environ['MODELBOX_DEBUG_PYTHON']=\"yes\" # 执行流程图 flow = modelbox.Flow() ... 环境变量中启用 export MODELBOX_DEBUG_PYTHON=yes 注意： 若启用失败，则需要先安装pydevd包。 pip install pydevd © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/debug/log.html":{"url":"develop/debug/log.html","title":"日志","keywords":"","body":"日志 ModelBox提供了运行日志，对应的组件在运行时会输出相关的日志信息。 日志的基本流程 C++，Python功能单元、ModelBox库、插件调用ModelBox的日志函数后，由ModelBox的Logger将数据发送到appender，appender可注册不同的类型。 ModelBox的日志级别分为DEBUG, INFO, NOTICE, WARN, ERROR, FATAL。 ModelBox Server日志 ModelBox Server/Tool内置了File日志组件，在运行时，会将对应的日志记录到相关的文件中。对应的日志路径，配置方法如下： 进程 日志路径 级别设置 modelbox server /var/log/modelbox/modelbox.log /usr/local/etc/modelbox/modelbox.conf配置文件中，log字段的level。 modelbox tool /var/log/modelbox-tool.log modelbox-tool命令参数。 默认console日志 ModelBox Library在未设置输出appender的情况下，所有打印输出到console，且默认情况下日志输出关闭，若要设置日志级别，可以通过环境变量设置。可设置的变量值为DEBUG, INFO, NOTICE, WARN, ERROR, FATAL。 export MODELBOX_CONSOLE_LOGLEVEL=INFO 日志SDK ModelBox日志提供了日志输出接口，日志appender捕获接口；功能单元，ModelBox库，插件使用日志接口输出日志，业务模块使用appender捕获日志到对应的日志组件。 日志输出信息包括 level: 日志级别 file: 日志文件 lineno: 行号 func: 函数名称 msg: 日志内容 appender可以按需求输出日志。 不同语言的SDK日志调用接口，日志捕获接口如下： C++ c++日志调用 C++调用日志时，需要包含头文件，然后使用类似std::cout的语法输出日志。 #include void LogExample() { MBLOG_DEBUG c++日志捕获 c++提供了日志接口logger，只需要实现logger中的方法，即可将日志重定向。 class Logger { public: // vprint接口 virtual void Vprint(LogLevel level, const char *file, int lineno, const char *func, const char *format, va_list ap); // print接口 virtual void Print(LogLevel level, const char *file, int lineno, const char *func, const char *msg); // 设置日志级别 virtual void SetLogLevel(LogLevel level); // 获取日志级别 virtual LogLevel GetLogLevel() = 0; }; // 注册日志函数 ModelBoxLogger.SetLogger(logger); 流程： 编写自定义日志对象，从Logger派生，实现相关的接口 初始化时，调用ModelBoxLogger.SetLogger(logger)注册日志处理函数。 调用ModelBoxLogger.GetLogger->SetLogLevel(level)设置日志级别。 Python Python日志调用 python输出日志时，需要包含modelbox包，使用上类似，print函数。 import modelbox modelbox.debug(\"this is debug\") modelbox.info(\"this is info\") modelbox.notice(\"this is notice\") modelbox.warn(\"this is warning\") modelbox.error(\"this is error\") modelbox.fatal(\"this is fatal\") Python日志捕获 # 导入相关的包 import modelbox import datetime __log = modelbox.Log() # 日志捕获函数 def LogCallback(level, file, lineno, func, msg): # 输出日志信息 print(\"[{time}][{level}][{file}:{lineno}] {msg}\".format( time=datetime.datetime.now(), level=level, file=file, lineno=lineno, msg=msg )) # 日志注册函数 def RegLog(): # 注册日志函数 __log.reg(LogCallback) # 设置日志级别为INFO __log.set_log_level(modelbox.Log.Level.INFO) # 注册自定义日志 RegLog() 流程： 编写自定义函数，函数原型为logfunc(level, file, lineno, func, msg)。 初始化日志对象modelbox.Log()。 将logfunc调用modelbox.Log::reg注册为日志处理函数 调用modelbox.Log::set_log_level(modelbox.Log.Level)设置日志级别。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/debug/profiling.html":{"url":"develop/debug/profiling.html","title":"性能","keywords":"","body":"性能统计 ModelBox提供了统计功能单元，以及运行任务的时间统计功能，开发者或维护人员可以开启性能统计功能，对功能单元或运行状态进行调试和维护。 性能统计操作流程 配置流程图。 配置文件中指定启动profiling。 运行流程图。 获取统计信息。 chrome浏览器打开chrome://tracing/。 优化代码，重新分析。 性能满足要求后，结束。 启用性能统计 启动ModelBox的性能统计功能，只需要在Flow的toml配置文件中增加如下配置，即可启用。 [profile] profile=\"enable\" # 启用profile trace=\"enable\" # 启用traceing dir=\"/tmp/modelbox/perf\" # 设置跟踪文件路径。 通过配置profile和trace开关启用性能统计，dir配置存储跟踪文件路径；配置启动后，启动运行流程图，profile会每隔60s记录一次统计信息，trace会在任务执行过程中和结束时，输出统计信息。 显示性能统计 运行流程图后，会周期生成timeline性能相关的json文件，通过将json文件加载到chrome trace viewer中即可查看timeline信息。 打开chrome浏览器。 浏览器中输入chrome://tracing/。 点击界面中的Load按钮，加载trace的json文件。 加载成功后，将看到类似下面的timeline视图。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/device/device.html":{"url":"develop/device/device.html","title":"设备接口","keywords":"","body":"设备接口 ModelBox支持多种设备的编程，其中包括对Huawei Ascend，Nvidia Cuda的支持，这两种设备都有流水线接口，即Stream接口。为使用Stream接口，在开发功能单元时，需要实现和设备相关的功能单元，具体链接如下： 设备 说明 连接 Ascend Huawei Ascend 链接 Cuda Nvidia Cuda 链接 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/device/ascend.html":{"url":"develop/device/ascend.html","title":"ASCEND","keywords":"","body":"Ascend卡 Huawei Ascend ACL支持接口编程，ACL接口相关的介绍，请点击此处 ModelBox为更好的支持Stream并发编程，默认情况下，ModelBox的Ascend ACL接口全部采用Stream模式，开发者需要在编程时，使用Ascend ACL的Stream接口以提升性能。 Ascend ACL功能单元接口 ModelBox框架会自动管理Stream，开发功能单元时，开发者可以通过process的入参获取到Stream，之后可以用于ACL接口的调用中。 在实现功能单元之前，Ascend ACL相关的功能单元，需要从AscendFlowUnit派生，并实现AscendProcess接口。 class SomeAscendFlowUnit : public modelbox::AscendFlowUnit { public: SomeAscendFlowUnit() = default; virtual ~SomeAscendFlowUnit() = default; // 数据处理接口，需要实现AscendProcess，第二个参数为Ascend ACL Stream。 virtual modelbox::Status AscendProcess(std::shared_ptr data_ctx, aclrtStream stream); }; 除AscendProcess以外，其他接口和通用功能单元一致，AscendProcess接口如下： modelbox::Status ColorTransposeFlowUnit::AscendProcess( std::shared_ptr data_ctx, aclrtStream stream) { auto inputs = ctx->Input(\"input\"); auto outputs = ctx->Output(\"output\"); // 申请内存 std::vector data_size_list(1, 2, 3); outputs->Build(data_size_list); // 循环处理每个输入数据，并产生相关的输出结果。 for (size_t i = 0; i Size(); ++i) { // 获取数据元数据信息 auto meta = inputs[i].Get(\"Meta\", \"Default\"); // 获取输入，输出的内存指针。输入为const只读数据，输出为可写入数据。 auto input_data = inputs[i].ConstData(); auto output_data = outputs[i].MutableData(); // 使用Stream处理数据 // aclmdlExecuteAsync(model_id_, input.get(), output.get(), stream); // 设置输出Meta outputs[i].Set(\"Meta\", \"Meta Data\"); } return modelbox::STATUS_OK; } 数据处理时，Ascend Stream会自动由ModelBox框架生成，再调用Ascend ACL接口时，直接使用此Stream对象即可。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"develop/device/cuda.html":{"url":"develop/device/cuda.html","title":"Nvidia Cuda","keywords":"","body":"Nvidia Cuda显卡 Nvidia Cuda支持stream并发编程，什么是stream可参考此处 ModelBox为更好的支持Stream并发编程，默认情况下，ModelBox的Cuda接口全部采用Stream模式，开发者需要在编程时，使用Cuda的Stream接口以提升性能。 Cuda功能单元接口 ModelBox框架会自动管理Stream，开发功能单元时，开发者可以通过process的入参获取到Stream，之后可以用于Cuda接口的调用中。 在实现功能单元之前，cuda相关的功能单元，需要从CudaFlowUnit派生，并实现CudaProcess接口。 class SomeCudaFlowUnit : public modelbox::CudaFlowUnit { public: SomeCudaFlowUnit() = default; virtual ~SomeCudaFlowUnit() = default; // 数据处理接口，需要实现CudaProcess，第二个参数为Cuda Stream。 virtual modelbox::Status CudaProcess(std::shared_ptr data_ctx,cudaStream_t stream); }; 除CudaProcess以外，其他接口和通用功能单元一致，CudaProcess接口如下： modelbox::Status ColorTransposeFlowUnit::CudaProcess( std::shared_ptr data_ctx, cudaStream_t stream) { auto inputs = ctx->Input(\"input\"); auto outputs = ctx->Output(\"output\"); // 申请内存 std::vector data_size_list(1, 2, 3); outputs->Build(data_size_list); // 循环处理每个输入数据，并产生相关的输出结果。 for (size_t i = 0; i Size(); ++i) { // 获取数据元数据信息 auto meta = inputs[i].Get(\"Meta\", \"Default\"); // 获取输入，输出的内存指针。输入为const只读数据，输出为可写入数据。 auto input_data = inputs[i].ConstData(); auto output_data = outputs[i].MutableData(); // 使用Stream处理数据 // kernel3 >> ( …, … ) ; // 设置输出Meta outputs[i].Set(\"Meta\", \"Meta Data\"); } return modelbox::STATUS_OK; } 数据处理时，Stream会自动由ModelBox框架生成，再调用Cuda接口时，直接使用此Stream对象即可。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"solution/solution.html":{"url":"solution/solution.html","title":"案例","keywords":"","body":"ModelBox 推理案例 Modelbox同时也提供了一些推理的案例 案例名称 案例图例 案例链接 车辆检测 车辆检测案例 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"solution/modelbox-guide.html":{"url":"solution/modelbox-guide.html","title":"ModelBox上手指南","keywords":"","body":"ModelBox 上手指南 准备工作 服务器端启动镜像 根据服务器端硬件规格下载ModelBox开发镜像，如X86+GPU的服务器可以用如下命令下载： docker pull modelbox/modelbox_cuda101_develop:v1.0.8 Arm+D310的服务器（如Atlas500）可以用如下命令下载： docker pull /modelbox/modelbox_ascend_aarch64_develop:v1.0.8 其中cuda版本、镜像版本可以根据需要进行选择。下载成功后可以用docker images命令查看本机已有的镜像： 启动ModelBox镜像，X86+GPU版本可使用如下脚本： #! /bin/bash # ssh map port [modify] SSH_MAP_PORT=50011 # editor map port [modify] EDITOR_MAP_PORT=1104 # http server port [modify] HTTP_SERVER_PORT=7788 # container name [modify] CONTAINER_NAME=\"modelbox_instance_`date +%s`_xxx\" HTTP_DOCKER_PORT_COMMAND=\"-p $HTTP_SERVER_PORT:$HTTP_SERVER_PORT\" sudo docker run -itd --gpus all \\ -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,video \\ --tmpfs /tmp \\ --tmpfs /run \\ -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ --name $CONTAINER_NAME \\ -v /opt/modelbox:/opt/modelbox \\ -v /home:/home \\ -p $SSH_MAP_PORT:22 \\ -p $EDITOR_MAP_PORT:1104 $HTTP_DOCKER_PORT_COMMAND \\ modelbox/modelbox_cuda101_develop:v1.0.8 Arm+D310版本可使用如下脚本： #!/bin/bash # ssh map port, [modify] SSH_MAP_PORT=50011 # editor map port [modify] EDITOR_MAP_PORT=1144 # http server port [modify] HTTP_SERVER_PORT=7788 # container name [modify] CONTAINER_NAME=\"modelbox_instance_arm64_`date +%s`_xxx\" HTTP_DOCKER_PORT_COMMAND=\"-p $HTTP_SERVER_PORT:$HTTP_SERVER_PORT\" sudo docker run -itd --privileged --cap-add=SYS_PTRACE \\ --tmpfs /tmp --tmpfs /run -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ --device=/dev/davinci0 --device=/dev/davinci_manager --device=/dev/hisi_hdc --device=/dev/devmm_svm \\ --name $CONTAINER_NAME -v /opt/modelbox:/opt/modelbox -v /home:/home \\ -p $SSH_MAP_PORT:22 -p $EDITOR_MAP_PORT:1144 $HTTP_DOCKER_PORT_COMMAND \\ modelbox/modelbox_ascend_aarch64_develop:v1.0.8 脚本中注明[modify]的地方都可以根据自己的需要修改 启动后可以用docker ps –a|grep modelbox看到本机已启动的所有镜像，其中镜像ID在第一列： 使用docker exec -it $docker_id bash命令进入ModelBox容器： 进入容器后使用passwd root修改root用户密码（注意密码要求至少三类字符的组合——数字、字母、特殊符号）： 安装RTSP服务器，用于推送实时的RTSP视频流（注意是在容器外安装执行）。X86+GPU的服务器可以选择EasyDarwin，Arm版可以选择rtsp-simple-server。以rtsp-simple-server为例，下载链接为https://github.com/aler9/rtsp-simple-server/releases，下载后解压得到可执行文件和配置文件： 打开rtsp-simple-server.yml可修改RTSP服务相关的配置参数，如端口号默认为8554，可以根据需要修改： 修改后执行命令nohup ./rtsp-simple-server &，即在后台启动了RTSP服务器。 PC端使用vscode远程连接ModelBox容器 安装Visual Studio Code（下称vscode，注意不要用1.58.x版本，最高只能用1.57.1，安装后关闭自动升级） vscode中安装 Remote-SSH、Remote-Containers、Docker等插件 vscode上使用remote ssh添加连接： 在config中按照如下方式配置ModelBox容器的远程连接： 设置完成后，旁边目录就会出现远程目标，点击后面的图标，重新打开一个窗口： 打开之后，输入上面修改的容器root账号密码，稍等片刻，就可以连接上了： 点击Open folder, 可以直接打开docker中的文件： 想要在代码中实现跳转，需要在远端安装跳转工具，点击插件，然后插件上会出现 install in SSH: ...。点击后会就在远端安装该插件，一般安装C/C++, python等，安装好之后会在插件的SSH栏中出现： 安装PotPlayer播放器，用于播放rtsp视频流 第一个ModelBox应用 准备工作完成后，就可以在vscode中开发ModelBox应用了，第一个应用非常简单：打开一个mp4视频文件，推送到RTSP服务器，然后在PC端使用PotPlayer播放出来。 流程图开发 ModelBox根据流程图构建应用的处理逻辑，我们使用TOML格式的文件来描述第一个应用： [driver] dir = [\"/usr/local/\"] skip-default = false [profile] profile=false trace=false dir=\"/tmp/\" [log] level=\"DEBUG\" [graph] format = \"graphviz\" graphconf = \"\"\"digraph test { node [shape=Mrecord] queue_size = 16 video_input[type=flowunit, flowunit=video_input, device=cpu, deviceid=0, source_url=\"xxx/xxx.mp4\"] videodemuxer[type=flowunit, flowunit=videodemuxer, device=cpu, deviceid=0] videodecoder[type=flowunit, flowunit=videodecoder, device=cpu, deviceid=0, pix_fmt=rgb] videoencoder[type=flowunit, flowunit=videoencoder, device=cpu, deviceid=0, default_dest_url=\"rtsp://ip:8554/stream\", format=rtsp] video_input:stream_meta -> videodemuxer:stream_meta videodemuxer:video_packet -> videodecoder:video_packet videodecoder:frame_info -> videoencoder: frame_info }\"\"\" [flow] desc = \"test for video streams\" 使用时，需要将video_input单元中的source_url属性内容修改为实际的mp4文件路径，将videoencoder单元default_dest_url属性内容中的ip修改为真实的服务器ip，端口号8554要与rtsp-simple-server或者EasyDarwin中配置的端口号保持一致。 如上所示，ModelBox使用graphviz格式描述流程图，我们可以把中间那一段digraph流程图定义内容拷贝到graphviz工具（如：https://dreampuf.github.io/GraphvizOnline）中进行查看： 流程图执行 流程图开发好之后，在vscode terminal中使用modelbox-tool -verbose -log-level INFO flow -run xxx/xxx.toml执行该流程图： 打开浏览器，输入TOML文件中配置的default_dest_url地址，点击弹出对话框中的“打开PotPlayer专用播放”按钮： 弹出的PotPlayer将会播放TOML文件中配置的mp4视频文件 开发自己的流单元 第一个应用中使用的都是ModelBox自带的流单元，接下来我们用Python开发一个最简单的流单元，嵌入到上面的应用中：在画面左上方写上“Hello World”，再输出。 新建流单元 在vscode terminal中执行modelbox-tool create -t python -n HelloWorld -d path-to-flowunits： 命令执行后将在指定路径下基于Python流单元模板生成HelloWorld流单元，包括一个py文件和一个toml配置文件： 在HelloWorld.toml中配置该流单元的名称、类别、输入输出端口等信息，当前不用修改；HelloWorld.py中描述了该流单元的处理逻辑，这里我们增加OpenCV与numpy包的引用，修改其中的process函数如下： import cv2 import numpy as np … def process(self, data_context): # Process the data in_data = data_context.input(\"Input\") out_data = data_context.output(\"Output\") for buffer in in_data: width = buffer_img.get('width') height = buffer_img.get('height') channel = buffer_img.get('channel') img_data = np.array(buffer_img.as_object(), copy=False) img_data = img_data.reshape((height, width, channel)) cv2.putText(img_data, 'Hello World', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2) out_buffer = self.create_buffer(img_data) out_buffer.copy_meta(buffer_img) out_data.push_back(out_buffer) return modelbox.Status.StatusCode.STATUS_SUCCESS 流程图执行方式 修改流程图TOML文件如下： [driver] dir = [\"/usr/local/\" , \"path-to-HelloWorld-flowunits\"] skip-default = false [profile] profile=false trace=false dir=\"/tmp/\" [log] level=\"DEBUG\" [graph] format = \"graphviz\" graphconf = \"\"\"digraph test { node [shape=Mrecord] queue_size = 16 video_input[type=flowunit, flowunit=video_input, device=cpu, deviceid=0, source_url=\"xxx/xxx.mp4\"] videodemuxer[type=flowunit, flowunit=videodemuxer, device=cpu, deviceid=0] videodecoder[type=flowunit, flowunit=videodecoder, device=cpu, deviceid=0, pix_fmt=rgb] HelloWorld[type=flowunit, flowunit=HelloWorld, device=cpu, deviceid=0] videoencoder[type=flowunit, flowunit=videoencoder, device=cpu, deviceid=0, default_dest_url=\"rtsp://ip:8554/stream\", format=rtsp] video_input:stream_meta -> videodemuxer:stream_meta videodemuxer:video_packet -> videodecoder:video_packet videodecoder:frame_info -> HelloWorld: Input HelloWorld: Output -> videoencoder: frame_info }\"\"\" [flow] desc = \"test for video streams\" 其中[driver]配置项的dir中添加了HelloWorld流单元的路径，graphviz图定义中插入HelloWorld流单元，新的流程图如下所示： 在vscode terminal中执行该流程图，通过浏览器打开PotPlayer播放，可看到输出的视频。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"solution/car-detect.html":{"url":"solution/car-detect.html","title":"车辆检测","keywords":"","body":"车辆检测 流程图 车牌检测流程如下图所示，video_input 功能单元接收视频流，往下分别经过 videodemuxer和videodecoder功能单元，videodecoder功能单元输出image，image经过前处理，包含resize、normalize之后，送给模型（vehicle_inference是一个yolov3模型），模型将推理得到的bbox结果传入后续的后处理功能单元进行处理（vehicle_bbox_post），可得到 最终的bbox框，将bbox框和videodecoder出来的image一同送入draw_bbox中，将绘制完bbox的image传入videoencoder，即得到带有检测框的视频。 上述提到的各个节点，在ModelBox中称为流程元（流程单元），模型图中的一个节点，编排功能单元构建运行图，运行图在ModelBox中的呈现形式为 toml文件。车辆检测运行toml文件内容如下： [driver] dir = [\"drivers\"] [log] level = \"INFO\" [graph] format = \"graphviz\" graphconf = \"\"\"digraph vehicle_detection { queue_size = 32 batch_size = 16 video_input[type=flowunit, flowunit=video_input, device=cpu, deviceid=0, source_url=\"test_video_vehicle.mp4\"] videodemuxer[type=flowunit, flowunit=videodemuxer, device=cpu, deviceid=0] videodecoder[type=flowunit, flowunit=videodecoder, device=cuda, deviceid=0, pix_fmt=rgb] frame_resize[type=flowunit, flowunit=nppi_resize, device=cuda, deviceid=0, width=800, height=480] vehicle_normalize[type=flowunit, flowunit=normalize_v2, device=cuda, deviceid=0, output_layout=\"CHW\", output_dtype=\"MODELBOX_FLOAT\", mean=\"0,0,0\", std=\"0.003921568627451,0.003921568627451,0.003921568627451\"] vehicle_inference[type=flowunit, flowunit=vehicle_inference, device=cuda, deviceid=0] vehicle_yolobox[type=flowunit, flowunit=vehicle_yolobox, device=cpu, deviceid=0] vehicle_bbox_post[type=flowunit, flowunit=vehicle_bbox_post, device=cpu, deviceid=0] draw_bbox[type=flowunit, flowunit=draw_bbox, device=cpu, deviceid=0] videoencoder[type=flowunit, flowunit=videoencoder, device=cpu, deviceid=0, default_dest_url=\"rtsp://localhost/video\", encoder=\"mpeg4\"] video_input:stream_meta -> videodemuxer:stream_meta videodemuxer:video_packet -> videodecoder:video_packet videodecoder:frame_info -> frame_resize:In_1 frame_resize: Out_1 -> vehicle_normalize: input vehicle_normalize: output -> vehicle_inference:data vehicle_inference: \"layer15-conv\" -> vehicle_yolobox: In_1 vehicle_inference: \"layer22-conv\" -> vehicle_yolobox: In_2 vehicle_yolobox: Out_1 -> vehicle_bbox_post: In_bbox videodecoder:frame_info -> vehicle_bbox_post: In_image vehicle_bbox_post: Out_bbox -> draw_bbox: In_1 videodecoder:frame_info -> draw_bbox: In_2 draw_bbox: Out_1 -> videoencoder: frame_info }\"\"\" toml构建图，定义节点和构建节点之间关系即可完成。输入配置在video_input中source_url中配置实际的视频所在路径，输出通过videoencoder输出rtsp流。 其中，[dirver]中dir的路径为，图中功能单元的so包或toml配置文件所在路径。 运行示例 用ModelBox的modelbox-tool命令，可以启动运行图。命令如下： modelbox-tool -verbose flow -run vehicle_detection.toml 其中vehicle_detection.toml即为车牌检测的运行图，shell命令中为实际路径，运行环境中安装好EasyDarwin软件后，将 rtsp://localhost/video（localhost为你的ip）复制到网页中，即可打开浏览器，查看最后结果，示例结果如下： © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"framework-conception/framework-conception.html":{"url":"framework-conception/framework-conception.html","title":"框架概念","keywords":"","body":"框架概念 ModelBox为更好的支撑应用流程，抽象了许多概念，理解这些概念对于应用问题的解决将会有很大帮助。 基础概念 相信在阅读完基本概念章节后，开发者已经对ModelBox的整体概念有了初步的认识，这可能已经足够应付部分的使用场景，但还是建议继续如下的阅读，帮助对ModelBox有更深层次的理解，助力应用的开发。 本章节主要从下图中的几个概念对ModelBox进行讲解，它们是ModelBox中十分重要的概念，开发者将时刻与它们打交道。 数据经由INPUT Node产生，按箭头指向，流向Process Node，Process Node处理数据后，在发送给Sink Node汇总处理结果，这是一个典型的数据处理过程，这个过程中，涉及到了图(Graph)、节点(Node)、端口(Port)、数据流(Stream)、数据缓存(Buffer)。 Graph 图定义了ModelBox的执行过程，ModelBox根据图中指示的路径，调度Node的功能，处理Buffer数据。图的详细说明可阅读图章节的内容。 Node 图的基本组成，是功能单元在图中的实例化，和输入、输出Port一起组成了数据的处理实体。不同Node间由边(edge)连接，边是有向的。功能单元的详细说明可阅读功能单元的内容。 Port 节点上的数据连接点，可用于数据的输入或者输出，在图中两个Node之间的连接需要指定Port。 Stream 一系列关联的顺序数据实体组成了数据流，在ModelBox中数据流是主要处理对象，比如视频流，音频数据流等。数据流的详细说明可阅读Stream流的内容。 Buffer 流中包含多个数据实体，单个数据实体在ModelBox中由buffer承载。单个buffer包含了数据的元数据Meta部分和数据内容部分，它是数据在Node间的流动实体。Buffer的详细说明可阅读Buffer的内容。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"framework-conception/graph.html":{"url":"framework-conception/graph.html","title":"图","keywords":"","body":"图 在Modelbox中由多个flowunit进行连接构成的实际业务的执行集合就是图。图是ModelBox的主要组件，ModelBox的数据处理过程，完全按照图中的拓扑关系进行。之前已经介绍过图的基本使用，本章主要介绍图的连接约束,图的加载方式，图的执行原理及优先级 图的连接约束 输入输出的约束 ModelBox中的图至少需要包含两种flowunit：source flowunit，sink flowunit。 source flowunit： 有输出无输入，如input，videoinput，httpserver_sync_receive和httpserver_async_receive。 sink flowunit： 有输入无输出，如output，httpserver_sync_reply和httpserver_async_reply。 ModelBox的图只能有一个source flowunit，这里主要是因为ModelBox中的运行的数据需要匹配，如果数据源是不同的，则其中的数据不一定能匹配。比如写了两个httpserver_sync_receive的flowunit，则会启动两个http服务来接受不同的请求，但无法确定两边的请求数量是否完全一致，如数量不一致时会出现无法匹配的情况，导致图中的数据无法正常运行。这里存在一个例外，即一个图中input flowunit是可以有多个的，因为对其输入做了强制匹配的要求，即多个input输入的buffer的数量必须是一样多的。 ModelBox的图可以有多个sink flowunit，如可以使用多个output flowunit，默认情况下，多个output flowunit的输出是匹配的。如果输出的部分不匹配则需要在output的节点的配置上增加output_type=unmatch的配置 回路的约束 ModelBox的图不支持出现回路的情况，因为回路会导致数据会循环流动而无法消耗。 匹配的约束 ModelBox的图在拼接时会检查图中的节点的输入边的数据是否是匹配的。如果出现某个节点输入的边互相不匹配则会报告图非法。详细的图匹配规则如下所示： 在上图中，第一个数据流在经过通用flowunit后，其产生的数据流与之前输入的数据流是可以匹配的，因此其输入输出数据流都是流a。第二个数据流在经过流flowunit后，其产生的数据流与之前输入的数据流是不可以匹配的，因此输入的数据流是流a，而输出的数据流是流b，流a与流b之间是无法匹配的。当功能单元的开发者确认流flowunit的输入输出数据是一样多的时候，可以在流flowunit中SetStreamSameCount(true)，增加了这个配置以后则输入和输出的数据都是流a，可以匹配。 用户在开发时可以根据输入的流是否匹配来检查一下自己的图是否合法，在上图中左侧的图因为某一条边上加入了stream功能单元，产生了流b，流b与流a无法匹配，因此左图是一个非法图。将该stream功能单元SetStreamSameCount(true)后其输出的流恢复为流a,可以匹配。因此右图可以正常运行。 条件分支的约束 在图中有条件功能单元的情况下，主流不能与子流做匹配，只有等全部的子流聚合恢复为主流后才可以与主流进行匹配。子流和主流的关系在条件功能单元数据处理有更详细的介绍 如上图所示，在左图中经过条件功能单元后，主流a会产生分流a1和分流a2，分流a1和分流a2都只有主流a的部分数据，因此不可以与主流a直接进行匹配，因此左图是非法的。在右图中，分流a1和分流a2在最后的通用功能单元聚合后恢复成主流a，可以与主流a进行匹配，因此该图是合法的 同时条件功能单元不允许在子流上添加流Flowunit，因为流Flowunit会按照主流中的序号顺序运行，而在子流中会出现缺失序号顺序的情况。如下图所示： 左图是一个非法的图，因为其在condition算子后的一个分路上加入了一个流Flowunit。右图将其流Flowunit设置为通用的Flowunit后，图恢复为合法图 层级的约束 在图有层级的情况下，子层与父层之间不能直接进行匹配。需要先将子层归拢为父层之后才能进行匹配。关于子层与父层之间的关系在数据流层级有更详细的介绍 左图中父流a在通用Flowunit 2上与子流a1和子流a2是无法匹配的才，因此左图是非法的。在右图中，将子流a1和子流a2收拢成为父流a后即可以与通用功能单元的另一路父流a进行匹配，因此右图是合法的 图的加载过程 图的加载过程如下所示： 图的执行及优先级 在图被放入调度器后，调度器会按照拓扑结构对图进行优先级排序，即越靠近出口的节点的优先级越高，这样做的目的是为了保证现有的请求可以尽快的完成，而不至于在入口处累计过多的buffer造成内存或显存的浪费。节点的优先级如下图所示： 当图中的某个节点收到数据时，会触发调度器去使用一个线程去执行该节点的run函数，并处于running状态直到run函数执行完毕。当该节点处在running状态的时候，后续的收到的数据会填入该节点的接收队列，但并不会触发run函数，直到该节点执行完run函数。在run的过程中，数据会按照流或者batch切分成多份，放入不同的线程去执行。当线程数量不够时，高优先级的节点会优先放入待执行的队列中，已保证高优先节点可以更快的完成。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"framework-conception/flowunit.html":{"url":"framework-conception/flowunit.html","title":"流单元","keywords":"","body":"功能单元 功能单元FlowUnit是ModelBox中处理数据的基本单元，可称之为流程单元，简称功能单元，功能单元的处理对象是数据流。在ModelBox中，功能单元根据数据处理需求差异划分为不同的类别，下面将从功能单元组成、加载、分类进行详细的讲解。 功能单元组成 功能单元是数据处理的基本功能单元，实现可以是多种语言和设备类型的。 功能单元可选的实体功能组件如下： 类型 设备类型 说明 开发指导 C++动态库 CPU 使用CPU执行的C++动态库 指导 C++动态库 Cuda 使用Cuda执行的C++动态库 指导 C++动态库 Ascend 使用Ascend执行的C++动态库 指导 Python模块 CPU 使用CPU执行的Python模块 指导 Python模块 Cuda 使用Cuda执行的Python模块 指导 Python模块 Ascend 使用Ascend执行的Python模块 指导 Java模块 CPU 使用CPU执行的Java模块 指导 推理 TensorRT 使用TensorRT推理框架的模型推理功能 指导 推理 TensorFlow 使用TensorFlow推理框架的模型推理功能 指导 ModelBox可根据图中的配置，加载对应的功能单元实体功能组件，在功能单元开发时，开发者也可以选择相关类型的功能单元实体进行开发。 加载运行流程 FlowUnit插件的加载流程如上图： 初始化过程 ModelBox模块先扫描插件目录。 对扫描到的插件，调用DriverDescription获取FlowUnit插件信息。信息包括功能单元名称，功能单元版本号，执行的硬件信息，描述信息，配置参数信息。 当启动Flow时，则调用插件的DriverInit初始化插件。 初始化插件完成后，调用CreateDriverFactor创建插件工厂类。 之后调用FLowUnitFactor::FlowUnitProbe获取FlowUnit信息，信息包括输入，输出参数。 初始化完成后，业务调用Flow初始化图。 在图初始化时，首先通过图中指定的FlowUnit名称选择对应的FlowUnit实例化，调用实例化对象的Flow::Open初始化FlowUnit对象。 图初始化完成后，若有数据达到，则调用当前节点实例的Flow::Process处理数据；若当前功能单元选择的类型是流，还会调用Flow::DataPre接口，再调用Flow::Process接口，流数据结束时，调用Flow::DataPost接口。 当图运行完成后，则调用Flow::Close接口关闭图。 整个Flow结束时，ModelBox模块卸载FlowUnit插件，调用插件的DriverFini函数清理资源。 分类 功能单元被按照多个维度进行了分类，不同纬度的分类在满足约束的情况下可以进行组合 维度一：按处理数据相关性分类 类别 类别说明 约束 常见的算子 需要重写的接口 |通用功能单元|在处理数据时，功能单元本身不关心数据之间的关联，只对当前数据做处理，并且不记录任何状态，则选择该类型。设置为该类型时，其一次process调用处理的数据可能来自多个数据处理任务，且process会并发调用，同一个任务内的数据在此时不保证处理的先后，当然数据处理完毕后会由框架重新收集排序，无需关心当前算子对整个数据流后续的影响|输入buffer的数量必须与输出buffer的数量一致|resize,crop|Process| |流数据功能单元|在处理数据时，功能单元需假设每次process都是处理当前任务数据流的数据，针对当前数据流可能还需要保存状态，并且process在数据流上要保持顺序处理，此时应当选择流类型的功能单元。设置为该类型时，框架会保证一个数据流的数据会顺序的进入process，不通数据流的数据会并发进入process，开发者无需关心数据之前是否是有序的，在process此处，已经由框架保证顺序。|输入buffer的数量无需与输出buffer的数量一致|decoder，encoder|DataPre,Process,DataPost| 通用功能单元举例：通用功能单元是比较常见的单元，比如要实现用固定的长宽去Resize其输入，这个功能单元，每次执行Resize时，不关心其输入数据的关联，即单个流的顺序上，以及多个流的隔离，因此这个Resize功能单元是一个通用功能单元，处理时一个process可以接收多个流混合的数据，并且process在并发调用，后续的数据可能会与之前的数据一同进行处理。关于通用功能单元数据运行的详细说明可以参考通用功能单元数据处理。配置：配置功能单元为通用功能单元，需要在MODELBOX_FLOWUNIT中将desc配置为modelbox::NORMAL，默认情况下功能单元是通用功能单元，此时功能单元的输入与输出必须一致。 MODELBOX_FLOWUNIT(ResizeFlowUnit, desc) { desc.SetFlowUnitName(\"Resize\"); desc.AddFlowUnitInput(modelbox::FlowUnitInput(\"In_1\", \"cpu\")); desc.AddFlowUnitOutput(modelbox::FlowUnitOutput(\"Out_1\", \"cpu\")); desc.SetFlowType(modelbox::NORMAL); } 流数据功能单元举例：流数据流数据处理的是需要关注数据是否来自同一个数据流，且同一个数据流的数据处理要有先后顺序。比如一个视频编码的功能单元，每次process处理时，都需要保证当前输入的数据来自同一个流，且可以获取到当前流的状态(编码器)，同时输入的数据都是顺序的，这样才能保证编码功能单元正确的对每一路流进行编码。因此这个视频编码功能单元必须设置为一个流数据功能单元。关于流数据功能单元数据运行的详细说明可以参考流数据功能单元数据处理。配置：配置功能单元为流数据功能单元，需要在MODELBOX_FLOWUNIT中将desc配置为modelbox::STREAM。流数据功能单元输出的buffer数量不一定与输入的buffer数量一致，默认情况下，输出输入的的buffer数量是认为不一致的，因此也是无法匹配，如果需要两者匹配可以配置SetStreamSameCount(true)。 MODELBOX_FLOWUNIT(EncoderFlowUnit, desc) { desc.SetFlowUnitName(\"Encoder\"); desc.AddFlowUnitInput(modelbox::FlowUnitInput(\"In_1\", \"cpu\")); desc.AddFlowUnitOutput(modelbox::FlowUnitOutput(\"Out_1\", \"cpu\")); desc.SetStreamSameCount(false); desc.SetFlowType(modelbox::STREAM); } 维度二：按输出数据层级分类 关于数据流层级，请参考数据流章节 类别 类别说明 约束 常见的算子 |同级功能单元|输出的数据与输入的数据属于同一层级|无|resize,crop,decoder,encoder| |展开功能单元|输出的数据是输入数据的下一层级，展开功能单元将输入的每个Buffer展开为一个新的Stream，该Stream属于输入Stream下一级的Stream|输入只能是一个buffer，且必须有输出buffer|demuxer| |收拢功能单元|输出的数据是输入数据的上一层级，收拢功能单元是将输入Stream中的数据收齐后形成一个Buffer，该Buffer属于输入Stream上一级的Stream|输出只能是一个buffer|enmuxer| 展开功能单元 举例：在开发功能单元时，输入输出的数据有固定的要求，如Resize输入的是图片流，产生的也是图片流。这是同一层级的。视频解封装(demuxer)功能单元输入的是视频组成的流(流里面的每一个buffer都是一个视频)，产生的是每个视频产生的packet流，每个packet流属于一个视频。因此packet流是视频组成的流的下一层级，而视频解封装功能单元是一个展开功能单元。关于层级的说明可以参考数据流层级 配置：配置功能单元为展开功能单元,需要SetOutputType(modelbox::EXPAND)，在默认情况下OutputType是modelbox::ORGINE，表示功能单元是同级功能单元。 MODELBOX_FLOWUNIT(DemuxerFlowUnit, desc) { desc.SetFlowUnitName(\"Encoder\"); desc.AddFlowUnitInput(modelbox::FlowUnitInput(\"In_1\", \"cpu\")); desc.AddFlowUnitOutput(modelbox::FlowUnitOutput(\"Out_1\", \"cpu\")); desc.SetOutputType(modelbox::EXPAND); } 收拢功能单元 举例：与展开功能单元相对应的是收拢功能单元,视频封装(enmuxer)功能单元输入和输出与视频解封装刚好是反过来的，视频封装功能单元是一个收拢功能单元。关于层级的说明可以参考数据流层级 配置：配置功能单元为展开功能单元,需要SetOutputType(modelbox::COLLAPSE)。默认情况下流中的数据只要到达了就会立即处理，如果需要一次性取完放入process中处理，可以配置SetCollapseAll(true)来实现。 MODELBOX_FLOWUNIT(DemuxerFlowUnit, desc) { desc.SetFlowUnitName(\"Encoder\"); desc.AddFlowUnitInput(modelbox::FlowUnitInput(\"In_1\", \"cpu\")); desc.AddFlowUnitOutput(modelbox::FlowUnitOutput(\"Out_1\", \"cpu\")); desc.SetOutputType(modelbox::COLLAPSE); desc.SetCollapseAll(false); } 特例：条件功能单元 条件功能单元是通用功能单元中的一种，当一个功能单元的输出，在不同的条件下需要用不同的流程去处理时，就可以使用条件功能单元来完成数据流的分支选择，使得同一路流可以在不同的流程处理，关于条件功能单元的说明可以参考条件功能单元数据处理 配置： 配置功能单元为条件功能单元，需要在MODELBOX_FLOWUNIT中将desc配置为modelbox::NORMAL，并且SetConditionType(modelbox::IF_ELSE)。 MODELBOX_FLOWUNIT(ConditionFlowUnit, desc) { desc.SetFlowUnitName(\"Condition\"); desc.AddFlowUnitInput(modelbox::FlowUnitInput(\"In_1\", \"cpu\")); desc.AddFlowUnitOutput(modelbox::FlowUnitOutput(\"Out_1\", \"cpu\")); desc.SetFlowType(modelbox::NORMAL); desc.SetConditionType(modelbox::IF_ELSE); } 注：使用条件算子时，必须配置当前算子batch_size= 1 功能单元类别与接口说明 类别 类别说明 需要重写的接口 同级通用功能单元 输入输出的数据是同一级别的，输入的数据不会排序，直接放入Process中 Process 同级流数据功能单元 输入输出的数据是同一级别的，流开始会调用DataPre,流结束会调用DataPost，输入的数据会排序后放入Process中 DataPre,Process,DataPost 展开通用功能单元 输出的数据是输入数据的下一层级，输入的数据不会排序直接放入Process中 Process 展开数据流功能单元 输出的数据是输入数据的下一层级，流开始会调用DataPre,流结束会调用DataPost，输入的数据会排序后放入Process中 DataPre,Process,DataPost 收拢通用功能单元 输出的数据是输入数据的上一层级，流开始会调用DataPre,流结束会调用DataPost，输入的数据会会排序后放入Process中。两个流之间不会按照输入的顺序排序 DataPre,Process,DataPost 收拢数据流功能单元 输出的数据是输入数据的上一层级，流开始会调用DataPre,流结束会调用DataPost，输入的数据会排序后放入Process中。流之间会按照输入的顺序排序调用，在所有流开始之前会调用DataGroupPre,在所有的流结束之后会调用DataGroupPost DataGroupPre,DataPre,Process,DataPost,DataGroupPost 按业务类型分类 按业务区分功能单元，功能单元的分类大致如下 类别 类别说明 例子 |输入类功能单元|数据输入类功能组件，输入数据使用|http，filereader，obs等| |输出类功能单元|数据输出类功能组件，输出数据使用|http，filewriter，obs等| |图像类功能单元|处理单个图像数据的功能组件，处理图像使用|resize, brightness等| |视频类功能单元|处理视频数据的功能组件|demutex，decode，encode等| |推理类功能单元|调用推理功能进行推理的组件|tensorrt, tensorflow等| |预处理、后处理功能单元|对tensor数据进行处理。|normalize，mean等| 功能单元类型样例 下图以车辆检测为例子说明涉及到功能单元类型 例子 车辆跟踪推理DEMO，当发现车辆时，对车辆画框图提示。 Flow流程图 流程说明： FileReader文件读取，从目录中读取Video.mpeg路径信息。 将文件数据发送给VideoDemux，VideoDemux将数据解开packet后发送给VideoDecoder。 VideoDecode获取packet并解码为图像。 图像数据分别发送到两个流程，一个发送给ImageResize，一个发送给ImageRender。 ImageResize将图像数据进行resize。 resize后的图像，发送给CarDetect进行模型推理。 推理后的数据发送给Box进行框信息处理。 Box输出框信息。 ImageRender接收两路输入，图像和框图信息，并对图像进行画框。 画框后的图像，输出到编码器VideoEncoder。 VideoEncode对图像进行编码，并发送给RTSP服务器。 流程上，使用了8个功能单元，1个是输入类，3个是处理流数据的视频功能单元，其他是图像处理和推理类的通用功能单元。 上图流程的涉及的功能单元列表以及类别 功能单元名称 功能 功能分类 业务分类 解释 File Reader 读取数据文件 流数据拆分类 输入类 输入是一个URL，输出是一个文件流。 Video Demux 解数据封包 流数据类 视频类 输入是文件流，输出是一组连续的packet流。 Video Decoder 视频解码 流数据类 视频类 输入是packet流，输出是独立的图像数据。 Image Resize 图像大小调整 通用类 图像类 输入是一张图像，输出也是一张图像。 Car Detect 车辆推理 通用类 推理类 输入是Tensor，输出是Tensor。 Box 框选取 通用类 后处理类 输入是Tensor，输出是框信息。 Image Render 合并框图信息 通用类 图像类 输入是两组数据，图像和框图，输出是图像。 Video Encoder 视频编码 流数据类 视频类 输入是多张图像，输出是一个视频流。 要查阅ModelBox所有预置的功能单元，请参考FlowUnits章节。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"framework-conception/stream.html":{"url":"framework-conception/stream.html","title":"数据流","keywords":"","body":"stream流 Stream流数据是一组顺序、连续到达的数据序列， ModelBox支持各种流数据的处理，这些数据包括，视频，音频，文件，结构化数据。 ModelBox中的流有如下属性： stream特点 保序的 流中的数据只能按照顺序处理，处理完成后的结果，要将按照顺序输出。 分段处理的。 流中的每个数据可以在处理逻辑上组成一个BufferList，每个BufferList都可以单独处理。 连续的。 流中的每个数据都是连续的，不能中断，也不能将流中的数据在图中执行不同的分支流程。 stream结构 ModelBox中流数据由Buffer组成，其结构如下： Buffer是组成stream的基本单位。 BufferList是一个逻辑概念，主要用于批量处理一组Buffer数据。 N(0 在stream中buffer有序号，标明buffer在流中的位置，位置是在流产生时决定的。 每个Buffer包含一个Meta结构，记录Buffer信息。 每个Stream包含一个Meta结构，记录Stream信息。 流的基本处理 流数据经过不同的功能单元处理后，可以匹配，产生兄弟流，聚合，展开，收拢。 匹配 为什么需要匹配 如上图所示，Source功能单元功能是产生图片，Inference功能单元的功能是输入图片输出图片中包含人的框，ColorTranspose功能单元的功能是将图片颜色进行变换，输入是图片输出也是图片，DrawBox功能单元的功能是输入图片和框，输出画了框的图片。在上图中，浅绿色表示图片，深绿色表示多个框的集合。在图-A中，每个端口的输入是单一明确的。在图-B中Inference功能单元输出的每一个buffer都是图片和多个框的集合的一个组合体。ColorTranspose功能单元的输入也必须是图片和多个框的集合组合成一个buffer，但ColorTranspose本身只处理组合体中的图片数据。而在图-A下ColorTranspose只需要处理图片数据即可。 但在图-A中需要解决框的集合和图片之间匹配的问题。 如何匹配 两个流匹配是基于流里面的buffer数量一致。 功能单元可能有多个端口作为输入，功能单元会将数据先进行匹配，匹配完成后将数据放入process中处理。在上图中，通用功能单元有first和second两个端口。first端口的数据先传入，second端口的数据后传入。则1-stream first端口进入时，功能单元会把buffer按照1 2 3 4顺序放入缓存但不会执行process，当second端口的数据传入后，匹配的数据完成匹配后才会放入process中执行，而产生的数据的顺序会按照first端口的顺序来排列。通用功能单元要求输入的流必须是一个流，而经过通用功能单元所产生的流与输入的流也必须是一个流。不同流之间的数据匹配是无关的，如上图中1-stream中的数据是可以互相匹配的，但是1-stream的数据与2-stream的数据是不能匹配的。 通用功能单元数据处理 如上图所示，1-stream和2-stream是两个流，经过同一个通用功能单元。 输入输出数量一致 通用功能单元的输入与输出的buffer数量必须一致，1-stream输入四个buffer，则输出也必须为四个buffer。1-stream经过通用功能单元产生的四个buffer还是在1-stream中，不会出现在2-stream中，因此对每个stream来说，可以认为其它的stream是不可见的。 按batch_size合并后输出 通用功能单元会按batch数量合并后由process处理。如上图所示，通用功能单元的batch_size设置为5,则通用功能单元会再将1-stream的数据填满后会取走2-stream的部分数据填满batch。如果剩下的数据无法填满batch，数据也会把剩下的所有数据聚合在下一个batch中处理。 条件功能单元数据处理 条件功能单元是一种特殊的通用功能单元，条件功能单元的输出必须大于1。条件功能单元其向每个端口的输出数量之和等于输入buffer的数量。如上图所示，1号 2号 3号 4号四个buffer经过条件功能单元A后，其上端口输出1 2 3 三个buffer，下端口输出4 一个buffer。但这两路数据都会被认为是1-stream流的一个部分流，部分流只能用通用功能单元处理，而不能用数据流功能单元处理。因为每一路数据中包含的流信息都是不完整的。部分流可以通过条件功能单元进一步切分成新的部分流。当将部分流的数据输入到一个通用功能单元的同一端口时，会产生聚合，即将输入的部分流聚合成条件功能单元输入时的流。如条件功能单元B产生的1-1-1-part-stream和1-1-2-part-stream最终由通用功能单元B聚合得到1-1-part-stream。注意部分流有层级结构，如图上所示1-1-1-part-stream只能与1-1-2-part-stream聚合，而不能与1-2-part-stream的部分流聚合，只有在聚合成1-1-part-stream的流后才能再进行聚合。 流数据功能单元数据处理 数据经过流处理功能单元会产生兄弟流。在单元数据在匹配后会先进行排序，如上图所示，1-stream的在进入流处理功能单元后会排序，然后放入同一个batch中由process处理,在流数据功能单元中不同的stream中的buffer会放入不同的batch中进行处理。在经过了流数据功能单元流处理后，会产生一个新的兄弟流，兄弟流在默认的情况下与前面的流不是一个流，无法匹配。但可以与可以通过设置SetStreamSameCount(true)来设置流处理单元输出的兄弟流与输入的流可以进行匹配。 流处理功能单元除了通常处理数据process函数，还会有其他一些函数也会在stream的开始和结束阶段被调用： 在处理流之前，会触发DataPre调用，这里可以用来申请资源，DataPre中可以设置下一个Stream的Meta信息，如图中2-stream的Meta信息。 当流中最后一个数据处理完成后，将会触发DataPost调用，这里可以用来释放资源。 数据流层级 数据流在业务中常常需要展开或收拢，如上图所示，main-stream中有两个buffer每个buffer表示一个视频流地址，而其后的图片普通功能单元处理的是多张图片组成的一个图片流，因此视频流地址需要展开为多张图片组成的一个流。展开功能单元主要就是用来完成将图中main-stream 的1 buffer展开成图片流1-stream，在图片普通功能单元处理后，如果需要收拢为图片流为视频流，可以通过收拢功能单元其还原成main-stream。 在展开功能单元中，process中输入的buffer每次只有一个。收拢功能单元接收下一层流时会排序，并对接收到的每个流都会触发Datapre和DataPost调用。一个流经过收拢后最终会产生一个上级流的buffer。 展开功能单元和收拢功能单元也可以分为通用展开功能单元，数据流展开功能单元，通用收拢功能单元，数据流收拢功能单元。如上图所示：通用展开功能单元和数据流展开功能单元的区别在于，在展开时，数据流展开功能单元如图-A所示，在展开完1-stream后才可以展开2-stream。通用展开功能单元会如图-B所示，会同时展开1-stream和2-stream。数据流收拢功能单元则会在1-stream收拢完后才会2-stream，通用收拢功能单元会同时收拢1-stream和2-stream。 如图-B所示，数据流收拢功能单元在所有的子流收拢之前会调用DataGroupPre，在所有的子流收拢之后则会调用DataGroupPost 展开归拢 VS 产生一个兄弟流 展开表现为由一个buffer产生多个buffer。但是由一个buffer产生多个buffer并不一定是展开。同样,归拢表现为由多个buffer合成一个buffer，但是由多个buffer合成一个buffer并不一定是归拢。展开和归拢限定于将一个buffer展开成一个完整的流和将一个流里全部的buffer合成为一个buffer。比如，一个长视频可以展开为N(0 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"framework-conception/buffer.html":{"url":"framework-conception/buffer.html","title":"Buffer","keywords":"","body":"Buffer Buffer是ModelBox中功能单元之前传递数据的唯一载体。采用Buffer进行传递的主要原因是功能单元会运行在多种设备上，因此当前功能单元实现时，不应假设其之前或者之后的功能单元所使用的内存属于哪一个设备，当前功能单元只需要指明其输入所期望的设备是什么，由框架完成数据的搬移工作，以降低功能单元的连接限制。 Buffer与数据的传递 在ModelBox的功能单元开发中，Buffer作为功能单元的输入或者输出数据的载体出现，对其使用的了解是基于ModelBox开发的基本要求，同时了解其如何承载数据、数据的存储规范对于更好的开发功能单元将会有很大帮助。 功能单元进行数据处理时，其process将要对输入数据完成操作，并产生输出，这个过程中，用户将从DataContext中获取到本次需要处理的数据。ModelBox根据如下因素的影响来决定本次需要处理的数据： 当前节点存在一个输入队列，用来存放最近未处理的数据，其长度可以通过配置进行设定。 当节点触发调用运行时，会将队列中的数据全部取出，准备进行处理。 数据取出后，调用process处理时，存在batch_size的选项，即每次process最多可以处理的数据量。框架根据batch_size和功能单元的类型对数据进行划分，同时将数据搬移到功能单元指定的输入设备上，最终决定了process调用的次数，以及每次process调用时的数据数量。 在功能单元开发者的process函数看到的输入BufferList便是在框架中完成了拆分后本次process需要处理的数据列表，列表满足的约束请参考流算子章节。 输出数据时，首先要从DataContext中取出的输出BufferList，然后进行BufferList的Build，或者在创建Buffer时使用GetBindDevice，这两个操作都会为输出的内存指定设备，保证输出的数据分配在指定的设备上。 Buffer的约束 作为输入数据时，不应当对当前内存进行修改，因此对象内存已被标记未数据不可修改的状态，此时读取数据需要使用ConstData来获得数据指针。 作为输出数据时，buffer的持有者唯一，它是可以写入的，需要调用MutableData返回数据指针，并对其进行写入。 数据主体应当存放于Buffer里获取的数据指针所指向的存储空间，对于当前数据的描述信息即元数据则需要存放在Buffer->Set中，因为这类元信息是与设备无关的，只需要一直保存在主机内存里即可。 Buffer常用接口介绍 以下列表展现了较为常用的Buffer的成员函数。 函数名称 返回值类型 函数功能 MutableData() void* 获取buffer可变数据的指针 ConstData() const const void* 获取buffer常量数据的指针 GetBytes() const size_t 获取buffer的字节大小 Get(const std::string& key) std::tuple 根据meta的键获取相对应的值 GetDevice() std::shared_ptr 返回buffer所对应的设备 Get(const std::string& key, T&& value) bool 在buffer中是否存在meta的key值 Set(const std::string& key, T&& value) void 给buffer设置meta的键值对 CopyMeta(const std::shared_ptr buf, bool is_override = false) Status 复制buf的meta值 Copy() std::shared_ptr buffer的浅拷贝 DeepCopy() std::shared_ptr buffer的深拷贝 BufferList BufferList是Buffer的vector集合。ModelBox提供了完备的api，可以简单地批量修改Buffer。 其中，BufferList->At(idx)可直接用BufferList[idx]代替。 函数名称 返回值类型 函数功能 MutableData() void* 获取bufferList首个buffer的可变数据的指针 ConstData() const void* 获取bufferList首个buffer的常量数据的指针 MutableBufferData(size_t idx) void* 获取bufferList首个buffer的可变数据的指针 ConstBufferData(size_t idx) const const void* 获取bufferList首个buffer的常量数据的指针 GetBytes() size_t 获取bufferList的字节大小 Size() size_t 获取bufferList的长度 GetDevice() std::shared_ptr 返回bufferList的首个buffer所对应的设备 At(size_t idx) std::shared_ptr 根据索引值 返回此处的buffer PushBack(const std::shared_ptr& buf) void 将新的buffer加入到bufferList末尾 Set(const std::string& key, T&& value) void 给bufferList中所有buffer设置相同的meta键值对 CopyMeta(const std::shared_ptr bufferList, bool is_override = false) Status 复制传入函数中的另一个bufferList的meta值 MakeContiguous() Status 使bufferList在显存或者内存中连续 Reset() Status 清空bufferList © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"api/api.html":{"url":"api/api.html","title":"API","keywords":"","body":"API ModelBox API包括了多种编程语言，有C++，python，具体API的支持范围如下： 请选择合适的开发语言进行扩展。 类型 说明 C++ Python ModelBox Server Plugin modelbox微服务插件。 ✔️ ❌ ModelBox Library modelbox开发API。 ✔️ ✔️(不包含基础组件) ModelBox FlowUnit ModelBox功能单元开发API。 ✔️ ✔️ ModelBox Device 设备支持开发API。 ✔️ ❌ 下面具体说明各个组件的API组件信息。 ModelBox Server Plugin ModelBox微服务插件，提供了开发为服务必要的API接口，对应的周边组件如下： ModelBox插件可以调用的接口有： Job: 任务管理，可以添加，删除，查询图以及对应的任务。 Config：配置读取，可以从ModelBox Server的配置文件/usr/local/etc/modelbox/modelbox.conf中读取配置项。 Listener: http server，可以注册HTTP请求的URL事件。 Timer: 定时器，可以注册定时任务，定时触发特定的函数。 Modelbox Library: ModelBox运行库的所有API。 ModelBox Library ModelBox运行库，提供了对业务开发需要的API接口，对应的周边组件如下： ModelBox Library包含基础Base部分和功能部分。基础部分用于支撑业务的运行，功能部分用于支撑AI推理的运行。 基础Base 基础Base，包含了各种支撑业务运行的组件，包括如下组件： BlockingQueue，阻塞队列。 Config，图配置读取。 Crypto，数据加解密。 Status，错误返回接口。 Utils，工具函数。 Device， 设备抽象接口。 Timer，定时器组件。 ThreadPool，线程池组件。 Log，日志组件。 Slab，Slab内存缓存组件。 OS Adapter API，OS抽象接口 Driver，ModelBox插件接口。 注意：python仅包含Log, Status, Config组件接口。 ModelBox推理接口 推理接口包含运行推理任务，和编写功能单元的模块，包括如下组件 Buffer: 数据接口，用于承载AI推理数据。 Flow，推理启动接口，用于加载编排图，并启动推理任务。 FlowUnit，功能单元接口，用于扩展新的功能单元组件。 TensorList，Buffer操作接口，支持使用Tensor相关的接口操作Buffer。 DataContext，支持FLowUint功能单元开发的接口，用于获取功能单元的输入，输出，和上下文的存储。 Session，会话上下文，用于开发存储和会话相关的信息。 Statistics，统计接口，用于统计，获取相关组件的统计信息。 注意：python仅包含Buffer, Flow, Flowunit, tensor, Datacontext, Session相关的接口。 开发扩展 ModelBox Server Plugin，ModelBox Device，ModelBox Flowunit的扩展，请参考相关的开发指导。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"api/c++.html":{"url":"api/c++.html","title":"C++","keywords":"","body":"C++ API 在ModelBox服务启动，并开启Editor编辑器后，可直接使用http://[host]:1104/api/访问。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"api/python.html":{"url":"api/python.html","title":"Python","keywords":"","body":"Python API 待完善... © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"flowunits/flowunits.html":{"url":"flowunits/flowunits.html","title":"FlowUnits","keywords":"","body":"FlowUnits ModelBox预置了多个FlowUnit（也可称为算子），可完成AI推理算法的基本流程。本章简要介绍所有预置FlowUnit，及其主要功能、输入输出、配置、约束等内容。 按业务类型分类，modelbox所有预置FlowUnit如下表所示。FlowUnit的使用案例，可参考功能单元连接案例。 业务分类 功能单元名称 功能简介 输入类 data_source_parse 解析外部到算法流水线的输入 输入类 video_input 获取视频输入地址 输出类 output_broker 将算法处理结果输出到外部 网络收发类 httpserver_async 异步收发http请求 网络收发类 httpserver_sync 同步收发http请求 视频类 videodecoder 视频解码 视频类 videodemuxer 视频解封装 视频类 videoencoder 视频编码 图像类 color_transpose 对图片进行颜色通道转换 图像类 crop 对图片进行裁剪 图像类 draw_bbox 在图像上画框 图像类 image_decoder 图像解码 图像类 mean 图像减均值 图像类 normalize 图像标准化 图像类 padding 图像填充 图像类 resize 图像尺寸调整 推理类 inference 模型推理 后处理类 common_yolobox 从yolo模型中获取检测目标的信息 后处理类 yolobox 从yolo模型中获取检测目标的信息 buffer处理类 meta_mapping 做元数据映射 color_transpose算子 功能 对图片进行颜色通道转换。 输入 待处理的图片。 输出 颜色空间转换后的图片。 配置 GPU版本color_transpose算子，输入输出配置：label，需设置为input|output，输出图片颜色类型配置：out_pix_fmt，需设置为gray、rgb或bgr中的一种。 CPU版本color_transpose算子，输入输出配置：label，需设置为in_image|out_image。 约束 目前GPU版本color_transpose算子仅支持以下图片类型： 三通道（bgr, rgb）和单通道（gray）UINT8类型的图像。 common_yolobox算子 功能 从yolo模型中得到检测目标的信息，包括检测框、置信度、类别等 输入 yolo模型输出层参数，包含类别数、阈值等信息的配置文件 输出 目标检测框坐标、置信度、类别等 配置 输入输出配置：label，需设置模型输出层名称，例如layer15_conv|layer22_conv|Out_1 约束 为CPU算子 crop算子 功能 对图片进行裁剪。 输入 待裁剪的图片，裁剪坐标。 输出 裁剪后的图片。 配置 CPU版本crop算子，算子名称配置为：flowunit=cv_crop，输入输出配置：label，需设置为In_img|In_box|Out_img。 GPU版本crop算子，算子名称配置为：flowunit=nppi_crop，输入输出配置：label，需设置为In_img|In_box|Out_img，方法配置：method，目前只支持u8c3r。 ascend版本crop算子，算子名称配置为：flowunit=crop，输入输出配置：label，需设置为in_img|in_box|out_img。 约束 目前CPU、GPU版crop算子支持的输入输出图片为rgb或bgr三通道图像，ascend版本支持的输出格式为rgb、bgr、nv12、nv21。 data_source_parse算子 功能 解析外部到算法流水线的输入，目前主要支持视频输入。 输入 外部输入配置(data_source_cfg) 输出 取视频流或视频文件的url地址(stream_meta) 配置 输入可以配置为obs、restful、url、vcn、vis。 约束 目前此算子仅接收视频类型输入，obs、vcn、vis需购买相应的华为云服务。 draw_bbox算子 功能 在图像上画框。 输入 待处理的图片，画框坐标。 输出 画框后的图片。 配置 输入输出配置：label，需设置为In_1 | In_2 | Out_1。 约束 为CPU算子。 httpserver_async算子 功能 接收http请求，并异步返回响应结果。 输入 http请求的uri、head、body等信息。 输出 发送请求是否成功的响应状态值。 配置 http请求的地址和端口（request_url，如\"https://localhost:56789\"），证书（cert），密钥（key），密码（passwd），密钥解密字符（key_pass），最大请求数量（max_requests）。 约束 为CPU算子。https请求时必须配置密钥证书认证。 httpserver_sync算子 功能 接收http请求，并同步返回处理结果。 输入 http请求的uri、head、body等信息。 输出 算法返回的处理结果和响应状态。 配置 http请求的地址和端口（request_url，如\"https://localhost:56789\"），证书（cert），密钥（key），密码（passwd），密钥解密字符（key_pass），最大请求数量（max_requests）。 约束 https请求时必须配置密钥证书认证。 image_decoder算子 功能 图像解码算子 输入 待解码的图像数据，packet流 输出 解码后的图像数据 配置 CPU版本，解码方式，如method=\"imread_color\"；GPU版本，输出图片格式，如pixel_format=\"bgr\"。 约束 CPU版本解码方式只支持以下几种：\"imread_color\", \"imread_anycolor\", \"imread_reduced_dolor_2\",\"imread_reduced_color_4\", \"imread_reduced_color_8\"，支持的输出图片格式为bgr、rgb、yuv；GPU版本支持bgr、rgb。 inference算子 功能 模型推理 输入 Tensor 输出 Tensor 配置 约束 目前支持的TensorFlow版本为1.13.1和1.15.0。 mean算子 功能 用于减去图像的均值。 输入 待处理图像 输出 减均值后的图像。 配置 各通道均值（例如mean=\"0.0,10.0,20.0\"） 约束 适用于RGB/BGR图像 meta_mapping算子 功能 做元数据映射，对指定meta字段进行映射修改，包括meta的名称、值，值只支持基本类型。 输入 输入需要转换meta的buffer 输出 输出转换meta后的buffer 配置 src_meta=\"src\" dest_meta=\"dest\" rules=\"v1=v2,v3=v4,v5=v6\" 约束 normalize算子 功能 图像标准化算子 输入 待处理图像 输出 标准化处理后的图像。 配置 标准化参数（例如normalize=\"0.003921568627451,1,1\"） 约束 适用于RGB/BGR图像 output_broker算子 功能 将算法处理结果输出到外部。 输入 算法处理结果，输出配置。 输出 算法处理结果输出到所配置的输出路径中。 配置 输出可以配置为obs、dis、webhook。可以设置重连相关参数（retry_count_limit=\"2\", retry_interval_base_ms=\"100\", retry_interval_increment_ms=\"100\", retry_interval_limit_ms=\"200\"） 约束 输出到obs\\dis需在华为云开通相关服务。 padding算子 功能 图像填充算子 输入 待处理图像 输出 填充处理后的图像。 配置 填充配置参数（例如image_width=200, image_height=100, vertical_align=top, horizontal_align=center, padding_data=\"0, 255, 0\"） 约束 resize算子 功能 图像尺寸调整 输入 待处理图像 输出 尺寸调整后的图像。 配置 图像调整后的长度和宽度，插值方法。（例如width=128, height=128, method=\"inter_nearest\"） 约束 videodecoder算子 功能 视频解码算子 输入 待解码视频数据 输出 解码后的视频 配置 约束 videodemuxer算子 功能 视频解封装算子 输入 文件流 输出 一组连续的packet流 配置 约束 videoencoder算子 功能 视频编码算子 输入 packet流 输出 独立的图像数据 配置 约束 video_input算子 功能 获取视频输入地址 输入 URL 输出 文件流 配置 视频地址：source_url=\"@SOLUTION_VIDEO_DIR@/test_video_vehicle.mp4\" 约束 yolobox算子 功能 从yolo模型中得到检测目标的信息，包括检测框、置信度、类别等 输入 tensor，yolo模型输出层参数，包含类别数、阈值等信息的配置文件 输出 目标检测框坐标、置信度、类别等 配置 输入输出配置：label，需设置模型输出层名称，例如layer15_conv|layer22_conv|Out_1 约束 为CPU算子 功能单元连接案例 图片输入场景如果算法以图片作为输入，一般是以httpserver算子接收图片信息。如下所示图配置，实现的功能是mnist数据集图片的推理预测功能。 httpserver_sync_receive[type=flowunit, flowunit=httpserver_sync_receive, device=cpu, deviceid=0, request_url=\"http://localhost:7778\", max_requests=100, time_out=10] mnist_preprocess[type=flowunit, flowunit=mnist_preprocess, device=cpu, deviceid=0] mnist_infer[type=flowunit, flowunit=mnist_infer, device=cuda, deviceid=0] mnist_response[type=flowunit, flowunit=mnist_response, device=cpu, deviceid=0] httpserver_sync_reply[type=flowunit, flowunit=httpserver_sync_reply, device=cpu, deviceid=0] httpserver_sync_receive:Out_1 -> mnist_preprocess:In_1 mnist_preprocess:Out_1 -> mnist_infer: Input mnist_infer: Output -> mnist_response: In_1 mnist_response: Out_1 -> httpserver_sync_reply: In_1 用户在客户端，图片信息放到body体中，然后将消息发送给httpserver_sync_receive算子，httpserver_sync_receive算子接收请求信息，并传给下一个算子mnist_preprocess，mnist_preprocess解析请求信息，获取图片数据，而后传给mnist_infer算子进行模型推理，得到预测结果，最后通过httpserver_sync_reply算子，将算法预测结果返回给用户的客户端。 视频输入场景如果算法以视频作为输入，一般是以video_input配置视频地址，如果视频通过OBS/VIS/VCN等服务获取，一般以data_source_parser解析输入信息。 如下示例为车辆检测流程图，输入为本地视频，以video_input作为第一个算子,配置本地视频地址，将视频数据传递给解封装算子，再经过视频解码、图像尺寸调整、颜色空间转换、标准化、推理、获取yolo检测结果、图像画框、图像编码等一系列过程，最后通过rtsp取流，客户就可以实时观察到车辆检测结果。 video_input[type=flowunit, flowunit=video_input, device=cpu, deviceid=0, source_url=\"@SOLUTION_VIDEO_DIR@/test_video_vehicle.mp4\"] videodemuxer[type=flowunit, flowunit=videodemuxer, device=cpu, deviceid=0] videodecoder[type=flowunit, flowunit=videodecoder, device=cpu, deviceid=0, pix_fmt=rgb, queue_size = 16, batch_size=5] frame_resize[type=flowunit, flowunit=cv_resize, device=cpu, deviceid=0, image_width=800, image_height=480, method=\"inter_nearest\", batch_size=5, queue_size = 16] car_color_transpose[type=flowunit, flowunit=color_transpose, device=cpu, deviceid=0, batch = 5, queue_size = 16] car_normalize[type=flowunit, flowunit=normalize, device=cpu, deviceid=0, normalize=\"0.003921568627451, 0.003921568627451, 0.003921568627451\", queue_size = 16, batch_size = 5] car_inference[type=flowunit, flowunit=car_inference, device=cuda, deviceid=0, queue_size = 16, batch_size = 5] car_yolobox[type=flowunit, flowunit=car_yolobox, device=cpu, deviceid=0, image_width=1920, image_height=1080, queue_size = 16, batch_size = 5] draw_bbox[type=flowunit, flowunit=draw_bbox, device=cpu, deviceid=0, queue_size = 16, batch_size = 5] videoencoder[type=flowunit, flowunit=videoencoder, device=cpu, deviceid=0, queue_size=16, default_dest_url=\"rtsp://localhost/test\", encoder=\"mpeg4\"] video_input:stream_meta -> videodemuxer:stream_meta videodemuxer:video_packet -> videodecoder:video_packet videodecoder:frame_info -> frame_resize:In_1 frame_resize: Out_1 -> car_color_transpose: in_image car_color_transpose: out_image -> car_normalize: input car_normalize: output -> car_inference:data car_inference: \"layer15-conv\" -> car_yolobox: \"layer15-conv\" car_inference: \"layer22-conv\" -> car_yolobox: \"layer22-conv\" car_yolobox: Out_1 -> draw_bbox: In_1 videodecoder:frame_info -> draw_bbox: In_2 draw_bbox: Out_1 -> videoencoder: frame_info 如果是以data_source_parser解析视频输入，则video_input部分可替换为以下内容： input1[type=input] data_source_parser[type=flowunit, flowunit=data_source_parser, device=cpu, deviceid=0, label=\" | \"] 输入信息在data_source_cfg中配置 视频流重连 因网络等原因导致取流推流失败时，ModelBox可以重试连接，相关参数可由用户自行设置。在data_source_parser可配置重连相关参数。 retry_enable：是否开启重连； retry_interval：重连间隔； retry_times:重连次数。 还可针对VIS\\VCN等具体服务来配置重连参数，如vis_retry_enable，vis_retry_interval，vis_retry_times。 在output_broker输出模块，相关配置类似。如retry_count_limit=\"2\", retry_interval_base_ms=\"100\", retry_interval_increment_ms=\"100\", retry_interval_limit_ms=\"200\"。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "},"faq/faq.html":{"url":"faq/faq.html","title":"FAQ","keywords":"","body":"FAQ ModelBox 什么是ModelBox，ModelBox有什么功能？ ModelBox是一个AI应用的推理框架，ModelBox通过编排和插件化的形式支持AI应用的开发，支持的数据有视频，音频，语音，文本，通用数据的编排处理，modelbox的主要功能列表可参考这里 相比直接调用底层API开发AI业务，ModelBox有什么优势？ ModelBox主要聚焦解决AI应用开发的问题，相比直接调用底层API，开发者需要关注每个底层的API使用方法，关注并发，关注GPU，NPU设备编程接口，关注tensorflow，tensorrt等推理框架的编程API，与云计算结合的接口，和分布式通信，日志等琐碎而复杂的周边代码。 ModelBox解决的就是业务开发的周边问题，将周边问题交由ModelBox处理，ModelBox通过对内存，CPU，GPU，周边组件的精细化管理，使AI推理业务开发更高效，性能也更高，质量也更好。 ModelBox目前支持哪些框架训练的模型（TensorFlow、Caffe、PyTorch等） ModelBox框架里面包含了支持TensorFlow, Caffe, Pytorch模型运行所需的功能单元Flowunit，我们称为推理功能单元(Inference Flowunit)，这些推理功能单元可以直接加载对应的模型文件，而不需要编写代码，只需提供一个简单的配置文件，即可将模型引入到ModelBox的流程中。目前支持的模型有TensorFlow, TensorRT, Ascend ACL模型。 ModelBox组件 ModelBox程序包含哪些部分 ModelBox目前包含如下组件 微服务ModelBox Server 运行库ModelBox Library 维护工具ModelBox Tool CPU相关的功能单元 Huawei Ascend相关的功能单元 Cuda相关的功能单元 可视化编辑工具 ModelBox支持服务式吗？ ModelBox有专门的微服务程序，ModelBox Server，ModelBox Server内置了通用的管理插件，和基本功能，开发者只需要配置ModelBox Server即可启动微服务。 如何调试ModelBox程序 ModelBox本身为C++代码编写，开发者可以通过如下方式调试ModelBox程序和相关的功能单元： GDB，IDE等工具调试 ModelBox运行日志。 ModelBox Profiling性能统计工具。 具体操作方法，可参考调试定位章节内容 模型问题 tensorrt在解析模型出错 当tensorrt在解析模型出错时，如果报错 \" expecting compute x.x got compute 7.5, please rebuild\"，说明模型和推理引擎不配套，需要转换模型到配套的硬件, 并在与当前环境配置相同的环境上重新编译模型。 运行环境 其他版本的cuda 如果想要下载其他cuda版本的镜像，可以选择使用以下命令。比如cuda10.1版本镜像，就是modelbox_cuda101_develop。其他版本均可以此类推。 docker pull modelbox/modelbox_cuda101_develop:latest docker启动脚本中，请注意启动的镜像版本是否与自己所需的镜像版本一致。 docker启动脚本详解 此处以modelbox_cuda101_develop:latest镜像举例 docker run -itd --gpus all -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,video \\ --tmpfs /tmp --tmpfs /run -v /sys/fs/cgroup:/sys/fs/cgroup:ro \\ modelbox/modelbox_cuda101_develop:latest -itd 选项 选项简写 说明 -detach -d 在后台运行容器,并且打印容器id。 -interactive -i 即使没有连接，也要保持标准输入保持打开状态，一般与 -t 连用。 –tty -t 容器重新分配一个伪输入终端，一般与 -i 连用。 -gpus all 请通过 docker -v 检查 Docker 版本。对于 19.03 之前的版本，需要使用 nvidia-docker2 和 --runtime=nvidia 标记；对于 19.03 及之后的版本，则使用 nvidia-container-toolkit 软件包和 --gpus all 标记。 -e 设置环境变量 --tmpfs 挂载目录到容器中，而且容器内的修改不会同步到宿主机，也不希望存储在容器内， 调用这个参数，将该挂载存储在主机的内存中，当容器停止后， tmpfs挂载被移除，即使提交容器，也不会保存tmpfs挂载 -v 挂载宿主机的指定目录 ( 或文件 ) 到容器内的指定目录 ( 或文件 ) ro表示read-only 注意事项： 容器目录必须为绝对路径 容器销毁后，挂载的文件以及 在容器修改过的内容仍然保留在宿主机中 --privileged=true 当开发者需要使用gdb调试功能时，需要使用特权模式启动docker --cap-add=SYS_PTRACE 增加容器镜像系统的权限 ptrace()系统调用函数提供了一个进程（the “tracer”）监察和控制 另一个进程（the “tracee”）的方法。 并且可以检查和改变“tracee”进程的内存和寄存器里的数据。 它可以用来实现断点调试和系统调用跟踪。（用于gdb） --security-opt seccomp=unconfined Seccomp是Secure computing mode的缩写。 设为unconfined可以允许容器执行全部的系统的调用。 有遇到无法启动的问题， 请检查是否安装nvidia-container-toolkit 和对应的cuda(10)版本 查看当前的镜像对应的欧拉系统的版本 cat /etc/EulerLinux.conf 功能单元 功能单元的分类 功能单元可以分为两大类 1. 实际的功能单元 由实际代码所实现，每套代码对应自己的功能单元 2. 虚拟功能单元 只有一个in配置文件，所有的具体实现在一个tensorrt的模块中，端口名、数据类型都是通过配置文件配置的。 其中的plugin参数指定了可以注册功能的类，比如plugin为yolo，也就是说，yolo的实例注册到tensorrt里面的。 sessioncontext与datacontext sessioncontext保存的是当前flow的全局变量，每一个flowunit存储在里面的数据在其他flowunit也可以读到。 datacontext表示当前flowunit在当前流的数据buffer，可以设置输入输出，也可以保存私有数据。 video_input video_input的repeat可以创建多个并发视频，并不是串行视频流 Modelbox Tool develop mode already enabled 在执行modelbox-tool develop -e开启开发者模式后，如果更改了默认位于/usr/local/etc/modelbox/的modelbox.conf配置文件的内容，需要先执行modelbox-tool develop -d来关闭开发者模式，再启动才行。 © 华为技术有限公司 all right reserved，powered by Gitbook文件修订时间： 2021-10-21 02:19:07 "}}